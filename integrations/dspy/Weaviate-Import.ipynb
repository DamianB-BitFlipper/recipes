{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28af9f4c",
   "metadata": {},
   "source": [
    "# Weaviate Import\n",
    "\n",
    "This notebook is used to populate the `WeaviateBlogChunk` class.\n",
    "\n",
    "1. Run `docker-compose up -d` with the docker script in the file to start Weaviate locally on localhost:8080\n",
    "\n",
    "2. Make sure the `/blog` folder is in this directory (these are parsed from github.com/weaviate/weaviate-io -- feel free to drag and drop that folder in here to update the content).\n",
    "\n",
    "3. Run this notebook and the 1182 blog chunks will be loaded into Weaviate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf69ba40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/weaviate/warnings.py:121: DeprecationWarning: Dep005: You are using weaviate-client version 3.26.2. The latest version is 4.4.4.\n",
      "            Please consider upgrading to the latest version. See https://weaviate.io/developers/weaviate/client-libraries/python for details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import Weaviate and Connect to Client\n",
    "import weaviate\n",
    "client = weaviate.Client(\"http://localhost:8080\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c3ecfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Schema\n",
    "schema = {\n",
    "   \"classes\": [\n",
    "       {\n",
    "           \"class\": \"WeaviateBlogChunk\",\n",
    "           \"description\": \"A snippet from a Weaviate blogpost.\",\n",
    "           \"moduleConfig\": {\n",
    "               \"text2vec-openai\": {\n",
    "                    \"skip\": False,\n",
    "                    \"vectorizeClassName\": False,\n",
    "                    \"vectorizePropertyName\": False\n",
    "                },\n",
    "                \"generative-openai\": {\n",
    "                    \"model\": \"gpt-3.5-turbo\"\n",
    "                }\n",
    "           },\n",
    "           \"vectorIndexType\": \"hnsw\",\n",
    "           \"vectorizer\": \"text2vec-openai\",\n",
    "           \"properties\": [\n",
    "               {\n",
    "                   \"name\": \"content\",\n",
    "                   \"dataType\": [\"text\"],\n",
    "                   \"description\": \"The text content of the podcast clip\",\n",
    "                   \"moduleConfig\": {\n",
    "                    \"text2vec-transformers\": {\n",
    "                        \"skip\": False,\n",
    "                        \"vectorizePropertyName\": False,\n",
    "                        \"vectorizeClassName\": False\n",
    "                    }\n",
    "                   }\n",
    "               },\n",
    "               {\n",
    "                \"name\": \"author\",\n",
    "                \"dataType\": [\"text\"],\n",
    "                \"description\": \"The author of the blog post.\",\n",
    "                \"moduleConfig\": {\n",
    "                    \"text2vec-openai\": {\n",
    "                        \"skip\": True,\n",
    "                        \"vectorizePropertyName\": False,\n",
    "                        \"vectorizeClassName\": False\n",
    "                    }\n",
    "                }\n",
    "               }\n",
    "           ]\n",
    "       }      \n",
    "   ]\n",
    "}\n",
    "    \n",
    "client.schema.create(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a6788d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def chunk_list(lst, chunk_size):\n",
    "    \"\"\"Break a list into chunks of the specified size.\"\"\"\n",
    "    return [lst[i:i + chunk_size] for i in range(0, len(lst), chunk_size)]\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    \"\"\"Split text into sentences using regular expressions.\"\"\"\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
    "    return [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "\n",
    "def read_and_chunk_index_files(main_folder_path):\n",
    "    \"\"\"Read index.md files from subfolders, split into sentences, and chunk every 5 sentences.\"\"\"\n",
    "    blog_chunks = []\n",
    "    for folder_name in os.listdir(main_folder_path):\n",
    "        subfolder_path = os.path.join(main_folder_path, folder_name)\n",
    "        if os.path.isdir(subfolder_path):\n",
    "            index_file_path = os.path.join(subfolder_path, 'index.mdx')\n",
    "            if os.path.isfile(index_file_path):\n",
    "                with open(index_file_path, 'r', encoding='utf-8') as file:\n",
    "                    content = file.read()\n",
    "                    sentences = split_into_sentences(content)\n",
    "                    sentence_chunks = chunk_list(sentences, 5)\n",
    "                    sentence_chunks = [' '.join(chunk) for chunk in sentence_chunks]\n",
    "                    blog_chunks.extend(sentence_chunks)\n",
    "    return blog_chunks\n",
    "\n",
    "# Example usage\n",
    "main_folder_path = './blog'\n",
    "blog_chunks = read_and_chunk_index_files(main_folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed58c948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1182"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(blog_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ea97830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'---\\ntitle: Combining LangChain and Weaviate\\nslug: combining-langchain-and-weaviate\\nauthors: [erika]\\ndate: 2023-02-21\\ntags: [\\'integrations\\']\\nimage: ./img/hero.png\\ndescription: \"LangChain is one of the most exciting new tools in AI. It helps overcome many limitations of LLMs, such as hallucination and limited input lengths.\"\\n---\\n![Combining LangChain and Weaviate](./img/hero.png)\\n\\nLarge Language Models (LLMs) have revolutionized the way we interact and communicate with computers. These machines can understand and generate human-like language on a massive scale. LLMs are a versatile tool that is seen in many applications like chatbots, content creation, and much more. Despite being a powerful tool, LLMs have the drawback of being too general.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blog_chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "115c2980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<weaviate.batch.crud_batch.Batch at 0x10572e3b0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.batch.configure(\n",
    "  # `batch_size` takes an `int` value to enable auto-batching\n",
    "  # (`None` is used for manual batching)\n",
    "  batch_size=100,\n",
    "  # dynamically update the `batch_size` based on import speed\n",
    "  dynamic=False,\n",
    "  # `timeout_retries` takes an `int` value to retry on time outs\n",
    "  timeout_retries=3,\n",
    "  # checks for batch-item creation errors\n",
    "  # this is the default in weaviate-client >= 3.6.0\n",
    "  callback=weaviate.util.check_batch_result,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c78925c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': [{'message': 'update vector: send POST request: Post \"https://api.openai.com/v1/embeddings\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)'}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x10575f100>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': [{'message': 'update vector: send POST request: Post \"https://api.openai.com/v1/embeddings\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)'}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x10575f040>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': [{'message': 'update vector: send POST request: Post \"https://api.openai.com/v1/embeddings\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)'}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x10575f7c0>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': [{'message': 'update vector: send POST request: Post \"https://api.openai.com/v1/embeddings\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)'}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x105d5e260>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': [{'message': 'update vector: send POST request: Post \"https://api.openai.com/v1/embeddings\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)'}]}\n",
      "Uploaded 1181 documents in 611.8361110687256 seconds.\n"
     ]
    }
   ],
   "source": [
    "from weaviate.util import get_valid_uuid\n",
    "from uuid import uuid4\n",
    "import time\n",
    "start = time.time()\n",
    "for idx, blog_chunk in enumerate(blog_chunks):\n",
    "    data_properties = {\n",
    "        \"content\": blog_chunk\n",
    "    }\n",
    "    id = get_valid_uuid(uuid4())\n",
    "    with client.batch as batch:\n",
    "        batch.add_data_object(\n",
    "            data_properties,\n",
    "            \"WeaviateBlogChunk\"\n",
    "        )\n",
    "    '''\n",
    "    client.data_object.create(\n",
    "        data_object = data_properties,\n",
    "        class_name = \"WeaviateBlogChunk\",\n",
    "        uuid=id\n",
    "    )\n",
    "    '''\n",
    "\n",
    "print(f\"Uploaded {idx} documents in {time.time() - start} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30828110",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
