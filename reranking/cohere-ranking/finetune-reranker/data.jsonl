{"relevant_passages": ["That [distance can be calculated in multiple ways](/blog/distance-metrics-in-vector-search), one of the simplest being \"The sum of the absolute differences between elements at position `i` in each vector\" (recall that all vectors have the same fixed length). Let's look at some numbers (no math, promise!) and illustrate with another text example:\n\nObjects (data): words including `cat`, `dog`, `apple`, `strawberry`, `building`, `car`\n\nSearch query: `fruit`\n\nA set of simplistic vector embeddings (with only 5 dimensions) for the objects and the query could look something like this:\n\n| Word               | Vector embedding                |\n|--------------------|---------------------------------|\n| `cat`              | `[1.5, -0.4, 7.2, 19.6, 20.2]`  |\n| `dog`              | `[1.7, -0.3, 6.9, 19.1, 21.1]`  |\n| `apple`            | `[-5.2, 3.1, 0.2, 8.1, 3.5]`    |\n| `strawberry`       | `[-4.9, 3.6, 0.9, 7.8, 3.6]`    |\n| `building`         | `[60.1, -60.3, 10, -12.3, 9.2]` |\n| `car`              | `[81.6, -72.1, 16, -20.2, 102]` |\n| **Query: `fruit`** | `[-5.1, 2.9, 0.8, 7.9, 3.1]`    |\n\nIf we look at each of the 5 elements of the vectors, we can see quickly that `cat` and `dog` are much closer than `dog` and `apple` (we don\u2019t even need to calculate the distances). In the same way, `fruit` is much closer to `apple` and `strawberry` than to the other words, so those will be the top results of the \u201cfruit\u201d query. But where do these numbers come from? That\u2019s where the real magic is, and where advances in modern deep learning have made a huge impact."], "query": "Which words are most similar to the search query 'fruit' based on their vector embeddings?"}
{"relevant_passages": ["## Overview\n![Overview](./img/hugging-face-module-overview.png)\n\nThe Hugging Face module is quite incredible, for many reasons. ### Public models\nYou get access to over 1600 pre-trained [sentence similarity models](https://huggingface.co/models?pipeline_tag=sentence-similarity). No need to train your own models, if there is already one that works well for your use case. In case you struggle with picking the right model, see our blog post on [choosing a sentence transformer from Hugging Face](/blog/how-to-choose-a-sentence-transformer-from-hugging-face). ### Private models\nIf you have your own models, trained specially for your data, then you can upload them to Hugging Face (as private modules), and use them in Weaviate."], "query": "How many pre-trained sentence similarity models does Hugging Face offer?"}
{"relevant_passages": ["Sometimes when multiple batches contained identical objects with the same UUID, they could be added more than once to Weaviate, each time with different DocIDs. This, in turn, could cause issues within Weaviate. Luckily, we've addressed this issue without sacrificing performance (yay!\ud83e\udd73). Here's our journey that got us to the current solution. ## Our initial solutions\nIn the initial solution, we added a lock (sync.Mutex in Go), so that now only a single goroutine can hold the lock, check for duplicate UUIDs, and assign DocIDs. This lock makes sure that the race does not occur anymore, but as an unintended side-effect the import time increased by ~20% due to lock-congestion. Upon further consideration, our team concluded that while using a single lock is effective, it's also overkill."], "query": "What was the initial solution to prevent identical objects with the same UUID from being added multiple times to Weaviate, and what side-effect did it have?"}
{"relevant_passages": ["*Note. Hugging Face Inference uses [a pay-per-use pricing model](https://huggingface.co/inference-api#pricing).<br />\nMake sure to study it well before you run a big job.*\n\nTo learn more, head to the [HuggingFace Module docs page](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-huggingface). ## Other improvements and bug fixes\n\n![Other improvements and bug fixes](./img/smaller-improvements.jpg)\n\nAnd, of course, there are many other improvements and bug fixes that went into this release. You can find the complete list and the relevant links in the [release notes](https://github.com/weaviate/weaviate/releases/tag/v1.15.0). ## Enjoy\nWe hope you enjoy all the new features, performance improvements, memory savings and bug fixes that made this the best Weaviate release yet!\ud83d\udd25\n\nimport ShareFeedback from '/_includes/share-feedback.md';\n\n<ShareFeedback />"], "query": "Where can I find the pricing model for Hugging Face Inference API?"}
{"relevant_passages": ["---\ntitle: Weaviate 1.18 release\nslug: weaviate-1-18-release\nauthors: [jp, erika, zain, dan]\ndate: 2023-03-07\ntags: ['release']\nimage: ./img/hero.png\ndescription: \"Weaviate 1.18 introduces Faster Filtering through Bitmap Indexing, HNSW-PQ, Cursor API, and more! Learn all about it.\"\n---\n\nimport Core118 from './_core-1-18-include.mdx' ;\n\n<Core118 />\n\nimport WhatsNext from '/_includes/what-next.mdx'\n\n<WhatsNext />\n\nimport Ending from '/_includes/blog-end-oss-comment.md' ;\n\n<Ending />"], "query": "What features were introduced in the Weaviate 1.18 release on March 7, 2023?"}
{"relevant_passages": ["Just add one of the following properties to the `POST payload`:\n* `include` - an array class names we want to backup or restore\n* `exclude` - an array class names we don't want to backup or restore\n\nFor example, you can create a backup that includes Cats, Dogs and Meerkats. ```js\nPOST /v1/backups/gcs\n{\n  \"id\": \"first_backup\",\n  \"include\": [\"Cats\", \"Dogs\", \"Meerkats\"]\n}\n```\n\nThen restore all classes, excluding Cats:\n\n```js\nPOST /v1/backups/gcs/first_backup/restore\n{\n  \"exclude\": [\"Cats\"]\n}\n```\n\n### Other use cases\nIt might not be immediately obvious, but you can use the above workflow to migrate your data to other environments. So, if one day you find yourself with an environment that is not set up for what you need (i.e. not enough resources). Then create a backup, and restore it in the new environment. \ud83d\ude09\n\n### Follow up\nAre you ready to set up backups for your environment?"], "query": "How can I create a backup of specific classes using the POST payload in my API request?"}
{"relevant_passages": ["using a special algorithm, the database find the [closest](/blog/distance-metrics-in-vector-search) vectors to the given vector computed for the query. The quality of the search depends crucially on the quality of the model - this is the \"secret sauce\", as many models are [still closed source](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither). The speed of the search depends crucially on Weaviate, which is open-source and [continuously improving its performance](/blog/weaviate-1-18-release). ## What exactly are vector embeddings? Vectors are numeric representations of data that capture certain features of the data."], "query": "What factors are crucial for the quality and speed of vector searches in databases?"}
{"relevant_passages": ["Thanks to the advances in machine learning in the past decade and the commoditization of AI-first database technologies, you can start using it in your business tomorrow. import WhatNext from '/_includes/what-next.mdx'\n\n<WhatNext />"], "query": "What recent developments in AI technology can be applied to businesses today?"}
{"relevant_passages": ["If multiple recoveries were required, they would happen in sequence. On a large machine, this could mean that startup would be slow, yet only one of many CPU cores was utilized. - The data structure used to hold the recovered items was never intended to hold many items. Each additional insertion would degrade its performance. As a result, the larger the WAL to recover, the slower the recovery would become."], "query": "Why does the recovery process from a large WAL lead to slow startup times on a machine with multiple CPU cores?"}
{"relevant_passages": ["### Indexing Knobs\nFor the sake of designing RAG systems, the most important indexing knob looks like vector compression settings. Launched in March 2023, Weaviate 1.18 introduced Product Quantization (PQ). PQ is a vector compression algorithm that groups contiguous segments of a vector, clusters their values across the collection, and then reduces the precision with centroids. For example, a contiguous segment of 4 32-bit floats requires 16 bytes to represent, a segment length of 4 with 8 centroids results in only needing 1 byte, a 16:1 memory reduction. Recent advances in PQ Rescoring help significantly with recall loss from this compression, but is still an important consideration with very high levels of compression."], "query": "What vector compression algorithm was introduced in Weaviate 1.18 to reduce memory usage for RAG systems, and what is its impact on recall?"}
{"relevant_passages": ["[Cross Encoders](#cross-encoders) (collapsing the use of Large Language Models for ranking into this category as well)\n1. [Metadata Rankers](#metadata-rankers)\n1. [Score Rankers](#score-rankers)\n\n## Cross Encoders\nCross Encoders are one of the most well known ranking models for content-based re-ranking. There is quite a collection of pre-trained cross encoders available on [sentence transformers](https://www.sbert.net/docs/pretrained_cross-encoders.html). We are currently envisioning interfacing cross encoders with Weaviate using the following syntax."], "query": "What are Cross Encoders, and how can they be interfaced with Weaviate?"}
{"relevant_passages": ["This process is known as \u201cde-noising\u201d, is illustrated below and, is carried out for each image in the training set with multiple levels of random noise added. Once the diffusion model is trained in this way it becomes an expert at taking images that are less likely to be seen in the dataset (noisy images) and incrementally turning them into something that is more likely to be seen in the training set. By teaching the model to \u201cde-noise\u201d images we have developed a way to alter images to make them more like images from the training set. ![denoising gif](./img/denoise.gif)\n*[Source](https://yang-song.net/blog/2021/score/)*\n\n![denoising images](./img/denoisingimage.png)\n*[Source](https://huggingface.co/blog/annotated-diffusion)*\n\nNow if we take this trained diffusion model and just give it a random static image and run the de-noising process it will transform the static image into an image that resembles images in the training set!\n\n![noising denoising images](./img/noising_denoising.png)\n\n## How Text Prompts Control the Image Generation Process\n\nSo far we have explained the general idea behind how diffusion models can start off from static noise and incrementally alter the pixel values so that the picture all together gains meaning and follows the distribution of the training set. However, another important detail is that most diffusion models don\u2019t just spit out random images that look like training set images, they allow us to add a text prompt that can control the specific types of images are generated."], "query": "What is the purpose of the \"de-noising\" process in diffusion models, and how does it relate to generating images from text prompts?"}
{"relevant_passages": ["The following image presents a high-level illustration of chunking text. ![chunking](img/chunk.png)\n\n### Retrieval \nThere are four major knobs to tune in Retrieval: Embedding models, Hybrid search weighting, whether to use AutoCut, and Re-ranker models. Most RAG developers may instantly jump to tuning the embedding model used, such as OpenAI, Cohere, Voyager, Jina AI, Sentence Transformers, and many others! Developers also need to consider the dimensionality of the models and how it affects the PQ compression. The next key decision is how to weight the aggregation of sparse and dense retrieval methods in Hybrid Search. The weighting is based on the `alpha` parameter."], "query": "What are the four major knobs to tune in Retrieval for RAG development?"}
{"relevant_passages": ["If you would like to follow along, the Jupyter notebook and data are available [here](https://github.com/weaviate/weaviate-examples/tree/main/text2vec-behind-curtain). You can use our free [Weaviate Cloud Services](https://console.weaviate.cloud) (WCS) sandbox, or set up your own Weaviate instance also. > Note: The vectorization is done by [Weaviate \u201ccore\u201d](https://github.com/weaviate/weaviate) and not at the client level. So even though we use Python examples, the principles are universally applicable. Let's get started!\n\n## Text2vec: behind the scenes\n\nWeaviate's `text2vec-*` modules transform text data into dense vectors for populating a Weaviate database."], "query": "Where can I find the Jupyter notebook and data to learn about Weaviate's text vectorization, and where is the vectorization process executed?"}
{"relevant_passages": ["This includes [improved APIs](https://github.com/weaviate/weaviate-python-client/issues/205) on the client side, new modules, for example, for [generative search](/developers/weaviate/modules/reader-generator-modules/generative-openai), and improvements to our existing modules. <br></br>\n\n### Community\n![community](./img/community.png)\n\nThe most important pillar is all of you \u2013 our community. This includes both free, open-source users that self-host their Weaviate setup, as well as paid enterprise users and anyone using our Weaviate-as-a-Service offerings. We value your feedback and love that you are part of shaping our future. Last year we introduced our [dynamic roadmap page](/developers/weaviate/roadmap) that allows you to create and upvote your favorite feature requests."], "query": "What new features were added to the Weaviate Python client as mentioned in the document?"}
{"relevant_passages": ["## Exploring the Power of Vector Databases\n\nThe year 2023 was all about dynamic experimentation at Weaviate. Vector databases became a strong and recognized foundation in building ever more effective AI applications, enabling **chatbots,** **agents,** and **advanced** **search systems**. ### Online Hackathons\n\nOur 2023 global online hackathons proved to be vibrant innovation hubs, fostering diversity and inclusion in collaborative work. We teamed up with friends from [Cohere](https://cohere.com/), [LangChain](https://www.langchain.com/), [AutoGPT](https://autogpt.net/), [lablab.ai](https://lablab.ai/), [SuperAGI](https://superagi.com/), and many others. ![hackathons](img/hackathons.png)\n\n### In-person Hackathons\nWhether you're a beginner just diving into the world of coding, a passionate AI enthusiast, or a seasoned expert in the field, in-person events create a burst of energy and creativity into everyone's personal AI journey."], "query": "What advancements did Weaviate make in the field of AI applications in 2023?"}
{"relevant_passages": ["---\ntitle: Why is Vector Search so fast? slug: why-is-vector-search-so-fast\nauthors: [laura]\ndate: 2022-09-13\ntags: ['search']\nimage: ./img/hero.png\ndescription: \"Vector Databases can run semantic queries on multi-million datasets in milliseconds. How is that possible?\"\n---\n![Why is Vector Search so fast?](./img/hero.png)\n\n<!-- truncate -->\n\n## Why is this so incredibly fast? Whenever I talk about vector search, I like to demonstrate it with an example of a semantic search. To add the wow factor, I like to run my queries on a Wikipedia dataset, which is populated with over 28 million paragraphs sourced from Wikipedia."], "query": "How can vector databases perform semantic searches on datasets as large as Wikipedia's 28 million paragraphs so quickly?"}
{"relevant_passages": ["We were able to import 200 million objects and more, while the import performance remained constant throughout the process. [See more on github](https://github.com/weaviate/weaviate/pull/1976). ### Drastically improved Mean-Time-To-Recovery (MTTR)\nWeaviate `1.14` fixes an issue where a crash-recovery could take multiple minutes, or even hours in some extreme cases. It is now a matter of just seconds. So even in the rare event that your instance crashes, it will come back up very quickly."], "query": "What improvements were made to Weaviate's Mean-Time-To-Recovery in version 1.14?"}
{"relevant_passages": ["Taken directly from the paper, \u201cOur findings indicate that cross-encoder re-rankers can efficiently be improved without additional computational burden and extra steps in the pipeline by explicitly adding the output of the first-stage ranker to the model input, and this effect is robust for different models and query types\u201d. Taking this a bit further, [Dinh et al.](https://arxiv.org/abs/2206.06565) shows that most tabular machine learning tasks can be translated to text and benefit from transfer learning of text-based models. Many of these metadata rankers may also take in something like a collaborative filtering score that is based on this user\u2019s history, as well as other users on the platform \u2014 another interesting feature to think of interfacing this way. The main point being, maybe we can just add these meta features to our [query, document] representation and keep the Zero-Shot party going. We recently had an interesting discussion about metadata ranking and future directions for ranking models broadly on our latest Weaviate podcast! \ud83d\udc49 Check it out [here](https://www.youtube.com/watch?v=aLY0q6V01G4)\n\n## Score Rankers\nScore rankers describe using either a classifier to detect things, or a regression model to score things, about our candidate documents to rank with."], "query": "How can cross-encoder re-rankers be improved without additional computational costs according to recent research findings?"}
{"relevant_passages": ["When compressing a vector we find the closest centroid per segment and return an array of bytes with the indexes of the centroids per segment. When decompressing a vector, we concatenate the centroids encoded by each byte on the array. The explanation above is a simplification of the complete algorithm, as we also need to be concerned about performance for which we need to address duplication of calculations, synchronization in multithreading and so on. However what we have covered above should be sufficient to understand what you can accomplish using the PQ feature released in v1.18. Let's see some of the results HNSW implemented with PQ in Weaviate can accomplish next! If you're interested in learning more about PQ you can refer to the documentation [here](/developers/weaviate/concepts/vector-index#hnsw-with-product-quantizationpq)."], "query": "What is the PQ feature in Weaviate v1.18 used for in vector compression?"}
{"relevant_passages": ["Take the longer start-up time for example. Adding replication caused node-level start-up time to increase in our experiment. But the end result was that a hundred percent of requests succeeded. In other words, the end user would not have noticed anything was going on. And again, what are peace of mind and avoiding the wrath of angry users during downtime worth to you??"], "query": "What was the impact on node-level start-up time and request success rate after adding replication in the experiment?"}
{"relevant_passages": ["Due to its relatively high memory footprint, HNSW is only cost-efficient in high-throughput scenarios. However, HNSW is inherently optimized for in-memory access. Simply storing the index or vectors on disk or memory-mapping the index kills performance. This is why we will offer you not just one but two memory-saving options to index your vectors without sacrificing latency and throughput. In early 2023, you will be able to use Product Quantization, a vector compression algorithm, in Weaviate for the first time."], "query": "When will Product Quantization be available in Weaviate for vector compression?"}
{"relevant_passages": ["Now armed with queries, answers, gold documents, and negatives, ARES fine-tunes lightweight classifiers for **context relevance**, **answer faithfulness**, and **answer relevance**. The authors experiment with fine-tuning [DeBERTa-v3-large](https://huggingface.co/microsoft/deberta-v3-large), which contains a more economical 437 million parameters, with each classifier head sharing the base language model, adding 3 total classification heads. The ARES system is then evaluated by dividing the synthetic data into a train-test split and comparing the fine-tuned judges with zero-shot and few-shot GPT-3.5-turbo-16k judges, finding that the fine-tuned models perform significantly better. For further details, such as a novel use of confidence intervals with prediction powered inference (PPI) and more experimental details, please see the paper from [Saad-Falcon et al](https://arxiv.org/abs/2311.09476). To better understand the potential impact of LLMs for evaluation, we will continue with a tour of the existing methods for benchmarking RAG systems and how they are particularly changed with LLM Evaluation."], "query": "What system uses DeBERTa-v3-large to fine-tune classifiers for context relevance, answer faithfulness, and answer relevance, and is compared with GPT-3.5-turbo-16k judges?"}
{"relevant_passages": ["### Stuffing\n\n<img\n    src={require('./img/stuffing.gif').default}\n    alt=\"alt\"\n    style={{ width: \"100%\" }}\n/>\n\nStuffing takes the related documents from the database and stuffs them into the prompt. The documents are passed in as context and go into the language model (the robot). This is the simplest method since it doesn\u2019t require multiple calls to the LLM. This can be seen as a disadvantage if the documents are too long and surpass the context length. ### Map Reduce\n\n<img\n    src={require('./img/map-reduce.gif').default}\n    alt=\"alt\"\n    style={{ width: \"100%\" }}\n/>\n\nMap Reduce applies an initial prompt to each chunk of data."], "query": "What is the method called that involves inserting related documents directly into the prompt for a language model, and what is its main limitation?"}
{"relevant_passages": ["The problem is that moving vectors to disk would have higher latency costs since we would then need lots of disk reads. The proposed solution by [DiskANN](https://suhasjs.github.io/files/diskann_neurips19.pdf) is to store large complete representations of vectors on disk and keep a compressed representation of them in memory. The compressed representation is used to sort the vectors while searching for the nearest neighbors, and the complete representation is fetched from disk every time we need to explore a new vector from the sorted list. In plain English, we start our search from our root in the graph. From there, we get a set of neighbor candidates."], "query": "What technique does DiskANN use to reduce latency when searching for nearest neighbors with vectors stored on disk?"}
{"relevant_passages": ["Your answer to the question must be grounded in the provided search results and nothing else!!\u201d. As described earlier, Few-Shot Examples describes collecting a few manually written examples of question, context, answer pairs to guide the language model\u2019s generation. Recent research such as [\u201cIn-Context Vectors\u201d](https://arxiv.org/abs/2311.06668) are further pointing to the importance of guiding latent space like this. We were using GPT-3.5-turbo to generate Weaviate queries in the Weaviate Gorilla project and performance skyrocketed once we added few-shot examples of natural language to query translations. Lastly, there is increasing interest in fine-tuning LLMs for RAG applications."], "query": "What impact did few-shot examples have on the performance of GPT-3.5-turbo in the Weaviate Gorilla project?"}
{"relevant_passages": ["---\ntitle: Authentication in Weaviate (videos)\nslug: authentication-in-weaviate\nauthors: [jp]\ndate: 2023-04-25\nimage: ./img/hero.png\ntags: ['concepts']\ndescription: \"Videos on authentication: an overview, how to log in, how to set it up, and core concepts - including recommendations.\"\n\n---\n\n![Authentication in Weaviate](./img/hero.png)\n\n<!-- truncate -->\n\nimport ReactPlayer from 'react-player/lazy'\n\n## Overview\n\nAuthentication is one of those topics that we get quite a few questions about. And we can see why. It's a big, complex topic, and even within Weaviate, there are many options available which can make it seem quite confusing. The core concept of authentication is relatively simple. When a client (e.g. a Weaviate client) sends a request to a server (e.g. a Weaviate database), it includes a \"secret\" that provides some assurances to Weaviate as to who that request is coming from, so that it can operate on that information."], "query": "What are the core concepts of authentication in Weaviate, and where can I find videos explaining how to set it up and log in?"}
{"relevant_passages": ["An `alpha` of 0 is pure bm25 and an alpha of 1 is pure vector search. Therefore, the set `alpha` is dependent on your data and application. Another emerging development is the effectiveness of zero-shot re-ranking models. Weaviate currently offers 2 [re-ranking models from Cohere](https://weaviate.io/developers/weaviate/modules/retriever-vectorizer-modules/reranker-cohere): `rerank-english-v2.0` and `rerank-multilingual-v2.0`. As evidenced from the name, these models mostly differ because of the training data used and the resulting multilingual capabilities."], "query": "What are the two re-ranking models from Cohere that Weaviate offers for search customization?"}
{"relevant_passages": ["So let's look behind the curtain, and see if we can reproduce the magic. More specifically, let's try to reproduce Weaviate's output vector for each object by using an external API. ![pulling back the curtains](./img/pulling-back-the-curtains-text2vec.png)\n\n### Matching Weaviate's vectorization\n\nWe know that the vector for each object corresponds to its text. What we don't know is how, exactly. As each object only contains the two properties, `question` and `answer`, let's try concatenating our text and comparing it to Weaviate's."], "query": "How can I reproduce Weaviate's output vector for an object using an external API?"}
{"relevant_passages": ["If we compress the vectors then the memory requirements goes down to the 4730 MB to 12367 MB range. After compression, recall drops to values ranging from 0.8566 to 0.9702. Latency rises up to the 1039 to 2708 microsends range. For Gist we would require roughly 4218 MB to 5103 MB of memory to index our data using uncompressed HNSW. This version would give us recall ranging from 0.7446 to 0.9962 and latencies ranging from 2133 to 15539 microseconds."], "query": "What is the range of memory requirements for compressed vectors according to the document?"}
{"relevant_passages": ["6**: *Time (min) to fit the Product Quantizer with 200,000 vectors and to encode 9,990,000 vectors, all compared to the recall achieved. The different curves are obtained varying the segment length (shown in the legend) from 1 to 6 dimensions per segment. The points in the curve are obtained varying the amount of centroids.*\n\n![res4](./img/image8.png)\n**Fig. 7**: *Average time (microseconds) to calculate distances from query vectors to all 9,990,000 vectors compared to the recall achieved. The different curves are obtained varying the segment length (shown in the legend) from 1 to 6 dimensions per segment."], "query": "How does varying the segment length affect the time efficiency and recall when using a Product Quantizer with 200,000 vectors for fitting and 9,990,000 vectors for encoding?"}
{"relevant_passages": ["### The issue\nFast forward, we identified two key issues. First, the Go library that we used for reading binary data (`binary.read`) isn't optimized for how we use it in Weaviate, as it makes many temporary memory allocations. Second, for every object in the aggregation, we would allocate new memory on the heap, process the read, and release the memory. <!-- TODO: add a picture with cakes -->\nThis is a bit like, if we want to eat a cake, we need to put it on a plate, eat the cake and then put the plate in the sink to wash. Now, if we want to eat a million cakes, we will be either very busy washing dishes or have a million plates in the sink (or even run out of plates).<br/>\nI am sure you would rather spend more time eating cakes than dealing with plates."], "query": "What are the two key issues with the Go library and memory allocation in Weaviate as described in the document?"}
{"relevant_passages": ["Finally, we can jump on a bike to reach our local destination. For a better understanding, consider the below graphic, which shows a graph with all the connections generated using 1000 objects in two dimensions. <img\n    src={require('./img/vamana-graph.png').default}\n    alt=\"Vamana graph with 1000 objects\"\n    style={{ maxWidth: \"50%\" }}\n/>\n\nIf we iterate over it in steps \u2013 we can analyze how Vamana navigates through the graph. <img\n    src={require('./img/vamana-graph-animated.gif').default}\n    alt=\"Vamana graph - animated in 3/6/9 steps\"\n    style={{ maxWidth: \"50%\" }}\n/>\n\nIn the **first step**, you can see that the entry point for the search is in the center, and then the long-range connections allow jumping to the edges. This means that when a query comes, it will quickly move in the appropriate direction.<br/>\nThe **second**, **third**, and **final steps** highlight the nodes reachable within **three**, **six**, and **nine** hops from the entry node."], "query": "How does the Vamana algorithm navigate through a graph with 1000 objects in steps?"}
{"relevant_passages": [":::info Glossary\n- **Node**: A single machine in a cluster. It often refers to a physical or virtual machine that runs part of an application or service. - **Pod**: A Kubernetes term for a group of one or more containers, with shared storage/network, and a specification for how to run the containers. Pods are the smallest deployable units in Kubernetes. - **Tenant**: In the context of Weaviate, an isolated environment or subsection within the system, designed to separate data and access between different end users or groups."], "query": "What is the definition of a \"Pod\" in Kubernetes?"}
{"relevant_passages": ["Some models, such as [CLIP](https://openai.com/blog/clip/), are capable of vectorizing multiple data types (images and text in this case) into one vector space, so that an image can be searched by its content using only text. ## Vector embeddings with Weaviate\n\nFor this reason, Weaviate is configured to support many different vectorizer models and vectorizer service providers. You can even [bring your own vectors](/developers/weaviate/starter-guides/custom-vectors), for example if you already have a vectorization pipeline available, or if none of the publicly available models are suitable for you. For one, Weaviate supports using any Hugging Face models through our `text2vec-hugginface` module, so that you can [choose one of the many sentence transformers published on Hugging Face](/blog/how-to-choose-a-sentence-transformer-from-hugging-face). Or, you can use other very popular vectorization APIs such as OpenAI or Cohere through the [`text2vec-openai`](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-openai) or [`text2vec-cohere`](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-cohere) modules."], "query": "What vectorization models and service providers does Weaviate support, and can it integrate with Hugging Face, OpenAI, or Cohere?"}
{"relevant_passages": ["The company preferred an open source platform, and since they would be indexing millions of products, they needed a solution that was both high-performing and cost-efficient at scale. ## Selecting Weaviate as the vector database of choice \nAfter a thorough evaluation of a handful of open and closed-source vector databases, the team decided that Weaviate was the best-fit solution for their needs. They cited the following reasons for choosing Weaviate:  \n\n* Open source, with an active community and managed cloud offering. * Comprehensive documentation and strong support for popular LLMs and multi-modal models. * Direct integration of machine learning models using a module system, with the ability to easily swap out and experiment with different models."], "query": "Why did the company choose Weaviate as their vector database solution?"}
{"relevant_passages": ["As we have mentioned before, this is not the final solution where we still do not move information to disk. Our final solution would use a compressed version of the vectors to guide the exploration and fetch the fully described data (vectors and graph) from disk as needed. With the final approach we will achieve better compression rate but also better recall since the uncompressed vectors will be used to correct the compression distortion. A final remark. Notice how the compression rate is better on Gist."], "query": "What is the proposed final solution for handling data that involves compression and disk storage as mentioned in the document?"}
{"relevant_passages": ["The schema is the place to define, among other things, the data type and vectorizer to be used, as well as cross-references between classes. As a corollary, the vectorization process can be modified for each class by setting the relevant schema options. In fact, you can [define the data schema](/developers/weaviate/manage-data/collections) for each class individually. All this means that you can also use the schema to tweak Weaviate's vectorization behavior. The relevant variables for vectorization are `dataType` and those listed under `moduleConfig` at both the class level and property level."], "query": "How can I customize the vectorization process for a class in Weaviate's schema?"}
{"relevant_passages": ["Later on you will see an example of how the partitioning bricks identified the elements in a research paper. Cleaning the data is an important step before passing it to an NLP model. The cleaning brick can \u2018sanitize\u2019 your text data by removing bullet points, extra whitespaces, and more. Staging is the last brick and it helps to prepare your data as input into downstream systems. It takes a list of document elements as input and returns a formatted dictionary as output."], "query": "What are the functions of partitioning bricks, cleaning brick, and staging in the context of preparing data for an NLP model?"}
{"relevant_passages": ["The idea behind conditioning the images on a text prompt requires another model, one that is trained on images along with their captions. Examples of this data are shown below:\n\n![MS COCO Image Caption Dataset](./img/mscoco.png)\n\nThis model learns to relate descriptions of an image in text format with the image representation itself. In doing so it gives us a way to represent our written prompts as vectors that also capture the visual meaning behind the prompt. We can then pass these prompt vectors into our diffusion model along with the noised images during the training process. This allows us to tame the image generation process of the diffusion model by specifying to the model what types of images in the training set to resemble as it alters the pixels step by step."], "query": "How are text prompts used to guide image generation in a diffusion model?"}
{"relevant_passages": ["Let us know in the comments below!\n\n\nimport WhatNext from '/_includes/what-next.mdx'\n\n<WhatNext />"], "query": "What component is included for user interaction in the comments section of a web page?"}
{"relevant_passages": ["I recommend checking out the GitHub repository to test this out yourself!\n\n## Additional Resources\n\u2022 [LangChain Guide](https://www.commandbar.com/blog/langchain-projects) by Paul from CommandBar. import StayConnected from '/_includes/stay-connected.mdx'\n\n<StayConnected />"], "query": "Where can I find the LangChain guide by Paul from CommandBar?"}
{"relevant_passages": ["## Q&A style questions on your own dataset answered in milliseconds\nWeaviate now allows you to get to sub-50ms results by using transformers on your own data. You can learn more about Weaviate\u2019s speed in combination with transformers in [this article](https://towardsdatascience.com/a-sub-50ms-neural-search-with-distilbert-and-weaviate-4857ae390154). import WhatNext from '/_includes/what-next.mdx'\n\n<WhatNext />"], "query": "How can Weaviate achieve sub-50ms search results with transformers?"}
{"relevant_passages": ["This means that holding the vectors in memory requires 1,000,000 x 128 x 4 bytes = 512,000,000 bytes. Additionally, a graph representation of neighborhoods is built when indexing. The graph represents the k-nearest neighbors for each vector. To identify each neighbor we use an `int64`, meaning we need 8 bytes to store each of the k-nearest neighbors per vector. The parameter controlling the size of the graph is `maxConnections`."], "query": "How much memory is required to store the k-nearest neighbors for each vector if each neighbor is identified by an int64?"}
{"relevant_passages": ["For example, I can query Weaviate for articles related to: \"urban planning in Europe\", and the vector database (in the case of my demo \u2013 [Weaviate](/developers/weaviate/)) responds with a series of articles about the topic, such as \"The cities designed to be capitals\".<br/>\nYou can try it for yourself by following this [link](https://link.weaviate.io/3LiVxqp), which is already pre-populated with the above question. Press the play button, to see the magic happen. Here is the thing, finding the correct answer in a gigantic repository of unstructured data is not the most impressive part of this demonstration (I mean, it is very impressive), but it is the \ud83d\ude80 speed at which it all happens. It takes a fraction of a second for the UI to show the results. We are talking about a semantic search query, which **takes milliseconds** to find an answer in a dataset containing **28 million paragraphs**."], "query": "How fast can Weaviate perform a semantic search on a dataset of 28 million paragraphs for articles about urban planning in Europe?"}
{"relevant_passages": ["### Improved performance for large scale data imports\nWeaviate `1.14` significantly improves the performance of data imports for large datasets. Note that the performance improvements should be noticeable for imports of over 10 million objects. Furthermore, this update enables you to import over 200 million objects into the database. #### Problem\nBefore, the HNSW index would grow in constant intervals of 25,000 objects. This was fine for datasets under 25 million objects."], "query": "What improvements does Weaviate version 1.14 offer for large scale data imports?"}
{"relevant_passages": ["At Weaviate, we pride ourselves on our research acumen and on providing state-of-the-art solutions. So we took time to explore these solutions to identify and evaluate the right building blocks for Weaviate's future. Here we share some of our findings from this research. ## On the HNSW vs. Vamana comparison\nAs the first step to disk-based vector indexing, we decided to explore Vamana \u2013 the algorithm behind the DiskANN solution."], "query": "Which algorithm did Weaviate explore as a first step towards disk-based vector indexing?"}
{"relevant_passages": ["Accuracy is measured based on Recall. Recall in vector indexing measures how many of the ground truth nearest neighbors determined by brute force are returned from the approximate indexing algorithm. This is distinct from how \u201cRecall\u201d is typically used in Information Retrieval to reference how many of the relevant documents are returned from the search. Both are typically measured with an associated @K parameter. The interesting question in the full context of a RAG stack is then: **When do ANN accuracy errors manifest in IR errors?** For example, we may be able to get 1,000 QPS at 80% recall versus 500 QPS at 95% recall, what is the impact of this on the search metrics presented above such as Search nDCG or an LLM Recall score?"], "query": "What is the impact of different ANN recall rates on search metrics in a RAG stack?"}
{"relevant_passages": ["<figure>\n\n![vectorspace](./img/vectorspace.png)\n<figcaption>Figure 1. Shows a joint embedding space of a multimodal model that understands both text and images. Notice how objects that are similar are closer together and dissimilar objects are farther apart, this means that the model preserves semantic similarity within and across modalities.</figcaption>\n\n</figure>\n\n## Addressing Challenges of Learning Multimodal Embeddings\n\n### 1. Lack of Rich & Aligned Multimodal Datasets\n\nCollecting and preparing multimodal datasets is very challenging. Each modality requires specific data collection techniques and preprocessing steps."], "query": "What is a challenge in collecting and preparing multimodal datasets for learning embeddings?"}
{"relevant_passages": ["### Generation\nWhen it comes to Generation, the obvious first place to look is the choice of LLM. For example, you have options from OpenAI, Cohere, Facebook, and many open-source options. It is also helpful that many LLM frameworks like [LangChain](https://www.langchain.com/) and [LlamaIndex](https://www.llamaindex.ai/), and [Weaviate\u2019s generate module](/developers/weaviate/modules/reader-generator-modules/generative-openai) offer easy integrations into various models. The model that you choose can be dependent on whether you want to keep your data private, the cost, resources, and more. A common LLM specific knob that you can tune is the temperature."], "query": "What are some frameworks that provide easy integration with various language learning models?"}
{"relevant_passages": ["If we compress the vectors then the memory requirements goes down to 610 MB to 1478 MB range. After compression recall, drops to values ranging from 0.9136 to 0.9965. Latency rises up to the 401 to 1937 microsends range. For DeepImage96 we would require roughly 9420 MB to 15226 MB of memory to index our data using uncompressed HNSW. This version would give us recall ranging from 0.8644 to 0.99757 and latencies ranging from 827 to 2601 microseconds."], "query": "What is the range of memory requirements for indexing data with compressed vectors compared to using uncompressed HNSW for DeepImage96?"}
{"relevant_passages": ["3**: *We are compressing a 128 dimensions vector into a 32 bytes compressed vector. For this, we define 32 segments meaning the first segment is composed of the first four dimensions, the second segment goes from dimension 5th to 8th and so on. Then for each segment we need a compression function that takes a four dimensional vector as an input and returns a byte representing the index of the center which best matches the input. The decompression function is straightforward, given a byte, we reconstruct the segment by returning the center at the index encoded by the input.*\n\nA straightforward encoding/compression function uses KMeans to generate the centers, each of which can be represented using an id/code and then each incoming vector segment can be assigned the id/code for the center closest to it. Putting all of this together, the final algorithm would work as follows: Given a set of N vectors, we segment each of them producing smaller dimensional vectors, then apply KMeans clustering per segment over the complete data and find 256 centroids that will be used as predefined centers."], "query": "How can a 128-dimensional vector be compressed into a 32-byte representation using segmentation and KMeans clustering?"}
{"relevant_passages": ["Then when a query arrives, Weaviate traverses the index to obtain a good approximated answer to the query in a fraction of the time that a brute-force approach would take. [HNSW](/developers/weaviate/concepts/vector-index#hnsw) is the first production-ready indexing algorithm we implemented in Weaviate. It is a robust and fast algorithm that builds a hierarchical representation of the index **in memory** that could be quickly traversed to find the k nearest neighbors of a query vector. ## Need for disk solutions\nThere are other challenges to overcome. Databases have grown so fast that even the above-described algorithms will not be enough."], "query": "What indexing algorithm does Weaviate use to quickly traverse its index in memory?"}
{"relevant_passages": ["<video width=\"100%\" autoplay loop controls>\n  <source src={hacktober_demo} type=\"video/mp4\" />\nYour browser does not support the video tag. </video>\n\n\n2. Ping us on the project\u2019s issue, saying you're interested and which parts of the issue you would like to contribute to. 3. Open the PR as instructed in the [Weaviate Contributor Guide](https://weaviate.io/developers/contributor-guide)."], "query": "How do I contribute to the Weaviate project during Hacktoberfest?"}
{"relevant_passages": ["The PQ centroids fit with the first K vectors that enter Weaviate may be impacted by a significant shift in the data distribution. The continual training of machine learning models has a notorious \u201ccatastrophic forgetting\u201d problem where training on the newest batch of data harms performance on earlier batches of data. This is also something that we are considering with the design of re-fitting PQ centroids. ## From RAG to Agent Evaluation\nThroughout the article, we have been concerned with **RAG**, rather than **Agent** Evaluation. In our view **RAG** is defined by the flow of Index, Retrieve, and Generate, whereas **Agents** have a more open-ended scope."], "query": "What is the impact of data distribution shifts on the fitting of PQ centroids in Weaviate, and how does it relate to catastrophic forgetting in machine learning?"}
{"relevant_passages": ["For this experiment we have added 200,000 vectors using the normal HNSW algorithm, then we switched to compressed and added the remaining 800,000 vectors.*\n\n![perf2](./img/image13.png)\n**Fig. 12**: *The chart shows Recall (vertical axis) Vs Indexing time (in minutes, on the horizontal axis). For this experiment we have added 200,000 vectors using the normal HNSW algorithm, then we switched to compressed and added the remaining 800,000 vectors.*\n\n![perf3](./img/image14.png)\n**Fig. 13**: *The chart shows Recall (vertical axis) Vs Latency (in microseconds, on the horizontal axis). For this experiment we have added 200,000 vectors using the normal HNSW algorithm, then we switched to compressed and added the remaining 800,000 vectors.*\n\n![perf4](./img/image15.png)\n**Fig."], "query": "How many vectors were added using the normal HNSW algorithm and the compressed method in the experiment, and what do the charts in the figures illustrate?"}
{"relevant_passages": ["Partitioning 2. Cleaning, 3. Staging. Partitioning bricks take an unstructured document and extract structured content from it. It takes the document and breaks it down into elements like `Title`, `Abstract`, and `Introduction`."], "query": "What process is used to extract structured content like `Title`, `Abstract`, and `Introduction` from an unstructured document?"}
{"relevant_passages": ["### Implementation observations\n\nWe initially implemented the Vamana algorithm as described, resulting in very good recall results. Yet the latency was not good at all. We have since realized that the performance decay was due to many set operations making the algorithm perform poorly as is. In our revised implementation, we have modified the algorithm a bit to keep a copy of visited and current nodes on a single sorted list. Also, as the parameter L grows, the search on the sets becomes more expensive, so we already decided to keep a bit-based representation of the vectors residing on the sets, which made a huge impact performance-wise."], "query": "What modifications were made to the original Vamana algorithm to improve its performance in terms of latency?"}
{"relevant_passages": ["But once the database got to around 25 million objects, adding new objects would be significantly slower. Then from 50\u2013100m, the import process would slow down to a walking pace. #### Solution\nTo address this problem, we changed how the HNSW index grows. We implemented a relative growth pattern, where the HNSW index size increases by either 25% or 25'000 objects (whichever is bigger). ![HNSW index growth chart](./img/hnsw-index-growth.jpg)\n\n#### Test\nAfter introducing the relative growth patterns, we've run a few tests."], "query": "What solution was implemented to improve the performance of adding new objects to a database with over 25 million objects?"}
{"relevant_passages": ["This will allow us to package the backup and run Weaviate in the last step directly from the backup. In the environment variables, we set a CLUSTER_HOSTNAME, an arbitrary name you can set to identify a cluster. ```yaml\nenvironment:\n  TRANSFORMERS_INFERENCE_API: 'http:loadbalancer:8080'\n  QUERY_DEFAULTS_LIMIT: 25\n  AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'\n  PERSISTENCE_DATA_PATH: '/var/lib/weaviate'\n  DEFAULT_VECTORIZER_MODULE: 'text2vec-transformers'\n  ENABLE_MODULES: 'text2vec-transformers'\n  CLUSTER_HOSTNAME: '63e2f234026d'\n```\n\n*Docker environment setup*\n\nWe will also set the location of the volume outside Weaviate, in this case the data will be stored in the /var/weaviate folder\n\n```yaml\nvolumes:\n  - /var/weaviate:/var/lib/weaviate\n```\n\n*Volumes for backup*\n\nYou can find the complete Docker Compose file we've used here. ## Query the Data\nThe current Weaviate setup has two modules enabled: semantic search and Q&A. The modules can be used for different types of queries."], "query": "What is the value of the CLUSTER_HOSTNAME environment variable in the Weaviate Docker setup?"}
{"relevant_passages": ["He needed to implement semantic search, a system that could interpret a user\u2019s intent rather than rely on exact keyword matches. And they needed a system that could index and search multimodal (text and image) data across millions of objects. Based on prior experience with semantic search and vector embeddings, Marais decided a vector database was the tool for the job. Marais quickly put together a checklist of priorities for their vector database selection. Moving fast was important, so ease of use was at the top of the list."], "query": "What were Marais's priorities when selecting a vector database for semantic search and multimodal data indexing?"}
{"relevant_passages": ["Latency when retrieving the hundred approximate nearest neighbors.*\n\n## Vamana implementation details\nWe have also included a development implementation of the Vamana indexing algorithm in Weaviate. For the algorithm to perform well, such an implementation needs careful attention to the optimization of the code. The original algorithm from Microsoft rests upon the greedy search and the robust prune methods, which are described in the [DiskANN paper](https://suhasjs.github.io/files/diskann_neurips19.pdf) as follows:\n\n![Vamana algorithm](./img/vamana-algorithm.png)\n\n### In plain English\nThese pseudo-code snippets are notoriously difficult to read, so here's a plain-English explanation of how Vamana works. The greedy search algorithm is used to find the solution for a query. The main idea is to start looking for the best points iteratively from the entry point."], "query": "What algorithm does Weaviate use to optimize the retrieval of the hundred approximate nearest neighbors?"}
{"relevant_passages": ["Here are some key differences between Vamana and HNSW:\n\n### Vamana indexing - in short:\n* Build a random graph. * Optimize the graph, so it only connects vectors close to each other. * Modify the graph by removing some short connections and adding some long-range edges to speed up the traversal of the graph. ### HNSW indexing - in short:\n* Build a hierarchy of layers to speed up the traversal of the nearest neighbor graph. * In this graph, the top layers contain only long-range edges."], "query": "What are the key differences between Vamana and HNSW indexing methods?"}
{"relevant_passages": [":::note Proposed feature\nTo reduce the length of this time, there is a proposed feature to proactively start repairing those inconsistencies (i.e. perform asynchronous replication). If this is important, please [upvote the feature here](https://github.com/weaviate/weaviate/issues/2405). :::\n\nBut we think that the cost of high availability is worth these prices. We take system availability seriously, and architect Weaviate according to this philosophy. This is one of the reasons that we use [leaderless replication](/developers/weaviate/concepts/replication-architecture/cluster-architecture#leaderless-design), and why replication in the first place is so important to us - because it enables our users to have robust systems on which they can rely."], "query": "What is the proposed feature in Weaviate to handle repairing inconsistencies, and how can users show their support for it?"}
{"relevant_passages": ["If we use `maxConnections = 64` when indexing Sift1M we end up with 1,000,000 x 64 x 8 bytes = 512,000,000 bytes also for the graph. This would bring our total memory requirements to around ~1 GB in order to hold both, the vectors and the graph, for 1,000,000 vectors each with 128 dimensions. 1 GB doesn't sound too bad! Why should we go through the trouble of compressing the vectors at all then!? Sift1M is a rather small dataset. Have a look at some of the other experimental datasets listed in Table 1 below; these are much bigger and this is where the memory requirements can begin to get out of hand."], "query": "How much memory is required to hold both the vectors and the graph for the Sift1M dataset with 1,000,000 vectors each of 128 dimensions using 64 max connections, and why might one consider compressing the vectors?"}
{"relevant_passages": ["---\ntitle: HNSW+PQ - Exploring ANN algorithms Part 2.1\nslug: ann-algorithms-hnsw-pq\nauthors: [abdel]\ndate: 2023-03-14\ntags: ['research']\nimage: ./img/hero.png\ndescription: \"Implementing HNSW + Product Quantization (PQ) vector compression in Weaviate.\"\n---\n![HNSW+PQ - Exploring ANN algorithms Part 2.1](./img/hero.png)\n\n<!-- truncate -->\n\nWeaviate is already a very performant and robust [vector database](https://weaviate.io/blog/what-is-a-vector-database) and with the recent release of  v1.18 we are now bringing vector compression algorithms to Weaviate users everywhere. The main goal of this new feature is to offer similar performance at a fraction of the memory requirements and cost. In this blog we expand on the details behind this delicate balance between recall performance and memory management. In our previous blog [Vamana vs. HNSW - Exploring ANN algorithms Part 1](/blog/ann-algorithms-vamana-vs-hnsw), we explained the challenges and benefits of the Vamana and HNSW indexing algorithms."], "query": "What are the details of implementing HNSW+PQ vector compression in Weaviate as discussed in the blog post from March 14, 2023?"}
{"relevant_passages": ["In this blog post, we will show you how to ingest PDF documents with Unstructured and query in Weaviate. :::info\nTo follow along with this blog post, check out this [repository](https://github.com/weaviate-tutorials/how-to-ingest-pdfs-with-unstructured). :::\n\n## The Basics\nThe data we\u2019re using are two research papers that are publicly available. We first want to convert the PDF to text in order to load it into Weaviate. Starting with the first brick (partitioning), we need to partition the document into text."], "query": "How do you convert PDF documents to text for ingestion into Weaviate?"}
{"relevant_passages": ["Notice the first peak in memory at the beginning. This was the memory used for loading the first fifth of the data plus compressing the data. We then wait for the garbage collection cycle to claim the memory back after which we send the remaining data to the server. Note that the peaks in the middle are due to memory not being freed by the garbage collection process immediately. At the end you see the actual memory used after the garbage collection process clean everything up."], "query": "What causes the initial and subsequent peaks in memory usage during the data handling process described, and what is the state of memory after garbage collection?"}
{"relevant_passages": ["While all the experiments above have been carried out directly on the algorithm, the table below is constructed using data collected from interactions with the Weaviate server. This means the latency is calculated end to end (with all the overhead on communication from clients). |                         |                         | Latency (ms) | Memory required (GB) | Memory to host the vectors (GB) |\n|-------------------------|-------------------------|--------------|----------------------|---------------------------------|\n| 10M vectors from Sphere | Uncompressed            | 119          | 32.54                | 28.66                           |\n|                         | Compressed<br/>(4-to-1) | 273 (x2.29)  | 10.57 (32.48%)       | 7.16 (24.98%)                   |\n\n**Tab. 4**: *Summary of latency and memory requirements on the 10M Sphere subset. Additionally we show the speed down rate in latency and the percentage of memory, compared to uncompressed, needed to operate under compressed options.*\n\n## Conclusions\n\nIn this post we explore the journey and details behind the HNSW+PQ feature released in Weaviate v1.18."], "query": "What are the latency and memory savings when using the compressed option for the 10M vectors from Sphere in Weaviate v1.18?"}
{"relevant_passages": ["There are also attempts to collect richer multimodal datasets such as the [EGO4D](https://ego4d-data.org/docs/) which collects multiple data modalities for a given scenario. EGO4D actually captures motion/tactile data by capturing accelerometer and gyroscopic data temporally aligned with video captures of activities! The problem with this is that generating a rich multimodal dataset is very costly and even impractical in some cases. ### 2. Model Architecture\n\nDesigning the architecture for a single model that can process multimodal data is difficult. Usually, machine learning models are specialists in one domain of data - for example computer vision or natural language."], "query": "What is the EGO4D dataset, and what types of data does it capture?"}
{"relevant_passages": ["run the experiments, and 3. return a high quality report to the human user. Weights & Biases has paved an incredible experiment tracking path for training deep learning models. We expect interest to accelerate in this kind of support for RAG experimentation with the knobs and metrics we have outlined in this article. There are a couple of directions we are watching this evolve in."], "query": "What tool is mentioned for tracking deep learning model training experiments, and what type of experimentation does it support?"}
{"relevant_passages": ["---\ntitle: Weaviate 2023 Recap\nslug: 2023-recap\nauthors: [femke]\ndate: 2023-12-26\ntags: []\nimage: ./img/hero.png\ndescription: \"A reflection on 2023 from team Weaviate!\"\n---\n![hero](img/hero.png)\n\n<!-- truncate -->\n\nIt\u2019s hard to imagine that less than a year ago, so very few people even knew about the concept of vector databases and how AI could benefit from them. Those who did still had many questions about how they worked and whether they could at all be helpful. Meanwhile, curiosity and interest in AI spiked, especially after OpenAI launched ChatGPT. Curiosity has sped up our progress and made more people aware of the opportunities AI offers, transforming our landscape. Let's all take a moment to reflect and appreciate the start of a momentous change in how we can communicate, learn, teach, and collaborate so much faster and more effectively by leveraging AI."], "query": "What are the key highlights from the Weaviate team's 2023 recap?"}
{"relevant_passages": ["Although a bigger machine (see below) is needed for importing the data, the serving is done on a 12 CPU, 100 GB RAM, 250Gb SSD Google Cloud VM with 1 x NVIDIA Tesla P4. The ML-models used are [multi-qa-MiniLM-L6-cos-v1](https://huggingface.co/sentence-transformers/multi-qa-MiniLM-L6-cos-v1) and [bert-large-uncased-whole-word-masking-finetuned-squad](https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad) both are available as [pre-built modules](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-transformers#pre-built-images) in Weaviate. \ud83d\udcc4 The complete dataset and code is open-source and available [on GitHub](https://github.com/weaviate/semantic-search-through-wikipedia-with-weaviate). ![Demo GIF of Weaviate using the Wikipedia dataset](./img/weaviate-using-the-Wikipedia-dataset.gif)\n*Example semantic search queries in Weaviate's GraphQL interface \u2014 GIF by Author*\n\n## Importing the Data In Two Steps\n> You can also directly import a backup into Weaviate without doing the import your self as outlined [here](https://github.com/weaviate/semantic-search-through-wikipedia-with-weaviate/tree/main#step-3-load-from-backup). To import the data we use two different methods."], "query": "What are the specifications of the Google Cloud VM used for serving in the Weaviate semantic search through Wikipedia project?"}
{"relevant_passages": ["<br/>\nIt compares the vector values dimension by dimension and returns a total count of differing values. The fewer differences, the closer the vectors. For example, the Hamming distance for the below vectors is **2**, which is the count of differing values. * A `[1, 9, 3, 4, 5]`\n* B `[1, 2, 3, 9, 5]`\n\n### Manhattan distance\nThe Manhattan distance (also known as L1 norm and Taxicab Distance) - calculates the distance between a pair of vectors, as if simulating a route for a Manhattan taxi driver driving from point A to point B - who is navigating the **streets of Manhattan** with the grid layout and one-way streets. For each difference in the compared vectors, the taxi driver needs to make a turn, thus making the ride this much longer."], "query": "What is the Hamming distance between the vectors [1, 9, 3, 4, 5] and [1, 2, 3, 9, 5]?"}
{"relevant_passages": ["What does replication get us? A big one is *availability*. With no replication, any node being down will make its data unavailable. But in a Kubernetes setup composed of say, three Weaviate nodes (three Kubernetes \u201cpods\u201d) and a replication factor of three, you can have any one of the three nodes down and still reach consensus. This reflects Weaviate\u2019s leaderless replication architecture, meaning any node can be down without affecting availability at a cluster level as long as the right data is available somewhere."], "query": "How does replication enhance data availability in a Weaviate Kubernetes cluster?"}
{"relevant_passages": ["---\ntitle: Pulling back the curtains on text2vec\nslug: pulling-back-the-curtains-on-text2vec\nauthors: [jp]\ndate: 2023-01-10\ntags: ['integrations', 'concepts']\nimage: ./img/hero.png\ndescription: Ever wonder how Weaviate turns objects into vectors, behind-the-scenes? Find out in this post!\n---\n\n<!-- truncate -->\n\n![Pulling back the curtains on text2vec](./img/hero.png)\n\nYou probably know that Weaviate converts a text corpus into a set of vectors - each object is given a vector that captures its 'meaning'. But you might not know exactly how it does that, or how to adjust that behavior. Here, we will pull back the curtains to examine those questions, by revealing some of the mechanics behind `text2vec`'s magic. First, we will reproduce Weaviate's output vector using only an external API."], "query": "How does Weaviate convert text into vectors using `text2vec`, and how can this process be adjusted?"}
{"relevant_passages": ["<!-- truncate -->\n\n\n## 1. Install the client library\n\nThe Python and TypeScript client libraries support running Weaviate embedded on Linux, and starting with versions 3.21.0 and 1.2.0 respectively, on macOS as well. <Tabs groupId=\"languages\">\n  <TabItem value=\"py\" label=\"Python\">\n\n  ```bash\n  pip install weaviate-client  --upgrade\n  ```\n\n  </TabItem>\n\n  <TabItem value=\"js\" label=\"JavaScript/TypeScript\">\n\n  ```bash\n  npm install weaviate-ts-embedded typescript ts-node jest  # also install support for TypeScript and Jest testing\n  ```\n\n  </TabItem>\n</Tabs>\n\n\n## 2. Run the code\n\n<Tabs groupId=\"languages\">\n  <TabItem value=\"py\" label=\"Python\">\n\n  Save as `embedded.py` and run `python embedded.py`:\n  <br/>\n\n  <FilteredTextBlock\n    text={PyCode}\n    startMarker=\"# START 10lines\"\n    endMarker=\"# END 10lines\"\n    language=\"py\"\n  />\n  </TabItem>\n\n  <TabItem value=\"js\" label=\"JavaScript/TypeScript\">\n\n  Save as `embedded.ts` and run `node --loader=ts-node/esm embedded.ts`:\n  <br/>\n\n  <FilteredTextBlock\n    text={TSCode}\n    startMarker=\"// START 10lines\"\n    endMarker=\"// END 10lines\"\n    language=\"js\"\n  />\n  </TabItem>\n</Tabs>\n\n\n## <i class=\"fa-solid fa-screwdriver-wrench\"></i> How does this work? Essentially, what happens behind the scenes is that the client library downloads the server binary, spawns it in a separate process, connects to it, then terminates it on exit."], "query": "What versions of the Weaviate client libraries support running embedded on macOS, and how do you install and run them?"}
{"relevant_passages": ["}\n    },\n    \"vectorizer\": \"text2vec-huggingface\",  # vectorizer for hugging face\n   ... }\n```\n\n*If you are wondering, yes, you can use a different model for each class.*\n\n### Step 2 \u2013 cook for some time \u2013 import data\nStart importing data into Weaviate. For this, you need your Hugging Face API token, which is used to authorize all calls with \ud83e\udd17. Add your token, to a Weaviate client configuration. For example in Python, you do it like this:\n\n```javascript\nclient = weaviate.Client(\n    url='http://localhost:8080',\n    additional_headers={\n        'X-HuggingFace-Api-Key': 'YOUR-HUGGINGFACE-API-KEY'\n    }\n)\n```\nThen import the data the same way as always."], "query": "How do you add a Hugging Face API token to a Weaviate client configuration in Python?"}
{"relevant_passages": ["To see a list of the newly spun up nodes, run:\n\n```shell\nkubectl get nodes -o wide\n```\n\nYou should see an output similar to the following, indicating that three nodes are up and onto which you can deploy Weaviate:\n\n```shell\nNAME           STATUS   ROLES           AGE    VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME\nminikube       Ready    control-plane   134m   v1.27.3   192.168.49.2   <none>        Ubuntu 22.04.2 LTS   5.15.49-linuxkit   docker://24.0.4\nminikube-m02   Ready    <none>          134m   v1.27.3   192.168.49.3   <none>        Ubuntu 22.04.2 LTS   5.15.49-linuxkit   docker://24.0.4\nminikube-m03   Ready    <none>          133m   v1.27.3   192.168.49.4   <none>        Ubuntu 22.04.2 LTS   5.15.49-linuxkit   docker://24.0.4\n```\n\nNow, add the Weaviate helm repository to your local helm configuration by running:\n\n```shell\nhelm repo add weaviate https://weaviate.github.io/weaviate-helm\n```\n\nAnd save the default configuration with:\n\n```shell\nhelm show values weaviate/weaviate > values.yaml\n```\n\nEdit `values.yaml` by changing the root-level configuration `replicas: 1` for the root image to `replicas: 3`, and save it. ```yaml\n... # Scale replicas of Weaviate. Note that as of v1.8.0 dynamic scaling is limited\n# to cases where no data is imported yet. Scaling down after importing data may\n# break usability."], "query": "How can I list the current nodes in a Kubernetes cluster and prepare to deploy Weaviate with three replicas?"}
{"relevant_passages": ["With these changes alone, we achieved performance close to Microsoft's implementation, as shown in **(fig. 3)** and **(fig. 4)**. Note that the implementation from Microsoft is in C++, and our implementation is in Go.\n\nWe evaluated different data structures and achieved the best performance by:\n* Keeping a sorted array-based set. * Making insertions use binary search."], "query": "How did the Go implementation achieve performance close to Microsoft's C++ implementation?"}
{"relevant_passages": ["In 2020, Google released [Meena](https://blog.research.google/2020/01/towards-conversational-agent-that-can.html), a conversational agent. The goal of Meena was to show that it can have **sensible** and **specific** conversations. To measure the performance of the open-domain chatbots, they introduced the Sensibleness and Specificity Average (SSA) evaluation metrics. The bot\u2019s response was measured by its sensibleness, meaning it needed to make sense in context and be specific (specificity average). This ensures the output is comprehensive without being too vague."], "query": "What is the Sensibleness and Specificity Average (SSA) introduced by Google for?"}
{"relevant_passages": ["What if you have a case where you have a large-scale dataset but very few queries? What if cost-effectiveness is more of a priority than the lowest possible latencies? We believe there is a need for other index types besides the battle-tested HNSW implementation. But cost-effectiveness can never justify sacrificing the user experience. As a result, we are working on establishing a new type of vector index that combines the low operating cost of SSD-based solutions with the ease of use of existing in-memory solutions."], "query": "What new type of vector index is being developed to be cost-effective for large-scale datasets with few queries while still being user-friendly?"}
{"relevant_passages": ["* Join our community on [Slack](https://weaviate.io/slack) or [the forum](https://forum.weaviate.io/), where we can all talk about vector databases. * Reach out to us on [Twitter](https://twitter.com/weaviate_io). import Ending from '/_includes/blog-end-oss-comment.md'\n\n<Ending />"], "query": "How can I join a community to discuss vector databases and follow their updates on Twitter?"}
{"relevant_passages": ["The first approach seems more feasible and thus we will explore it next. In addition to the above mentioned problem, we also need the new functions to operate efficiently so the overall performance of the system doesn't suffer. Keep in mind that for a single query, any ANN algorithm would require distance calculations so by adding unnecessary complexity to the distance function we might severely damage the latency of querying or the overall time to index vectors. For these reasons we need to select the compression mechanism very carefully! There already exist many compression mechanisms, of which a very popular one is Product Quantization(PQ). This is the compression algorithm we chose to implement in Weaviate with v1.18."], "query": "What compression algorithm was chosen for implementation in Weaviate v1.18 to ensure efficient operation of new functions without compromising system performance?"}
{"relevant_passages": ["### Announcement\n\n\ud83c\udf89We are happy to share that all Weaviate `v1.15` binaries and distributions have been **compiled with Go 1.19** which comes with **GOMEMLIMIT**. Now, you can set your **soft memory cap** by setting the `GOMEMLIMIT` environment variable like this:\n\n```\nGOMEMLIMIT=120GiB\n```\n\nFor more information, see the [Docker Compose environment variables](/developers/weaviate/installation/docker-compose#environment-variables) in the docs. ## Faster imports for ordered data\n\n![Faster imports for ordered data](./img/ordered-imports.png)\n\nWeaviate `v1.5` introduced an **LSM store** (Log-Structured Merge Trees) to increase write-throughput. The high-level idea is that writes are batched up in logs, sorted into a **Binary Search Tree** (BST) structure, and then these batched-up trees are merged into the tree on disk. ### The Problem\n\nWhen importing objects with an inherent order, such as timestamps or row numbers that increase monotonously, the BST becomes unbalanced: New objects are always inserted at the \"greater than\" pointer / right node of the BST."], "query": "What environment variable can be set to define a soft memory cap in Weaviate v1.15?"}
{"relevant_passages": ["This lets you find the right balance between the **recall tradeoff** (the fraction of results that are the true top-k nearest neighbors), **latency**, **throughput** (queries per second) and **import time**.*<br/>\n*For a great example, check [Weaviate benchmarks](/developers/weaviate/benchmarks/ann#sift1m-1m-128d-vectors-l2-distance), to see how three parameters \u2013 [efConstruction, maxConnections and ef](/developers/weaviate/benchmarks/ann#what-is-being-measured) - affect recall, latency, throughput and import times.*\n\n### Examples of ANN algorithms\nExamples of ANN methods are:\n* **trees** \u2013 e.g. [ANNOY](https://github.com/spotify/annoy) (Figure 3),\n* **proximity** **graphs** - e.g. [HNSW](https://arxiv.org/abs/1603.09320) (Figure 4),\n* **clustering** - e.g. [FAISS](https://github.com/facebookresearch/faiss),\n* **hashing** - e.g. [LSH](https://en.wikipedia.org/wiki/Locality-sensitive_hashing),\n* **vector compression** - e.g. [PQ](https://ieeexplore.ieee.org/document/5432202) or [SCANN](https://ai.googleblog.com/2020/07/announcing-scann-efficient-vector.html). ![ANNOY](./img/ann-annoy.png)<br/>\n*[Figure 3 - Tree-based ANN search]*\n\nWhich algorithm works best depends on your project. Performance can be measured in terms of latency, throughput (queries per second), build time, and accuracy (recall). These four components often have a tradeoff, so it depends on the use case which method works best. So, while ANN is not some magic method that will always find the true k nearest neighbors in a dataset, it can find a pretty good approximation of the true k neighbors."], "query": "What factors should be considered when evaluating the performance of approximate nearest neighbor search algorithms?"}
{"relevant_passages": ["* [**RAG Metrics**](#rag-metrics): Common metrics used to evaluate Generation, Search, and Indexing and how they interact with each other. * [**RAG: Knobs to Tune**](#rag-knobs-to-tune): What decision makes RAG systems perform significantly different from one another? * [**Orchestrating Tuning**](#tuning-orchestration): How do we manage tracking experimental configurations of RAG systems? * [**From RAG to Agent Evaluation**](#from-rag-to-agent-evaluation): We define RAG as a three step procss of index, retrieve, and generate. This section describes when a RAG system becomes an Agent system and how Agent Evaluation differs."], "query": "What are the common metrics used to evaluate the performance of RAG systems in AI?"}
{"relevant_passages": ["---\ntitle: Ranking Models for Better Search\nslug: ranking-models-for-better-search\nauthors: [connor, erika]\ndate: 2023-04-11\nimage: ./img/hero.png\ntags: ['search']\ndescription: \"Learn about the different ranking models that are used for better search.\"\n\n---\n\n![ranking models animation](./img/hero.png)\n<!-- truncate -->\n\nWhether searching to present information to a human, or a large language model, quality matters. One of the low hanging fruit strategies to improve search quality are ranking models. Broadly speaking, ranking models describe taking the query and each candidate document, one-by-one, as input to predict relevance. This is different from vector and lexical search where representations are computed offline and indexed for speed. Back in August, we [published](/blog/cross-encoders-as-reranker) our thoughts on Cross Encoder Ranking."], "query": "What are ranking models, and how do they differ from vector and lexical search in improving search quality?"}
{"relevant_passages": ["Approximate nearest neighbor errors are typically measured by finding pareto-optimal points that trade off accuracy with queries per second and recall, ANN recall being the ground truth nearest neighbors to a query, rather than documents labeled as \u201crelevant\u201d to the query. ### Generation Metrics\nThe overall goal of a RAG application is to have a helpful output that uses the retrieved context for support. The evaluation must consider that the output has used the context without directly taking it from the source, avoiding redundant information, as well as preventing incomplete answers. To score the output, there needs to be a metric that covers each criteria. [Ragas](https://docs.ragas.io/en/latest/concepts/metrics/index.html#ragas-metrics) introduced two scores to measure the performance of an LLM output: faithfulness and answer relevancy."], "query": "What metrics are introduced by Ragas to measure the performance of an LLM output in RAG applications?"}
{"relevant_passages": ["This case is quite similar to our discussion of Multi-Index Routing and we can similarly evaluate generations with a prompt that explains the needs for SQL and Vector Databases and then asks the LLM whether the router made the right decision. We can also use the RAGAS Context Relevance score for the results of the SQL query. <img\n  src={require('./img/sql-router.png').default}\n  alt=\"SQL Router Query Engine\"\n  style={{ maxWidth: \"60%\" }}\n/>\n\nConcluding our discussion of \u201cFrom RAG to Agent Evaluation\u201d, we believe that it is still too early to tell what the common patterns will be for agent use. We have intentionally shown the multi-hop query engine and query router because these are relatively straightforward to understand. Once we add more open-ended planning loops, tool use and the associated evaluation of how well the model can format API requests to the tool, and more meta internal memory management prompts such as the ideas in MemGPT, it is very difficult to provide a general abstraction around how Agents will be evaluated."], "query": "What are the considerations for evaluating agents in the context of Multi-Index Routing and SQL and Vector Databases?"}
{"relevant_passages": ["This is especially interesting when tuning a multi-tenant use case with new versus power users. Within ANN indexing, we have PQ\u2019s hyperparameters of (segments, centroids, and the training limit). HNSW comes with (ef, efConstruction, and maxConnections). * Retrieval: Choosing an embedding model, weighting hybrid search, choosing a re-ranker, and dividing collections into multiple indexes. * Generation: Choosing an LLM and when to make the transition from Prompt Tuning to Few-Shot Examples, or Fine-Tuning."], "query": "What are the hyperparameters associated with PQ and HNSW in ANN indexing, and what considerations are involved in retrieval and generation for multi-tenant use cases?"}
{"relevant_passages": ["One on hand, the Zero-Shot LLMs out there such as GPT-4, Command, Claude, and open-source options such as Llama-2 and Mistral perform fairly well when they have **oracle context**. Thus, there is a massive opportunity to **focus solely on the search half**. This requires finding the needle in the haystack of configurations that trade-off ANN error from PQ or HNSW trade-offs, with embedding models, hybrid search weightings, and re-ranking as described earlier in the article. Weaviate 1.22 introduces Async Indexing with corresponding node status APIs. Our hope with partnerships focusing on RAG evaluation and tuning orchestration, is that they can use these APIs to see when an index is finished building and then run the test. This is particularly exciting when considering interfaces between these node statuses and tuning orchestration per tenant where some tenants may get away with brute force, and others need to find the right embedding model and HNSW configuration for their data."], "query": "What new feature does Weaviate 1.22 introduce to assist with RAG evaluation and tuning orchestration?"}
{"relevant_passages": ["But that is content for another article.\ud83d\ude09*\n\n## Learn more\nThe Weaviate Core team is currently working on research and implementation for other ANN algorithms. We are going to publish some of our findings in the next couple of weeks. So, stay tuned for more content on the topic. Until then, you can:\n* Listen to a [podcast about ANN Benchmarks](https://youtu.be/kG3ji89AFyQ) with Connor and Etienne from Weaviate. * Check out the [Getting Started with Weaviate](/developers/weaviate/quickstart) guide and begin building amazing apps with Weaviate."], "query": "What upcoming content is the Weaviate Core team planning to publish regarding ANN algorithms?"}
{"relevant_passages": ["### The solution\n\nAt the beginning of August, the Go team released `Go 1.19`, which introduced `GOMEMLIMIT`. `GOMEMLIMIT` turned out to be a **game changer for high-memory applications**. With GOMEMLIMIT we can provide a soft memory cap, which tells Go how much memory we expect the application to need. This makes the GC more relaxed when RAM is plentiful and more aggressive when memory is scarce. To learn more about memory management, GC and GOMEMLIMIT, check out [this article](/blog/gomemlimit-a-game-changer-for-high-memory-applications), which explains it all in more depth."], "query": "What feature introduced in Go 1.19 helps manage memory for high-memory applications?"}
{"relevant_passages": ["It\u2019s just that easy. Before you rush off to switch on replication, though, stick with us to read about the trade-offs and our recommendations. \ud83d\ude09\n\n## Trade-offs & discussions\n\nWhile replication and high availability are wonderful, we won\u2019t quite pretend that it comes for free. Having additional replicas of course means that there are more tenants and objects overall. Although they are duplicated, they are just as *real* as objects as any others."], "query": "What are some trade-offs to consider before enabling replication for high availability?"}
{"relevant_passages": ["{search_results}\u201d. We similarly encourage readers to check out some of the prompts available in Ragas [here](https://github.com/explodinggradients/ragas/tree/main/src/ragas/metrics). Another metric worth exploring is LLM Wins, where an LLM is prompted with: \u201cBased on the query {query}, which set of search results are more relevant? Set A {Set_A} or Set B {Set_B}. VERY IMPORTANT! Please restrict your output to \u201cSet A\u201d or \u201cSet B\u201d."], "query": "How can you determine which set of search results is more relevant using the LLM Wins metric?"}
{"relevant_passages": ["This collapses the binary tree into a linked list with `O(n)` insertion rather than the `O(log n)` promise of the BST. ### The Fix\n\nTo address that in Weaviate `v1.15`, we've extended the BST with a **self-balancing Red-black tree**. Through rotations of the tree at insert, [Red-black trees](https://www.programiz.com/dsa/red-black-tree) ensure that no path from the root to leaf is more than twice as long as any other path. This achieves O(log n) insert times for ordered inserts. ![Red-black tree demonstration](./img/red-black-tree.gif)\n*A visual representation of how the RBT works.*\n\nYou can try it yourself [here](https://www.cs.usfca.edu/~galles/visualization/RedBlack.html)."], "query": "What feature was introduced in Weaviate `v1.15` to ensure `O(log n)` insertion times for ordered inserts?"}
{"relevant_passages": ["There is no performance gain if we have more parallelization than there are available CPUs. However, each thread needs additional memory. So with 32 parallel imports, we had the worst of both worlds: High memory usage and no performance gains beyond 8. With the fix, even if you import from multiple clients, Weaviate automatically handles the parallelization to ensure that it does not exceed the number of CPU cores. As a result, you get the maximum performance without \"unnecessary\" memory usage. ### HNSW optimization\n\nNext, we optimized memory allocations for the HNSW (vector) index."], "query": "How does Weaviate ensure optimal performance and memory usage during parallel imports?"}
{"relevant_passages": ["Though we still have a ways to go on this journey, as noted earlier, but what we've accomplished thus far can add loads of value to Weaviate users. This solution already allows for significant savings in terms of memory. When compressing vectors ranging from 100 to 1000 dimensions you can save half to three fourths of the memory you would normally need to run HNSW. This saving comes with a little cost in recall or latency. The indexing time is also larger but to address this we've cooked up a second encoder, developed specially by Weaviate, based on the distribution of the data that will reduce the indexing time significantly."], "query": "What are the benefits and trade-offs of the new solution for compressing vectors in Weaviate?"}
{"relevant_passages": ["![Manhattan taxi driver](./img/manhatten-distance-cars.png)\n\nThe Manhattan distance is calculated by adding up the differences between vector values. Following our previous example:\n* A `[1, 9, 3, 4, 5]`\n* B `[1, 2, 3, 9, 5]`\n\nWe can calculate the Manhattan distance in these steps:\n1. distance = `|1-1| + |9-2| + |3-3| + |4-9| + |5-5|`\n1. distance = `0 + 7 + 0 + 5 + 0`\n1. distance = `12`\n\nFor a deeper dive into the Hamming and Manhattan distances, check out our [blog post on distance metrics](/blog/distance-metrics-in-vector-search)."], "query": "How is the Manhattan distance between the vectors `[1, 9, 3, 4, 5]` and `[1, 2, 3, 9, 5]` calculated?"}
{"relevant_passages": ["We are already working on a fully disk-based solution which we will release in late 2023. <br></br>\n\n### Improving our Client and Module Ecosystem\n![client modules](./img/client-modules.png)\n\nSo far, we have only discussed features related to Weaviate Core, the server in your setup. But the Weaviate experience is more than that. Modules allow you to integrate seamlessly with various embedding providers, and our language clients make Weaviate accessible right from your application. This year, we will further improve both."], "query": "When is the fully disk-based solution for Weaviate expected to be released?"}
{"relevant_passages": ["Within a schema, you can set different vectorizers and vectorize instructions on a class level. First, because our use case is semantic search over Wikipedia, we will be dividing the dataset into paragraphs and use Weaviate's graph schema to link them back to the articles. Therefore we need two classes; *Article* and *Paragraph*. ```javascript\n{\n  classes: [\n    {\n      class: \"Article\",\n      description: \"A wikipedia article with a title\",\n      properties: {...},\n      vectorIndexType: \"hnsw\",\n      vectorizer: \"none\"\n    },\n    {\n      class: \"Paragraph\",\n      description: \"A wiki paragraph\",\n      properties: {...},\n      vectorIndexType: \"hnsw\",\n      vectorizer: \"text2vec-transformers\"\n    },\n  ]\n}\n```\n\n*Weaviate class structure*\n\nNext, we want to make sure that the content of the paragraphs gets vectorized properly, the vector representations that the SentenceBERT transformers will generate are used for all our semantic search queries. ```javascript\n{\n  name: \"content\",\n  datatype: [\n    \"text\"\n  ],\n  description: \"The content of the paragraph\",\n  invertedIndex: false,\n  moduleConfig: {\n    text2vec-transformers: {\n      skip: false,\n      vectorizePropertyName: false\n    }\n  }\n}\n```\n\n*A single data type that gets vectorized*\n\nLast, we want to make graph relations, in the dataset from step one we will distill all the graph relations between articles that we can reference like this:\n\n```javascript\n{\n  name: \"hasParagraphs\"\n  dataType: [\n    \"Paragraph\"\n  ],\n  description: \"List of paragraphs this article has\",\n  invertedIndex: true\n}\n```\n*Paragraph cross-references*\n\nThe complete schema we import using the [Python client](/developers/weaviate/client-libraries/python) can be found [here](https://github.com/weaviate/semantic-search-through-wikipedia-with-weaviate/blob/main/step-2/import.py#L19-L120)."], "query": "How do you set up a Weaviate schema for semantic search on Wikipedia data with vectorized content and graph relations?"}
{"relevant_passages": ["---\ntitle: Support for Hugging Face Inference API in Weaviate\nslug: hugging-face-inference-api-in-weaviate\nauthors: [sebastian]\ndate: 2022-09-27\ntags: ['integrations']\nimage: ./img/hero.png\ndescription: \"Running ML Model Inference in production is hard. You can use Weaviate \u2013 a vector database \u2013 with Hugging Face Inference module to delegate the heavy lifting.\"\n---\n![Support for Hugging Face Inference API in Weaviate](./img/hero.png)\n\n<!-- truncate -->\n\nVector databases use Machine Learning models to offer incredible functionality to operate on your data. We are looking at anything from **summarizers** (that can summarize any text into a short) sentence), through **auto-labelers** (that can classify your data tokens), to **transformers** and **vectorizers** (that can convert any data \u2013 text, image, audio, etc. \u2013 into vectors and use that for context-based queries) and many more use cases. All of these use cases require `Machine Learning model inference` \u2013 a process of running data through an ML model and calculating an output (e.g. take a paragraph, and summarize into to a short sentence) \u2013 which is a compute-heavy process."], "query": "What functionalities does the integration of Hugging Face Inference API with Weaviate offer?"}
{"relevant_passages": ["Determining whether a generated response is good or bad is dependent on a few metrics. You can have answers that are factual, but not relevant to the given query. Additionally, answers can be vague and miss out key contextual information to support the response. We will now go one step back through the pipeline and cover retrieval metrics. ### Retrieval Metrics\nThe next layer in the evaluation stack is information retrieval."], "query": "What factors are important in evaluating the quality of generated responses and what role do retrieval metrics play?"}
{"relevant_passages": ["We can get higher quality generations with a more expensive language model, we can get higher quality retrieval by filtering results with re-rankers, and we can get higher recall indexing by using more memory. How to manage these trade-offs to improve performance will hopefully become more clear as we continue our investigation into \u201cRAG: Knobs to Tune\u201d. As a quick reminder, we chose to present metrics from a top-down view from Generation to Search and Indexing because evaluation time is closer to the user experience. We will alternatively present the knobs to tune from the bottom-up of Indexing to Retrieval and Generation because this is similar to the experience of the RAG application developer. ## RAG Knobs to Tune\nNow that we\u2019ve covered the metrics to compare RAG systems, let\u2019s dive further into significant decisions that can alter the performance."], "query": "What are the trade-offs in improving the performance of a RAG system, and how does the perspective of metrics presentation differ for users and developers?"}
{"relevant_passages": ["## Recommendations & Wrap-up\n\nAs we mentioned before, all you need to configure to enable replication is this in the collection definition:\n\n```json\n{\n  class: 'YOUR_CLASS_NAME',\n  ... replicationConfig: {\n    factor: 3,\n  }\n}\n```\n\nBut what replication factor would we recommend? That\u2019s something of a subjective question, but our starting recommendation is 3. The reason is that odd numbers are preferred for consistency so that consensus can always be reached. Higher factors are also possible, but this is more of a measure to scale query throughput, rather than lead to more availability."], "query": "What is the recommended replication factor for enabling replication in a collection definition, and why is an odd number preferred?"}
{"relevant_passages": ["1**: *Suppose we have vectors $x$ and $y$ represented in their original space. We apply a compression function $C$ to obtain a shorter representation of $x$ ($x'$) and $y$ ($y'$) on a compressed space but would require a decompression function $C'$ from the compressed space into the original space to be able to use the original distance function. In this case we would obtain $x''$ and $y''$ from $x'$ and $y'$ respectively and apply the distance on the approximations of the original $x$ and $y$ so $d(x,y)=d(x'',y'') + \\delta$ where $\\delta$ is the distortion added to the distance calculation due of the reconstruction of the original vectors. The compression/decompression mechanisms should be such that the distortion is minimized.*\n\n![comp2](./img/image2.jpg)\n**Fig. 2**: *Suppose we have vectors $x$ and $y$ represented in their original space."], "query": "How does the compression and subsequent decompression of vectors affect the accuracy of distance calculations between them?"}
{"relevant_passages": ["Now imagine you turn on the captions, and now you can understand pretty much everything that\u2019s going on and you can even fill in sound effects with your imagination supported by the modalities of data you have available. In order for our machine learning models to interact more naturally with data, the way we do, and ultimately be more general and powerful reasoning engines we need them to understand a data point through its corresponding image, video, text, audio, tactile and other representations - it needs to be able to preserve the meaning behind all of these data modalities once embedded in high dimensional vector space as demonstrated in Figure1 below. For example, in order to understand what a train is, we need our ML models to \u201csee\u201d and \u201chear\u201d the train move, \u201cfeel\u201d the motion of the ground near the train, and \u201cread\u201d about it. This is easier said and understood than done. Having a model that jointly comprehends all of these data modalities is very difficult, let's discuss some of the challenges that stand between us and truly multimodal models."], "query": "What are the challenges in developing machine learning models that can understand multiple data modalities?"}
{"relevant_passages": ["These embeddings better reflect the polysemantic nature of words, which can only be disambiguated when they are considered in context. Some of the potential downsides include:\n* increased compute requirements: fine-tuning transformer models is much slower (on the order of hours vs. minutes)\n* increased memory requirements: context-sensitivity greatly increases memory requirements, which often leads to limited possible input lengths\n\nDespite these downsides, transformer models have been wildly successful. Countless text vectorizer models have proliferated over the recent past. Plus, many more vectorizer models exist for other data types such as audio, video and images, to name a few."], "query": "What are the advantages and disadvantages of using transformer models for word embeddings?"}
{"relevant_passages": ["The amount of control you have over exactly what is generated is quite fascinating, you can even engineer a paragraph length prompt to describe exactly what you want the diffusion model to create and watch as it brings multiple variations of your description to life. ## Diffusion Model Resources to Create Art\n\nHere are some diffusion models that you can use to generate images:\n\n- [Stable Diffusion on Hugging Face](https://huggingface.co/CompVis/stable-diffusion-v1-4)\n- A [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb) that uses the same Stable Diffusion model as above if you\u2019d like to tinker with the code. - Midjourney - allows you to create images by submitting prompts as discord messages\n- A [Web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) for Stable Diffusion\n\n![searching through vector spaces](./img/searching_through_multi_dimensional_vector_space.jpg)\n*\u201cSearching through a high dimensional vector space\u201d*\n## Sources and Further Reading\n\nHere I\u2019ve attempted to provide a general intuition of how diffusion models work, there are many many more details that are involved in this process. If you enjoyed this introduction to diffusion models and would like to dive in deeper to get a better understanding of the code and algorithms involved I would recommend the following blog posts and courses in the following order or increasing complexity:\n\n- [The Illustrated Stable Diffusion](https://jalammar.github.io/illustrated-stable-diffusion/)\n- [Generative Modeling by Estimating Gradients of the Data Distribution](https://yang-song.net/blog/2021/score/)\n- [The Annotated Diffusion Model](https://huggingface.co/blog/annotated-diffusion)\n\nimport WhatNext from '/_includes/what-next.mdx'\n\n<WhatNext />"], "query": "What resources can I use to generate images with diffusion models?"}
{"relevant_passages": ["And it can do this in a fraction of the time!\n\n## HNSW in Weaviate\n[Weaviate](/developers/weaviate/) is a great example of a vector database that uses ANN algorithms to offer ultra-fast queries. The first ANN algorithm introduced to Weaviate is a custom implementation of [Hierarchical Navigable Small World graphs (HNSW)](/developers/weaviate/concepts/vector-index#hnsw). <img\n  src={require('./img/ann-hnsw.png').default}\n  alt=\"HNSW\"\n  style={{ maxWidth: \"50%\" }}\n/>\n\n*[Figure 4 - Proximity graph-based ANN search]*\n\nCheck out [Weaviate ANN benchmarks](/developers/weaviate/benchmarks/ann) to see how HNSW performed on realistic large-scale datasets. You can use it to compare the tradeoffs between recall, QPS, latency, and import time.<br/>\nYou will find it interesting to see, that Weaviate can maintain very high recall rates (>95%), whilst keeping high throughput and low latency (both in milliseconds). That is exactly what you need for fast, but reliable vector search!\n\n## Summary\nA quick recap:\n* Vector databases use Machine Learning models to calculate and attach Vector embeddings to all data objects\n* Vector embeddings capture the meaning and context of data\n* Vector databases offer super fast queries thanks to ANN algorithms\n* ANN algorithms trade a small amount of accuracy for huge gains in performance\n\n*Of course, there is a lot more going on in a vector database that makes it so efficient."], "query": "What ANN algorithm does Weaviate use to enable fast vector search, and how does it perform in terms of recall rates, throughput, and latency?"}
{"relevant_passages": ["With 128 locks, we only have 1/128th of the congestion of a single lock while still only using 128 * 8B = 1KB of memory. With the **lock striping** pattern, the import time is the same as without a lock, and we fixed the race condition without any negative performance impact. ![lock striping solution](./img/lock-striping-solution.png)\n\nWe are very pleased to introduce this solution, which should eliminate the above issues that can be caused by data duplication at import. Additionally, we are also very happy to have arrived at a solution that comes with no data import performance penalty, having seen the mammoth datasets that our users often deal with. ## Update Weaviate\nThe **lock striping** pattern was introduced in Weaviate `v1.15.4`."], "query": "What technique was implemented in Weaviate v1.15.4 to reduce congestion and memory usage without impacting data import performance?"}
{"relevant_passages": ["This has led to people questioning whether a Few-Shot LLM Evaluation is necessary. Due to its \u201cgood enough\u201d status on the tipping scale, Zero-Shot LLM Evaluation may be all that is needed to be a north star for RAG system tuning. Shown below, the RAGAS score is made up of 4 prompts for Zero-Shot LLMs that evaluate the 2 metrics for generation, **Faithfulness** and **Answer Relevancy**, as well as 2 metrics for retrieval, **Context Precision** and **Context Recall**. ![Ragas-score](img/ragas-score.png)\n[Source](https://docs.ragas.io/en/latest/concepts/metrics/index.html)\n\nThe transition from Zero-Shot to Few-Shot LLM Evaluation is quite straightforward. Instead of using just an instruction template, we add a few labeled examples of the relevancy of linked search results to a query."], "query": "What are the four metrics included in the RAGAS score for evaluating Zero-Shot LLMs in RAG systems?"}
{"relevant_passages": ["We could reduce latency by paying some penalty in the distortion of the distances (observed in the recall drop) or we could aim for more accurate results with a higher latency penalty. ### Deep-Image\n\nNext we show the results on DeepImage96. We see a similar profile compared to Sift1M. Latency is nearly 10 times slower but this is due to the fact that we have nearly 10 times more vectors. ![res3](./img/image7.png)\n**Fig."], "query": "How does the latency compare between DeepImage96 and Sift1M datasets when aiming for accurate search results?"}
{"relevant_passages": ["Sometimes, it's enough to output relevant data really fast. In such cases we care about latency a lot more than recall. Compressing, not only saves memory but also time. As an example, consider brute force where we compress very aggressively. This would mean we do not need memory at all for indexing data and very little memory for holding the vectors."], "query": "Why might aggressive data compression be beneficial in a brute force approach?"}
{"relevant_passages": ["In this example we used two research papers; however, there is the possibility to add Powerpoint presentations or even scanned letters to your Weaviate instance. Unstructured has really simplified the process of using visual document parsing for diverse document types. We tested a few queries above, but we can take this one step further by using [LangChain](https://python.langchain.com/docs/modules/data_connection/vectorstores/integrations/weaviate). Once the documents are imported into Weaviate, you can build a simple chatbot to chat with your pdfs by using LangChain\u2019s vectorstore. ```python\nfrom langchain.vectorstores.weaviate import Weaviate\nfrom langchain.llms import OpenAI\nfrom langchain.chains import ChatVectorDBChain\nimport weaviate\n\nclient = weaviate.Client(\"http://localhost:8080\")\n\nvectorstore = Weaviate(client, \"NAME_OF_CLASS\", \"NAME_OF_PROPERTY\")\n\nMyOpenAI = OpenAI(temperature=0.2,\n    openai_api_key=\"ENTER YOUR OPENAI KEY HERE\")\n\nqa = ChatVectorDBChain.from_llm(MyOpenAI, vectorstore)\n\nchat_history = []\n\nwhile True:\n    query = input(\"\")\n    result = qa({\"question\": query, \"chat_history\": chat_history})\n    print(result[\"answer\"])\n    chat_history = [(query, result[\"answer\"])]\n```\n\nimport WhatNext from '/_includes/what-next.mdx'\n\n<WhatNext />"], "query": "How can you build a chatbot to interact with documents in Weaviate using LangChain?"}
{"relevant_passages": ["During the Anthropic Hackathon in London, Ken and Pascal built the v1 version of their product. At the hackathon, they developed a support copilot capable of answering any user queries about a product or API. What's impressive is that it can be easily integrated into any website with just a single line of code. Now, their copilots provide answers and execute actions for users. [Ricards Liskovskis](https://www.linkedin.com/in/ricards-liskovskis-1872781a9/) crafted [RicAI](https://lablab.ai/event/autonomous-agents-hackathon/ricai/ricai-autonomous-testing-agent), securing 2nd place in the [SuperAGI Hackathon](https://superagi.com/autonomous-agents-hackathon/)."], "query": "Who developed a support copilot at the Anthropic Hackathon in London that can be integrated into websites with a single line of code?"}
{"relevant_passages": ["Document understanding techniques use an encoder-decoder pipeline that leverages the power of both computer vision and natural language processing methods. On the Weaviate Podcast, Brian Raymond described one of the founding motivations of Unstructured as follows: \u201cHey, HuggingFace is exploding over here with 10s of thousands of models and an incredible community. What if we did something similar to the left of HuggingFace, and we made it cheap, fast, and easy for data scientists to get through that data engineering step, so they can consume more of that!\u201d Now that the stage is set, let\u2019s explore how Unstructured works. Unstructured simplifies the process of importing a PDF and converting it into text. The core abstraction of Unstructured is the 'brick.' Unstructured uses bricks for document pre-processing: 1."], "query": "What was the motivation behind creating Unstructured as described by Brian Raymond on the Weaviate Podcast?"}
{"relevant_passages": ["We can send these features to wherever our Metadata Ranker is hosted and get those scores back to Weaviate to sort our search results. This is also closely related to another category of ranking methods that use models like XGBoost to combine features, as well as say the bm25 score, vector distance, and maybe even the cross encoder score as well. This is a pretty interesting technique when you additionally factor in multiple properties. For example, we could have a bm25, vector, and cross encoders for the `title`, as well as `content` properties and use a learned model to combine these into a final ranking score. I recently came across a paper titled \u201cInjecting the BM25 Score as Text Improves BERT-Based Re-rankers\u201d published in ECIR 2023."], "query": "What is the title of the paper mentioned in the document that discusses improving BERT-based re-rankers by injecting the BM25 score?"}
{"relevant_passages": ["### The problem\n\nWorking with a garbage collector is very safe, and the resource cost of running GC cycles is a fairly small tradeoff. We just need to ensure that we have the right balance of frequency of GC cycles and the buffer of available memory (on top of what we have estimated for our application setup). Now, increasing each of those comes with a price:\n* Increasing the frequency of GC cycles will use more CPU, which we could make better use of elsewhere. * Increasing RAM costs money - and for memory demanding setups, that can be a big \ud83d\udcb0sum. And if we get that balance wrong, we might end up with an Out Of Memory crash."], "query": "What are the tradeoffs and potential risks associated with adjusting the frequency of garbage collector cycles and the amount of available memory in an application setup?"}
{"relevant_passages": ["Now, that's a lot of new models. \ud83d\ude09\n\n#### How this works\nThe way the module works, Weaviate coordinates the efforts around data imports, data updates, queries, etc. and delegates requests to the Hugging Face Inference API. You need a `Hugging Face API Token` to use the Hugging Face module. You can [request it here](https://huggingface.co/login?next=%2Fsettings%2Ftokens)."], "query": "How do I obtain a Hugging Face API Token to use with the Weaviate Hugging Face module?"}
{"relevant_passages": ["When we rerun the original test (with ten parallel aggregations), we saw the memory consumption drop to 30GB (vs 200GB). <!-- TODO: add a quote from Juraj\nBut don't take our word for it, this was from a test run by one of our community members\u2026\n -->\n\n## New distance metrics\n\n![Hamming and Manhattan distance metrics](./img/distance-metrics.png)\n\nThanks to the community contributions from [Aakash Thatte](https://github.com/sky-2002), Weaviate `v1.15` adds two new distance metrics: **Hamming** distance and **Manhattan** distance. In total, you can now choose between five various distance metrics to support your datasets. Check out the [metrics documentation page](/developers/weaviate/config-refs/distances#distance-implementations-and-optimizations) for a complete overview of all available metrics in Weaviate. ### Hamming distance\nThe Hamming distance is a metric for comparing two numerical vectors."], "query": "What new distance metrics were added in Weaviate version 1.15?"}
{"relevant_passages": ["Vector embeddings capture the meaning and context of data, usually predicted by Machine Learning models. At the time of entry/import (or any significant changes to data objects), for every new/updated data object, vector databases use Machine Learning models to predict and calculate vector embeddings, which then are stored together with the object. > Every data object in a dataset gets a vector\n\nIn a nutshell, vector embeddings are an array of numbers, which can be used as coordinates in a high-dimensional space. Although it is hard to imagine coordinates in more than 3-dimensional space (x, y, z), we can still use the vectors to compute the distance between vectors, which can be used to indicate similarity between objects. <br/>\n\nThere are many different distance metrics, like [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) and [Euclidean distance (L2 distance)](https://en.wikipedia.org/wiki/Euclidean_distance)."], "query": "How are vector embeddings used in vector databases to determine the similarity between data objects?"}
{"relevant_passages": ["We analyze Product Quantization with KMeans encoding using different segment lengths. Notice that the lengths for the segments should be an integer divisor of the total amount of dimensions. For Sift1M we have used 1, 2, 4 and 8 dimensions per segment, while for Gist and DeepImage we have used 1, 2, 3, 4 and 6 dimensions per segment. Using segment length of 1 on Sift1M would give us 128 segments, while a segment length of 8 would give us 16 segments. We also offer the flexibility of choosing the amount of centroids to use with KMeans."], "query": "How many segments are created when using a segment length of 8 for the Sift1M dataset in Product Quantization with KMeans encoding?"}
{"relevant_passages": ["Tech giants like Google, AWS, or Microsoft Azure offer their vector-search capabilities to customers willing to upload their data. But there's now an ecosystem of newer companies with AI-first specific (often open-source) solutions and vector-search capabilities that customers can run on a SaaS basis or on their own systems. ## The AI-first Database Ecosystem\nThe companies that make up this ecosystem provide specialized services that overlap to various degrees. Combined, four sub-groups make up the ecosystem. 1."], "query": "What are the four sub-groups that make up the AI-first database ecosystem with vector-search capabilities?"}
{"relevant_passages": ["PDF documents contain valuable insights and information that are key to unlocking text information for businesses. With the latest advancements in multimodal deep learning (models that process both images and text), it is now possible to extract high quality data from PDF documents and add it to your Weaviate workflow. [Optical Character Recognition](https://en.wikipedia.org/wiki/Optical_character_recognition) (OCR) describes technology that converts different types of visual documents (research papers, letters, etc.) into a machine readable format. [RVL-CDIP](https://huggingface.co/datasets/aharley/rvl_cdip) is a benchmark that tests the performance of classifying document images. New models like [LayoutLMv3](https://huggingface.co/docs/transformers/main/en/model_doc/layoutlmv3) and [Donut](https://huggingface.co/docs/transformers/model_doc/donut) leverage both the text and visual information by using a multimodal transformer."], "query": "What are the latest advancements in extracting data from PDF documents using multimodal deep learning, and which models leverage both text and visual information?"}
{"relevant_passages": ["[Faithfulness](https://docs.ragas.io/en/latest/concepts/metrics/faithfulness.html) observes how factually correct the answer is based on the retrieved context. [Answer relevance](https://docs.ragas.io/en/latest/concepts/metrics/answer_relevance.html) determines how relevant the answer is given the question. An answer can have a high faithfulness score, but a low answer relevance score. For example, a faithful response is one that copies the context verbatim, however, that would result in a low answer relevance. The answer relevance score is penalized when an answer lacks completeness or has duplicate information."], "query": "What is the difference between faithfulness and answer relevance in the context of evaluating answers?"}
{"relevant_passages": ["This is also known as weak supervision in machine learning. Once a dataset has been prepared, there are three common metrics used for evaluation: **nDCG**, **Recall**, and **Precision**. NDCG (Normalized Discounted Cumulative Gain) measures ranking with multiple relevance labels. For example, a document about Vitamin B12 may not be the most relevant result to a query about Vitamin D, but it is more relevant than a document about the Boston Celtics. Due to the additional difficulty of relative ranking, binary relevance labels are often used (1 for relevant, 0 for irrelevant)."], "query": "What is nDCG and how is it used in evaluating machine learning models?"}
{"relevant_passages": ["This delta refers to the error we introduce due to the compression - we are using a lossy compression process. As we mentioned before, we are lowering the accuracy of our data, meaning the distance we calculate is a bit distorted; this distortion is exactly what we represent using delta. We don't aim to calculate such an error, nor to try to correct it. We should however acknowledge it and try to keep it low. ![comp1](./img/image1.jpg)\n**Fig."], "query": "What does the term \"delta\" refer to in the context of lossy data compression?"}
{"relevant_passages": ["How, you ask? Having redundancy is key to achieving this. And such redundancy, or replication, has been available for a while in Weaviate for production use. In fact, configuring replication is actually easy and simple, and using it can lead to huge benefits. In this post, we will explain the benefits of replication and show you how to configure it."], "query": "What are the benefits of using replication in Weaviate and how is it configured?"}
{"relevant_passages": ["Further, we may want to speed up testing by parallelizing resource allocation. For example, evaluating 4 embedding models at the same time. As discussed earlier, another interesting component to this is tuning chunking or other symbolic metadata that may come from our data importer. To paint the picture with an example, the Weaviate Verba dataset contains 3 folders of Weaviate `Blogs`, `Documentation`, and `Video` transcripts. If we want to ablate chunk sizes of 100 versus 300, it probably doesn\u2019t make sense to re-invoke the web scraper."], "query": "What dataset contains `Blogs`, `Documentation`, and `Video` transcripts used for evaluating embedding models and tuning chunk sizes?"}
{"relevant_passages": ["---\ntitle: Weaviate 1.14 release\nslug: weaviate-1-14-release\nauthors: [connor, etienne, laura, sebastian]\ndate: 2022-07-06\ntags: ['release']\nimage: ./img/hero.png\ndescription: \"Learn what is new in Weaviate 1.14, the most reliable and observable Weaviate release yet!\"\n---\n![Weaviate 1.14 release](./img/hero.png)\n\n<!-- truncate -->\n\n## What is new\nWe are excited to announce the release of Weaviate 1.14, the most reliable and observable Weaviate release yet. > Later this week, we will release Weaviate v1.14, possibly the most boring release so far.\ud83d\ude31 Yet I'm incredibly excited about it and so should you. Why? <br/>\n> (1/9)<i className=\"fas fa-reel\"></i><br/>\n> [See <i className=\"fab fa-twitter\"></i> by @etiennedi](https://twitter.com/etiennedi/status/1544689150217027584?ref_src=twsrc%5Etfw)\n\nBesides many bug fixes and reliability improvements, we have a few neat features that you might find interesting. In short, this release covers:\n\n* Reliability fixes and improvements\n* Monitoring and Observability\n* Support for non-cosine distances\n* API changes\n\n## Reliability fixes and improvements\n![Reliability and fixes](./img/reliability.png)\n\nAt Weaviate, Reliability is one of our core values, which means that we will always strive to make our software dependable, bug-free, and behave as expected."], "query": "What are the main features of the Weaviate 1.14 release?"}
{"relevant_passages": ["Look for entries closest to `fast food chains` like so:\n\n```python\nnear_text = {\"concepts\": [\"fast food chains\"]}\nwv_resp = client.query.get(\n    \"Question\",\n    [\"question\", \"answer\"]\n).with_limit(2).with_near_text(\n    near_text\n).with_additional(['distance', 'vector']).do()\n```\n\nThis yields the McDonald's `Question` object above, including the object vector and the distance. The result is a `768`-dimensional vector that is about `0.1` away from the query vector. This all makes intuitive sense - the entry related to the largest fast food chain (McDonald's) is returned from our \u201cfast food chains\u201d query. But wait, how was that vector derived? This is the 'magic' part."], "query": "How do you use vector search to find database entries related to fast food chains?"}
{"relevant_passages": ["We will review and help you out in the process. \ud83d\udc9a\n\nYou can also contribute by adding your own Weaviate examples. If you have other great ideas for contributions, let us know on our [Discourse](https://forum.weaviate.io/) and [Slack](https://weaviate.slack.com/) channels, and we will figure out how to highlight it in Hacktoberfest. You don't need to be an expert to contribute to these demo projects!\n\n\n## Resources to Get Started\n\nWe're thrilled to help you make your first open-source contribution! Here are some helpful resources to kickstart your journey:\n\nWhat is Open Source, and how do you contribute to it? - \ud83c\udfaf [What is Open Source](https://www.digitalocean.com/community/tutorials/what-is-open-source)\n- \ud83c\udfaf [Introduction to GitHub and Open-Source Projects](https://www.digitalocean.com/community/tutorial_series/an-introduction-to-open-source)\n- \ud83c\udfaf [How to Contribute to Open Source](https://opensource.guide/how-to-contribute/)\n- \ud83c\udfaf\u00a0[GitHub Contribution Guide by Hugging Face](https://www.notion.so/Contribution-Guide-19411c29298644df8e9656af45a7686d?pvs=21)\n- \ud83c\udfaf [How to Use Git](https://www.digitalocean.com/community/cheatsheets/how-to-use-git-a-reference-guide)\n- \ud83c\udfaf [Weaviate Contributor Guide](https://weaviate.io/developers/contributor-guide)\n\nIf you're new to Weaviate, get up and running quickly with these beginner-friendly guides:\n\n- [Quickstart Guide](https://weaviate.io/developers/weaviate/quickstart) \ud83d\ude80\n- [Weaviate Academy](https://weaviate.io/developers/academy) \ud83c\udf93\n\nDive deeper into specific topics with these detailed guides:\n\n- [How-to Search Guides](https://weaviate.io/developers/weaviate/search) \ud83d\udd0d\n- [Keyword, Vector, Hybrid, and Generative Search](https://github.com/weaviate-tutorials/generative-search/blob/main/GenerativeSearchDemo.ipynb) \ud83d\udd0d\n- [How-to Manage Data (CRUD Operations)](https://weaviate.io/developers/weaviate/manage-data) \ud83d\udcbe\n- [Tutorial: Importing Data with Your Own Vectors](https://weaviate.io/developers/weaviate/tutorials/wikipedia) \ud83d\udcca\n- [Weaviate Architecture Concepts](https://weaviate.io/developers/weaviate/concepts#weaviate-architecture) \ud83c\udfdb\ufe0f\n\nJoin one of our [workshops](https://weaviate.io/learn/workshops) for an introduction to Weaviate."], "query": "Where can I find resources to help me make my first open-source contribution to Weaviate?"}
{"relevant_passages": ["<br/>\n\nJust like that, you\u2019ve got a multi-node Weaviate cluster. Remember that when you create a class, you must have replication enabled. You can do this by adding the `replicationConfig` parameter to the collection definition, like so:\n\n```json\n{\n  \"class\": \"ClassWithReplication\",\n  \"properties\": [\n    {\n      \"name\": \"exampleProperty\",\n      \"dataType\": [\"text\"]\n    }\n  ],\n  // highlight-start\n  \"replicationConfig\": {\n    \"factor\": 3\n  }\n  // highlight-end\n}\n```\n\nAnd when you insert objects into `ClassWithReplication`, they will be replicated across the three nodes. You can verify this by visiting the `/nodes` endpoint, which will verify that each node contains the same number of objects. </details>\n\n## Benefits of replication\n\nSo, let\u2019s cover the implications of doing this."], "query": "How do you enable replication for a class in a Weaviate cluster and verify it?"}
{"relevant_passages": ["We apply a compression function $C$ to obtain a shorter representation of $x$ ($x'$) and $y$ ($y'$) on a compressed space. This saves storage space but would require a new distance function $d'$ to operate directly on the compressed space so $d(x,y) = d'(x',y') + \\delta$ where $\\delta$ is the distortion of the distance. We also need the new distance to be such that the distortion is minimized.*\n\nThe second approach might be better in terms of performance since it does not require the decompression step. The main problem is that we might need to define a new distance function for each encoding compression function. This solution looks harder to maintain in the long run."], "query": "What are the considerations for using a compression function in relation to maintaining distance relationships in a compressed space?"}
{"relevant_passages": ["Within Weaviate the transformer module can be used to vectorize and query your data. ## Getting started with out-of-the-box transformers in Weaviate\nBy selecting the text-module in the [Weaviate configuration tool](/developers/weaviate/installation/docker-compose#configurator), you can run Weaviate with transformers in one command. You can learn more about the Weaviate transformer module [here](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-transformers). ![Weaviate configurator \u2014 selecting the Transformers module](./img/configurator-demo.gif)\n*Weaviate configurator \u2014 selecting the Transformers module*\n\n## Custom transformer models\nYou can also use custom transformer models that are compatible with Hugging Face's `AutoModel` and `AutoTokenzier`. Learn more about using custom models in Weaviate [here](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-transformers)."], "query": "How can I start using the transformer module in Weaviate?"}
{"relevant_passages": ["---\ntitle: How A.I. Creates Art - A Gentle Introduction to Diffusion Models\nslug: how-ai-creates-art\nauthors: [zain]\ndate: 2023-01-24\ntags: ['concepts']\nimage: ./img/hero.png\ndescription: \"Machine learning models can create beautiful and novel images. Learn how Diffusion Models work and how you could make use of them.\"\n---\n![How A.I. Creates Art](./img/hero.png)\n\n<!-- truncate -->\n\nOne of the major developments this past year were the advancements made in machine learning models that can create beautiful and novel images such as the ones below. Though machine learning models with the capability to create images have existed for a while, this previous year we saw a marked improvement in the quality and photo-realism of the images created by these models. ![Perception of the world](./img/perception_of_the_world.jpg)\n\nModels like [DALL\u00b7E 2](https://openai.com/product/dall-e-2), [Stable Diffusion](https://github.com/Stability-AI/stablediffusion) and others which are the technologies underlying many platforms such as Lensa and Midjourney are being used by millions of people and are quickly becoming main stream as people realize their potential. These models not only have the ability to dream up photo-realistic images when prompted with text input but can also modify given images to add details, replace objects or even paint in a given artists style."], "query": "What are diffusion models in AI, and can you give examples of their use in creating art?"}
{"relevant_passages": ["There are a couple of flavors to consider with this. Again reminiscent of our discussion of LLM Evaluation, we may want to use a more powerful LLM to generate the training data to produce a smaller, more economical model owned by you. Another idea could be to provide human annotations of response quality to fine-tune an LLM with instruction following. If you\u2019re interested in fine-tuning models, check out this [tutorial](https://brev.dev/blog/fine-tuning-mistral) from Brev on how to use the HuggingFace PEFT library. ### Concluding thoughts on RAG Knobs to Tune\nIn conclusion, we have presented the main knobs to tune in RAG systems:\n\n* Indexing: At the highest level, we consider when to just use brute force and when to bring in an ANN index."], "query": "What are some strategies for fine-tuning language models mentioned in the document?"}
{"relevant_passages": ["## Conclusion\nThank you so much for reading our overview of RAG Evaluation! As a quick recap, we began by covering new trends in using LLMs for Evaluation, providing enormous cost and time savings for iterating on RAG systems. We then provided some more background on the traditional metrics used to evaluate the RAG stack from Generation, to Search, and then Indexing. For builders looking to improve their performance on these metrics, we then presented some knobs to tune in Indexing, then Search, and Generation. We presented the incoming challenge of experiment tracking for these systems, and our view on what differentiates RAG evaluation from Agent evaluation. We hope you found this article useful! We would love to connect on **X** at [@ecardenas300](https://twitter.com/ecardenas300) and [@CShorten30](https://twitter.com/CShorten30)!\n\nimport WhatNext from '/_includes/what-next.mdx'\n\n<WhatNext />"], "query": "What does the document say about using LLMs for RAG Evaluation and the traditional metrics involved?"}
{"relevant_passages": ["* We made sure to reuse the same memory where possible. In the world of cakes, we optimized our eating technique to consume more cakes, and we only need one plate to eat all the cakes. On top of that, we've introduced other minor optimizations. So if you are curious about that, drop us a message on Slack, and we can chat some more. ### Results\n\nAs a result, the filtered aggregations are to **10-20x faster** and require less memory."], "query": "How much faster have filtered aggregations become after the recent optimizations?"}
{"relevant_passages": ["Armed with an understanding of RAG metrics and what we can tune to improve them. Let\u2019s discuss what experiment tracking may look like. ## Tuning Orchestration\nGiven the recent advances in LLM Evaluation and an overview of some of the knobs to tune, one of the most exciting opportunities is to tie all this together with experiment tracking frameworks. For example, a simple orchestrator that has an intuitive API for a human user to 1. request an exhaustive test of say: 5 LLMs, 2 embedding models, and 5 index configurations, 2."], "query": "What is an example of how an orchestrator with an intuitive API can be used in LLM evaluation and experiment tracking?"}
{"relevant_passages": ["And Weaviate will handle all the communication with Hugging Face. ### Step 3 \u2013 serving portions \u2013 querying data\nOnce, you imported some or all of the data, you can start running queries. (yes, you can start querying your database even during the import). Running queries also requires the same token. But you can reuse the same client, so you are good to go."], "query": "Can you start querying a database in Weaviate during the data import process?"}
{"relevant_passages": ["Thus, quality at the expense of speed, becomes more interesting. ### LLMs as Cross Encoders\n\nSo, let\u2019s dive into the LLM hype a little more, how can we use LLMs for re-ranking? There are generally 2 ways to do this. The first strategy is identical to the cross encoder, we give the LLM the [query, document] input and prompt it to output a score of how relevant the document is to the query. The tricky thing with this is bounding the score."], "query": "How are Large Language Models used as cross encoders for re-ranking, and what is the challenge associated with scoring?"}
{"relevant_passages": ["Almost all objects are unique and it is not a problem to process those concurrently. We found that what we really needed was just a lock for each unique UUID. Cleverly, this approach would ensure that only one object per UUID is handled at each point in time, so that Weaviate cannot add multiple instances of objects with the same UUID. Meanwhile, it would still allow full parallelization of import processes to maximize performance. ![Single-lock solution](./img/single-lock-solution.png)\n\nAs it often happens, implementing a lock-per-key solution created a different issue."], "query": "What concurrency control mechanism does Weaviate use to handle objects with the same UUID during import processes?"}
{"relevant_passages": ["![graf](./img/image11.png)\n**Fig. 10**: *Heap usage while loading the data into the Weaviate server. Memory does not grow smoothly. Instead there is a higher peak at the beginning before the vectors are compressed.*\n\nLets focus the comparison in three directions:\n- How much longer does it take to index compressed data? - How much does it affect the recall and latency?"], "query": "What is the impact of vector compression on heap usage, indexing time, recall, and latency in the Weaviate server?"}
{"relevant_passages": ["This leads to the simplest definition of a third database wave: A vector database that stores data indexed by machine learning models. Different types of databases (e.g., vector search engines) allow users to search through these vectorized datasets, and others (e.g., feature stores) allow users to store vectors at a large scale for later use. ## We're awash in unstructured data. We're living in a time of massive data accumulation and much, if not most of it, is unstructured: text, photos, video, audio files, as well as other things such as genetic information. Vector search is particularly good at extracting value from such data."], "query": "What is the third database wave and what types of unstructured data does vector search excel at handling?"}
{"relevant_passages": ["Let's explore how PQ works. ## Product Quantization\n![ann](./img/Ann.png)\n\nIf you already know the details behind how Product Quantization works feel free to skip this section!\n\nThe main intuition behind Product Quantization is that it adds the concept of segments to the compression function. Basically, to compress the full vector, we will chop it up and operate on segments of it. For example, if we have a 128 dimensional vector we could chop it up into 32 segments, meaning each segment would contain 4 dimensions. Following this segmentation step we compress each segment independently."], "query": "How does Product Quantization compress vectors?"}
{"relevant_passages": [":::infoEach video is only 3-6 minutes, and you do *not* need to watch them in order. :::\n\n:::note Tell us what you think!\nWe would love to know what you think of video content. Would you like to see more of them? Are there any particular types of videos you'd like more of? Please let us know below or on YouTube, and we'll do our best to listen."], "query": "How long are the videos mentioned in the document?"}
{"relevant_passages": ["This pillar is all about performance. The first big step will be the move towards a [Native Roaring Bitmap Index](https://github.com/weaviate/weaviate/issues/2511). In the most extreme case, this new index time can speed up filtered vector search [by a factor of 1000](https://twitter.com/etiennedi/status/1621180981519458305). But it doesn\u2019t stop there; we are already thinking about the next steps. Whether you want faster aggregations or new types of specialized indexes, we will ensure you can hit all your p99 latency targets with Weaviate."], "query": "What is the expected performance improvement factor for filtered vector search with the new Native Roaring Bitmap Index in Weaviate?"}
{"relevant_passages": ["This will take the candidates from hybrid search and apply the cross encoder to rank the final results. :::warning\nThis feature is not implemented into Weaviate yet, so the below code is an example of what it will look like. :::\n\n```graphql\n{\n  Get {\n    PodClip(\n      hybrid: {\n        query: \"How can I use ref2vec to build a home feed?\"\n        alpha: 0.5\n      }\n    ){\n    content\n    _additional {\n      crossrank(\n        query: \"How can I use ref2vec to build a homefeed?\"\n        property: \"content\"\n      ){\n      score\n    }\n  }\n}\n```\n\nLet\u2019s see an example of this in action using the Weaviate Podcast Search dataset!\n\n**Query**: Are there any ways to benchmark the performance of self-ask prompting? | Ranker                | Output |\n|---------------------------|------------------------------------|\n| Cross Encoder                  | That's a great question. Honestly, you should invite Ofir to chat about that."], "query": "How does the cross encoder feature work in Weaviate for ranking search results?"}
{"relevant_passages": ["The history of evaluating retrieval has required humans to annotate which documents are relevant for a query. So thus, to create 1 query annotation, we may need to annotate the relevance of 100 documents. This is already an immensely difficult task for general search queries, but becomes additionally challenging when building search engines for specific domains such as legal contracts, medical patient history, to give a few examples. To lighten the costs of labeling, heuristics are often used for search relevancy. The most common of which being the click log where: given a query, the title that was clicked on is likely relevant and the others are not."], "query": "What are the challenges in annotating documents for search engine relevance, and what heuristic is commonly used to reduce labeling costs?"}
{"relevant_passages": ["Weaviate generates vector embeddings using [modules](/developers/weaviate/modules/retriever-vectorizer-modules) (OpenAI, Cohere, Google PaLM etc.), and conveniently stores both objects and vectors in the same database. For example, vectorizing the two words above might result in:\n\n```text\ncat = [1.5, -0.4, 7.2, 19.6, 3.1, ..., 20.2]\nkitty = [1.5, -0.4, 7.2, 19.5, 3.2, ..., 20.8]\n```\n\nThese two vectors have a very high similarity. In contrast, vectors for \u201cbanjo\u201d or \u201ccomedy\u201d would not be very similar to either of these vectors. To this extent, vectors capture the semantic similarity of words. Now that you\u2019ve seen what vectors are, and that they can represent meaning to some extent, you might have further questions."], "query": "How does Weaviate generate and store vector embeddings for words?"}
{"relevant_passages": ["One technique is to prompt it with:\n`please output a relevance score on a scale of 1 to 100.`\n\nI think the second strategy is a bit more interesting, in which we put as many documents as we can in the input and ask the LLM to rank them. The key to making this work is the emergence of LLMs to follow instructions, especially with formatting their output. By prompting this ranking with \u201cplease output the ranking as a dictionary of IDs with the key equal to the rank and the value equal to the document id\u201d. Also interesting is the question around how many documents we can rank like this and how expensive it is. For example, if we want to re-rank 100 documents, but can only fit 5 in the input at a time, we are going to need to construct some kind of tournament-style decomposition of the ranking task."], "query": "What are two strategies for evaluating documents using large language models, and what are the considerations for ranking a large number of documents?"}
{"relevant_passages": ["In this article, we will cover the need for disk-based solutions, explore Vamana (how it works and contrast it against HNSW), and present the result of Vamana implementation. > If you are new to vector databases and ANN, I recommend you to read \"[Why is Vector Search so Fast?](/blog/why-is-vector-search-so-fast)\"<br/>\n> The article explains what vector databases are and how they work. ## Need for approximation\nIn order for a vector database to efficiently search through a vast number of vectors, the database needs to be smart about it. A brute-force approach would calculate the distance between the query vector and every data vector in the queried collection, but this is computationally very expensive. For a database with millions of objects and thousands of high-dimensional vectors, this would take far too long."], "query": "What is the focus of the article that discusses Vamana and its comparison with HNSW in the context of vector databases?"}
{"relevant_passages": ["Training a single, jack-of-all-modalities model is very difficult. Current multimodal approaches like [ImageBind](https://arxiv.org/abs/2305.05665) from FAIR, Meta AI(which combines image, speech, text, video, motion and depth/thermal data) approach this problem by taking separate specialist pre-trained models for each modality and then finetuning them to bind their latent space representations using a contrastive loss function; which essentially pulls together representations of similar examples across modalities closer together and pushes apart distinct examples in a joint vector space. The key insight is that all combinations of paired modalities are not necessary to train such a joint embedding, and only image-paired data is sufficient to align and bind the modalities together. The limitation however is that adding and finetuning a separate pretrained model for every modality or task becomes prohibitively expensive and is not scalable. ### 3."], "query": "What approach does ImageBind use to train a multimodal model, and what is its scalability limitation?"}
{"relevant_passages": ["2)** objects per query. The chart shows the recall vs. latency on the same Google Cloud setup Microsoft used. In both cases, you might notice that both Vamana and HNSW implementations are performing similarly. *Keep in mind that \u2013 while Vamana is the algorithm that powers DiskANN \u2013 at this stage, we are comparing both solutions in memory.*\n\n![Recall vs."], "query": "How do Vamana and HNSW implementations compare in terms of recall and latency in an in-memory setup on Google Cloud?"}
{"relevant_passages": ["How even with the most illustrious descriptions, shoppers struggle to find the products they\u2019re looking for. They knew that retail discoverability wasn\u2019t what it could be, and built Moonsift to improve it. > From the beginning, Wood and Reed\u2019s vision was to use the power of AI to help shoppers more easily discover the products they love. ## Collecting data to enable understanding \nThe first generation of Moonsift has been a stepping stone toward this vision. In order to create a comprehensive AI shopping copilot, they needed product data from retailers along with cross-retailer shopping data to enable understanding of the discovery process and train machine learning models."], "query": "What is the purpose of Moonsift, and what kind of data did its founders collect to train its AI?"}
{"relevant_passages": ["Does adding more search results to the input improve answer quality? The Lost in the Middle experiments have tempered expectations here a bit. In [Lost in the Middle](https://arxiv.org/abs/2307.03172), researchers from Stanford University, UC Berkeley, and Samaya AI presented controlled experiments showing that if relevant information was placed in the middle of search results, rather than the beginning or end, the language model would be unable to integrate it in the generated response. Another paper from researchers at Google DeepMind, Toyota, and Purdue University showed that [\u201cLarge Language Models Can Be Easily Distracted by Irrelevant Contex.\u201d](https://arxiv.org/abs/2302.00093) Although the potential is captivating, at the time of this writing, it seems early on for Long-Context RAG. Luckily, metrics such as the Ragas score are here to help us quickly test the new systems!\n\nSimilar to our earlier discussion on recent breakthroughs in LLM Evaluation, there are 3 stages of tuning for generation: 1."], "query": "Do language models perform worse when relevant information is placed in the middle of search results?"}
{"relevant_passages": ["These events are not just about innovation. They provide a safe and welcoming space to collaborate, meet in person, and celebrate successes. It was fantastic bringing together the community in person and meeting so many of you!\n\n<div className=\"youtube\">\n    <iframe src=\"//www.youtube.com/embed/_whxda27bCQ\" frameBorder=\"0\" allowFullScreen></iframe>\n</div>\nVector Database & Large Language Model Hackathon, Berlin, 2023 - In collaboration with the MLOps community, aimed at fostering teamwork and knowledge sharing in machine learning. ## Building with Weaviate\nIn the realm of Generative AI, especially with the emergence of OpenAI's ChatGPT, the technology landscape has had a significant impact. Users began experimenting with ChatGPT early in the year."], "query": "What was the purpose of the Vector Database & Large Language Model Hackathon in Berlin, 2023?"}
{"relevant_passages": ["These models are reaching new heights in performance because they leverage visual information, not just text. <figure>\n\n![Donut pipeline](./img/donut.png)\n<figcaption> Pipeline of Donut from Kim, G. et al (2022) </figcaption>\n</figure>\n\n\n## About Unstructured\n[Unstructured](https://www.unstructured.io/) is an open-source company working at the cutting edge of PDF processing and more. They allow businesses to ingest their diverse data sources, whether this be a `PDF`, `JPEG`, or `PPT`, and convert it into data that can be passed to a LLM. This means that you could take private documents from your company and pass it to a LLM to chat with your PDFs.\n\nUnstructured\u2019s open-source [core library](https://github.com/Unstructured-IO/unstructured) is powered by document understanding models."], "query": "What can Unstructured's open-source technology do with private company documents?"}
{"relevant_passages": ["See below for the Mona Lisa drip painted in the style of Jackson Pollock!\n\n![Mona lisa drip painting](./img/the_mona_lisa_drip_painted.jpg)\n\nThe technology behind these images is called **diffusion models**. In this post I aim to provide a gentle introduction to diffusion models so that even someone with minimal understanding of machine learning or the underlying statistical algorithms will be able to build a general intuition of how they work. Additionally I\u2019ll also provide some external resources that you can use to access pre-trained diffusion models so you can start to generate your own art!\n\nThese are the points that we\u2019ll expand on in this article:\n\n- How diffusion models can create realistic images\n- How and why we can control and influence the images these models create using text prompts\n- Access to some resources you can use to generate your own images. ## How Diffusion Models Work\n\nDiffusion models are a type of generative model - which means that they can generate data points that are similar to the data points they\u2019ve been trained on(the training set). So when we ask Stable Diffusion to create an image it starts to dream up images that are similar to the billions of images from the internet that it was trained on - it\u2019s important to note that it doesn\u2019t simply copy an image from the training set(which would be no fun!) but rather creates a new image that is similar to the training set."], "query": "What are diffusion models in machine learning, and how can they be used to generate art?"}
{"relevant_passages": ["## The Six Pillars for 2023\n\n### Ingestion and Search Pipelines\n![Ingestion and Search Pipelines](./img/search-pipeline.png)\nWeaviate\u2019s strong and growing module ecosystem gives you plenty of flexibility. Whether you use Weaviate as a pure vector search engine or with the addition of vectorizer, reader, and generator modules, you can always configure it to your liking. In early 2023 we even saw the addition of the [`generative-openai`](/developers/weaviate/modules/reader-generator-modules/generative-openai) module (with other generative modules to come). We want to give you even more flexibility in combining these steps this year. You can control arbitrary querying steps through the [proposed Pipe API](https://github.com/weaviate/weaviate/issues/2560), such as reading, re-ranking, summarizing, generating, and others."], "query": "What new module was added to Weaviate's ecosystem in early 2023?"}
{"relevant_passages": ["---\ntitle: An Overview on RAG Evaluation\nslug: rag-evaluation\nauthors: [erika, connor]\ndate: 2023-11-21\ntags: [concepts]\nimage: ./img/hero.png\ndescription: \"Learn about new trends in RAG evaluation and the current state of the art.\"\n---\n![hero](img/hero.png)\n\n<!-- truncate -->\n\nRetrieval Augmented Generation (RAG) is picking up steam as one of the most popular applications of Large Language Models and Vector Databases. RAG is the process of augmenting inputs to a Large Language Model (LLM) with context retrieved from a vector database, like [Weaviate](https://weaviate.io/). RAG applications are commonly used for chatbots and question-answering systems. Like any engineering system, evaluating performance is crucial to the development of RAG applications. The RAG pipeline is broken down into three components: 1."], "query": "What is Retrieval Augmented Generation (RAG) and how is it evaluated?"}
{"relevant_passages": ["And instead of the `text2vec-cohere` module, we will go straight to the Cohere API. Concatenating the text from the object:\n\n```python\nstr_in = ' '.join([i for i in properties.values()])\n```\n\nWe see:\n\n```text\n'In 1963, live on \"The Art Linkletter Show\", this company served its billionth burger McDonald\\'s'\n```\n\nThen, use the Cohere API to generate the vector like so, where `cohere_key` is the API key (keep it secret!), and `model` is the vectorizer. ```python\nimport cohere\nco = cohere.Client(cohere_key)\nco_resp = co.embed([str_in], model=\"embed-multilingual-v2.0\")\n```\n\nThen we run a `nearVector` based query to find the best matching object to this vector:\n\n```python\nclient.query.get(\n    \"Question\",\n    [\"question\", \"answer\"]\n).with_limit(2).with_near_vector(\n    {'vector': co_resp.embeddings[0]}\n).with_additional(['distance']).do()\n```\n\nInterestingly, we get a distance of `0.0181` - small, but not zero. In other words - we did something differently to Weaviate!\n\nLet's go through the differences one by one, which hopefully will help you to see Weaviate's default behavior. First, Weaviate sorts properties alphabetically (a-z) before concatenation."], "query": "Which company served its billionth burger live on \"The Art Linkletter Show\" in 1963?"}
{"relevant_passages": ["## How We Got Here\nFirst-wave database technology is often called by the acronym SQL\u2014the initials of a ubiquitous query language used to manage relational databases, which are conceptually similar to spreadsheets or tables. Throughout the 1980s, this technology was dominated by companies like Oracle and Microsoft. The second wave of databases is called \"NoSQL\". These are the domain of companies like MongoDB. They store data in different ways, for example, key-value stores, document stores, wide-column stores and graph databases, but what they all have in common is that they're not relational tables."], "query": "What type of database technology is associated with the company MongoDB?"}
{"relevant_passages": ["We need to explore each candidate one at a time. We sort the candidates using the compressed representations, stored in memory, and decide what the best next candidate to explore is. Exploring a candidate would mean fetching it from disk. Notice now that we are not reading all vectors from disk, but only the most promising candidates we wish to explore. This way we still need to host the compressed vectors in memory, but it will give us enough information to reduce disk reads significantly."], "query": "How does the use of compressed representations in memory help in reducing disk reads when exploring candidates?"}
{"relevant_passages": ["This means that a replication factor of 3 leads to the cluster essentially handling three times the load as no replication. The benefit here is of course redundancy and thus availability, but the cost is additional cost, such as increased hardware (e.g. memory) requirements. Note that the rolling update time was longer on our replication example than the non-replication example, as each pod now holds ~10,000 tenants rather than ~3,333 tenants. Another challenge is that any write request that comes in while any nodes are down will be temporarily missing on that node for a while. This will be repaired in time through a read-repair that [happens automatically in the background](/developers/weaviate/concepts/replication-architecture/consistency#repairs)."], "query": "What are the implications of using a replication factor of 3 on cluster load and update times?"}
{"relevant_passages": ["We don\u2019t believe it is clear what the interaction effect will be between continued training and keeping information fresh purely with RAG. Some research, such as MEMIT, experiments with updating facts such as \u201cLeBron James plays basketball\u201d to \u201cLeBron James plays soccer\u201d purely using causal mediation analysis of weight attribution. This is quite an advanced technique, and another opportunity could be simply tagging the chunks used in training such as \u201cLeBron James plays basketball\u201d and re-training with retrieval-augmented training data with the new information. This is a major area we are keeping an eye on. As mentioned earlier, we are also thinking of how to interface this kind of continual tuning directly into Weaviate with PQ centroids."], "query": "What are the methods being considered for updating factual information in retrieval-augmented generation models, and how might they be integrated with Weaviate?"}
{"relevant_passages": ["This is done with:\n\n```python\nfrom unstructured.partition.pdf import partition_pdf\n\nelements = partition_pdf(filename=\"../data/paper01.pdf\")\n```\n\nNow, if we want to see all of the elements that Unstructured found, we run:\n\n```python\ntitles = [elem for elem in elements if elem.category == \"Title\"]\n\nfor title in titles:\n    print(title.text)\n```\n\n<details>\n  <summary>Response from Unstructured</summary>\n\nA survey on Image Data Augmentation for Deep Learning\nAbstract\nIntroduction\nBackground\nImage Data Augmentation techniques\nData Augmentations based on basic image manipulations\nFlipping\nColor space\nCropping\nRotation\nTranslation\nNoise injection\nColor space transformations\nGeometric versus photometric transformations\nKernel filters\nMixing images\nRandom erasing\nA note on combining augmentations\nData Augmentations based on Deep Feature space augmentation\nData Augmentations based on Deep Learning\nFeature space augmentation\nAdversarial training\nGAN\u2011based Data Augmentation\nGenerated images\nNeural Style Transfer\nMeta learning Data Augmentations\nComparing Augmentations\nDesign considerations for image Data Augmentation\nTest-time augmentation\nCurriculum learning\nResolution impact\nFinal dataset size\nAlleviating class imbalance with Data Augmentation\nDiscussion\nFuture work\nConclusion\nAbbreviations\nAcknowledgements\nAuthors\u2019 contributions\nFunding\nReferences\nPublisher\u2019s Note\n\n</details>\n\nIf we want to store the elements along with the content, you run:\n\n```python\nimport textwrap\n\nnarrative_texts = [elem for elem in elements if elem.category == \"NarrativeText\"]\n\nfor index, elem in enumerate(narrative_texts[:5]):\n    print(f\"Narrative text {index + 1}:\")\n    print(\"\\n\".join(textwrap.wrap(elem.text, width=70)))\n    print(\"\\n\" + \"-\" * 70 + \"\\n\")\n```\n\nYou can then take this data, vectorize it and store it in Weaviate. ![PDFs to Weaviate](./img/Weaviate-ingesting-dark.png#gh-dark-mode-only)\n![PDFs to Weaviate](./img/Weaviate-ingesting-light.png#gh-light-mode-only)\n\n## End-to-End Example\nNow that we\u2019ve introduced the basics of using Unstructured, we want to provide an end-to-end example. We\u2019ll read a folder containing the two research papers, extract their abstracts and store them in Weaviate. Starting with importing the necessary libraries:\n\n```python\nfrom pathlib import Path\nimport weaviate\nfrom weaviate.embedded import EmbeddedOptions\nimport os\n```\n\nIn this example, we are using [Embedded Weaviate](/developers/weaviate/installation/embedded). You can also run it on [WCS](https://console.weaviate.cloud) or [docker](/developers/weaviate/installation/docker-compose)."], "query": "How can I extract titles from a PDF and store them in Weaviate using Python?"}
{"relevant_passages": ["There are many different ANN algorithms, each with different advantages and limitations. ### Large Scale\nWhen we talk about a vast number of objects, today we often see use cases with hundreds of millions of vectors, but it won't take long until billions, or even trillions, will be a pretty standard use case. To get vector databases to that kind of scale, we need to constantly evolve and look for more efficient solutions. A big part of this search is to explore ANN algorithms that would let us go beyond the available RAM (which is a bit of a bottleneck) without sacrificing the performance and the UX. ### What to expect\nIn this series of blog posts, we will take you on a journey with us as we research and implement new ANN algorithms in our quest to reach the 1T goal."], "query": "What is the future scalability goal for vector databases using ANN algorithms as mentioned in the blog series?"}
{"relevant_passages": ["We will look at the [ChatVectorDB chain](https://python.langchain.com/docs/modules/chains/popular/chat_vector_db), it lets you build an LLM that stores chat history and retrieves context from Weaviate to help with generation. To begin, the chat history in this chain uses the `stuff` configuration of CombineDocuments. This means we take as much of the chat history as we can fit in our context window and use it for a query reformulation prompt. The prompt is as follows:\n\n```python\n_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question. Chat History:\n{chat_history}\nFollow Up Input: {question}\nStandalone question:\"\"\"\n```\n\nWe put the chat_history and the latest user input in the curly bracket syntax."], "query": "What is the purpose of the `stuff` configuration in the ChatVectorDB chain's chat history, and how is it used in query reformulation?"}
{"relevant_passages": ["* The deeper the search traverses through the hierarchy, the shorter the distance between vectors captured in the edges. Put simply, Vamana can build a flat graph, in contrast to HNSW, which uses a hierarchical representation. And a flat graph may suffer less performance degradation from being stored on disk than a hierarchical representation might. The reason is that since the outgoing connections from each node are known, it is possible to store the information in such a way that we can calculate the exact position on the file to read when retrieving information on the neighbors of a given node. This makes the information retrieval process very efficient, and thus the lower speed imposed by disk storage becomes less of a problem."], "query": "Why might Vamana's flat graph structure perform better than HNSW's hierarchical representation when stored on disk?"}
{"relevant_passages": ["The points in the curve are obtained varying the amount of centroids. The \u201cbase\u201d curve refers to the fixed average time to calculate distances between uncompressed vectors.*\n\n### Gist\n\nFinally, we show results on the Gist database using the 1,000,000 vectors. We see a similar profile compared to Sift1M again. Latency is nearly 10 times slower but this time it is due to the fact that we have nearly 10 times more dimensions per vectors. ![res5](./img/image9.png)\n**Fig."], "query": "How does the latency of distance calculations on the Gist database compare to that of the Sift1M database, and what role do vector dimensions play?"}
{"relevant_passages": ["Then we will see how the text vectorization process can be tweaked, before wrapping up by discussing a few considerations also. ## Background\n\nI often find myself saying that Weaviate makes it fast and easy to produce a vector database from text. But it can be easy to forget just how fast and how easy it can make things. It is true that even in the \u201cold days\u201d of say, five to ten years ago, producing a database with vector capabilities was technically possible. You *simply* had to (*inhales deeply*) develop a vectorization algorithm, vectorize the data, build a vector index, build a database with the underlying data, integrate the vector index with the database, then forward results from a vector index query to the database and combine the outputs from both (*exhales*)."], "query": "What were the steps involved in creating a vector database before the advent of tools like Weaviate?"}
{"relevant_passages": ["The prompt is to summarize the abstract of the two papers in one sentence. This type of summarization is very useful when scouting out new research papers. This enables us to get a quick summary of the abstract and ask questions specific to the paper. ```python\nprompt = \"\"\"\nPlease summarize the following academic abstract in a one-liner for a layperson:\n\n{abstract}\n\"\"\"\n\nresults = (\n    client.query.get(\"Document\", \"source\").with_generate(single_prompt=prompt).do()\n)\n\ndocs = results[\"data\"][\"Get\"][\"Document\"]\n\nfor doc in docs:\n    source = doc[\"source\"]\n    abstract = doc[\"_additional\"][\"generate\"][\"singleResult\"]\n    wrapped_abstract = textwrap.fill(abstract, width=80)\n    print(f\"Source: {source}\\nSummary:\\n{wrapped_abstract}\\n\")\n```\n\n<details>\n  <summary>Output</summary>\n\n```\nSource: paper01.pdf\nSummary:\nData Augmentation is a technique that enhances the size and quality of training\ndatasets for Deep Learning models, particularly useful in domains with limited\ndata such as medical image analysis. ```\n```\nSource: paper02.pdf\nSummary:\nUsing machine learning techniques, researchers explore predicting house prices\nwith structured and unstructured data, finding that the best predictive\nperformance is achieved with term frequency-inverse document frequency (TF-IDF)\nrepresentations of house descriptions."], "query": "What are the one-sentence summaries of the abstracts for paper01.pdf and paper02.pdf as generated by the Python script?"}
{"relevant_passages": ["For example, many of our users already run Weaviate with multi-tenancy (introduced in version `1.20`) to host thousands of active tenants or even more. One side effect of scaling is that as load increases on each node, it will take longer to start up. While a fresh Weaviate instance typically starts up essentially instantaneously, a node with 1000s of tenants can take up over 1 minute. Node-level downtime is an unavoidable fact of life, since either hardware and software may necessitate restarts for maintenance and/or updates. But node-level downtime doesn\u2019t have to lead to user-level downtime and failed requests."], "query": "In which version of Weaviate was multi-tenancy introduced?"}
{"relevant_passages": ["---\ntitle: How we solved a race condition with the Lock Striping pattern\nslug: Lock-striping-pattern\nauthors: [dirk]\ndate: 2022-10-25\ntags: ['engineering']\nimage: ./img/hero.png\ndescription: \"The Lock Striping pattern is a great way to solve race conditions without sacrificing performance. Lean about Weaviate's improvements.\"\n---\n![How we solved a race condition with the Lock Striping pattern](./img/hero.png)\n\n<!-- truncate -->\n\n## Lock striping in database design\nDatabase design comes with interesting challenges. Like, dealing with race conditions when importing data in parallel streams. But for every new challenge, there is a clever solution. One of those clever solutions is Lock striping."], "query": "What is the Lock Striping pattern and how did it help solve race conditions in Weaviate's database design?"}
{"relevant_passages": ["Imagine that we are building a quiz app that allows our users to search for questions. Then, it may be preferable to import each quiz item into two classes, one for the question and one for the answer to avoid giving away the answer based on the user's query. But we may yet use the answer to compare to the user's input. On the other hand, in some cases, it may be preferable to import text data with many fields as one object. This will allow the user to search for matching meanings without as much consideration given to which field the information is contained in exactly."], "query": "What is a design consideration for structuring quiz items in a quiz app to enhance search functionality without revealing answers?"}
{"relevant_passages": ["Luckily, hybrid search comes to the rescue by combining the contextual semantics from the vector search and the keyword matching from the BM25 scoring. If the query is: `How can I use ref2vec to build a home feed?` The pairing of vector search and BM25 will return a good set of candidates. Now with the ranking model, it takes the [query, candidate document] pair as input and is able to further reason about the relevance of these results without specialized training. Let\u2019s begin with categories of ranking models. We see roughly 3 different genres of ranking models with:\n\n1."], "query": "What is hybrid search and how does it apply to building a home feed with ref2vec?"}
{"relevant_passages": ["the language model used for evaluation, such as GPT-4, Coral, Llama-2, Mistral, and many others. At the time of writing this blog post, people are mainly curious on the cost of evaluation using an LLM. Let\u2019s use GPT-4 as an example to see the cost of evaluating 10 search results, assuming 500 tokens per result and 100 tokens for the query and instructions, totaling roughly 6,000 tokens per LLM call to make the napkin math easier. Then assuming a rate of $0.005 per 1K tokens, this would cost $3 to evaluate 100 queries. The adoption of Zero-Shot LLM Evaluation from frameworks like Ragas is widely spreading."], "query": "How much does it cost to evaluate 100 queries using GPT-4, assuming 500 tokens per search result and a rate of $0.005 per 1K tokens?"}
{"relevant_passages": ["The fact that the graph is still in memory makes it hard to see the difference between those different levels of compression. The more we compress the lower the recall we would expect. Let us discuss the lowest level of compression along with some expectations. For Sift1M we would require roughly 1277 MB to 1674 MB of memory to index our data using uncompressed HNSW. This version would give us recall ranging from 0.96811 to 0.99974 and latencies ranging from 293 to 1772 microseconds."], "query": "What is the range of memory required to index the Sift1M dataset using uncompressed HNSW, and what recall and latency can be expected?"}
{"relevant_passages": ["Know someone who made a difference in your Weaviate journey? Nominate them as a Weaviate Hero! Let's recognize the people who make our community great. \ud83d\udc9a\n\nNominate your hero [here](https://docs.google.com/forms/d/e/1FAIpQLSePtgvTPZGx2gbCzVdyjitQ0WHrq4gNNZKiaqrQhphmJc3vJA/viewform). As we keep building, learning, and collaborating, we're excited to see the innovations that 2024 will bring. \ud83d\udcab\n\n![Weaviate Hero](img/Weaviate-hero_(1).gif)\n\nimport WhatNext from '/_includes/what-next.mdx'\n\n<WhatNext />"], "query": "How can I nominate someone for the Weaviate Hero recognition?"}
{"relevant_passages": ["This is also known as In-Context Learning, and the discovery of this technique was one of the key breakthroughs in GPT-3. For example, adding 5 examples of human relevance ratings, we add 30,000 tokens to the prompt. Assuming the same cost as above, we 5x our cost to evaluate 100 queries from $3 to $15. Note this is a toy example and not based on the real pricing models of LLMs. A key consideration here is that adding few-shot examples may require longer context models, which are currently priced higher than smaller input LLMs.\n\nThis is already a very attractive price for producing an LLM Evaluation with Zero-Shot or Few-Shot inference, but further research suggests that the price of LLM evaluation can be further reduced with Knowledge Distillation training algorithms. This describes taking an LLM, using it to generate training data for the task of evaluation and then fine-tuning it into a smaller model."], "query": "What is the technique that improves GPT-3's performance by adding examples to the prompt, and how does it affect the cost of evaluation?"}
{"relevant_passages": ["Latency when retrieving the ten approximate nearest neighbors](./img/fig-1.png)\n*Fig. 1: Recall vs. Latency when retrieving the ten approximate nearest neighbors.*\n\n\n![Recall vs. Latency when retrieving the hundred approximate nearest neighbors](./img/fig-2.png)\n*Fig. 2: Recall vs."], "query": "What is the relationship between recall and latency when retrieving the ten approximate nearest neighbors?"}
{"relevant_passages": ["Furthermore, you might be using Weaviate with your own data which might be even bigger than the datasets we report on below. If we extrapolate out, consider how big these memory requirements could grow as you add more objects or represent these objects with long vector embeddings. | DataSet     | Dimensions | Vectors   | Size in memory (MB) |\n|-------------|------------|-----------|---------------------|\n| Sift1M      | 128        | 1,000,000 | 512                 |\n| Gist        | 960        | 1,000,000 | 3840                |\n| DeepImage96 | 96         | 9,990,000 | 3836.16             |\n\n**Table 1**: *Description of datasets.*\n\nIncreasing the number of objects vs. storing longer dimensional vectors has an equivalent effect on the overall memory required to store the vectors. As an example consider the Gist dataset, which contains 1,000,000 vectors, each with 960 dimensions."], "query": "How much memory does the Gist dataset require to store its vectors?"}
{"relevant_passages": ["The reason for this is the fact that we have larger vectors. This means that the memory dedicated to vectors is larger than the memory dedicated to the graph and since we only compress vectors, then the larger the vectors the higher the compression rate. Having even larger vectors such as OpenAI embeddings (1536 dimensions) would reach even better compression rates. ![perf5](./img/image16.png)\n**Fig.15**: *The charts show heap usage. Top down it shows Sift1M, DeepImage and Gist."], "query": "Why do larger vectors result in higher compression rates in the context of memory usage?"}
{"relevant_passages": ["We would be amiss to not mention other search methods such as BM25F, or hybrid searches, both of which would be affected by these decisions. Now that you've seen exactly what happens behind the curtains, we encourage you to try applying these concepts yourself the next time you are building something with Weaviate. While the changes to the similarities were somewhat minor in our examples, in some domains and corpora their impact may be certainly larger. And tweaking the exact vectorization scheme may provide that extra boost your Weaviate instance is looking for. import StayConnected from '/_includes/stay-connected.mdx'\n\n<StayConnected />"], "query": "What search methods might be influenced by vectorization scheme adjustments in Weaviate?"}
{"relevant_passages": ["From this single data point, the child receives an abundance of information: they see how the dog moves, interacts, and reacts to the world around it, hear how the dog barks, see how the fur on the dog blows in the wind, can touch the dog to see how it feels and even smell the dog. So, from this single \u201cdata point\u201d the child extracts a very rich representation of multiple interdependent modalities of data that very distinctly define the concept of a dog. ![dogs](./img/dogs.jpg)\n\nOver time and with age, this fusion of sensory inputs gets more refined and nuanced and allows infants to develop higher levels of abstraction and understanding of objects' properties, such as shape, texture, and weight. Humans are such good multimodal reasoning engines that we do it without noticing - let's consider a practical scenario. Imagine you\u2019re on an airplane and only have your wireless headset that cannot be plugged into the in-flight entertainment system - a predicament I find myself in more often than not these days!\ud83d\ude05 So you start watching a movie with no sound and for the most part, you notice that you can get a pretty good, although imperfect, understanding of what\u2019s going on."], "query": "How do children develop a rich understanding of the concept of a dog through sensory experiences?"}
{"relevant_passages": ["---\ntitle: Weaviate 1.19 Release\nslug: weaviate-1-19-release\nauthors: [jp,zain,erika]\ndate: 2023-05-04\nimage: ./img/hero.png\ntags: ['release']\ndescription: \"Weaviate 1.19 introduces generative cohere module, gRPC API support, improved data types, and more.\"\n\n---\n\nimport Core119 from './_core-1-19-include.mdx' ;\n\n<Core119 />\n\nimport WhatsNext from '/_includes/what-next.mdx'\n\n<WhatsNext />\n\nimport Ending from '/_includes/blog-end-oss-comment.md' ;\n\n<Ending />"], "query": "What are the new features introduced in Weaviate 1.19?"}
{"relevant_passages": ["---\ntitle: The AI-First Database Ecosystem\nslug: the-ai-first-database-ecosystem\nauthors: [bob]\ndate: 2022-06-23\ntags: ['concepts']\nimage: ./img/hero.jpeg\n# canonical-url: https://www.forbes.com/sites/forbestechcouncil/2022/06/23/the-ai-first-database-ecosystem/\n# canonical-name: Forbes\ndescription: \"Learn about the vision of the AI-First Database Ecosystem, which drives the R&D of the databases of the future.\"\n---\n![The AI-First Database Ecosystem](./img/hero.jpeg)\n\n<!-- truncate -->\n\nA new ecosystem of smaller companies is ushering in a \"third wave\" of AI-first database technology. New search engines and databases brilliantly answer queries posed in natural language, but their machine-learning models are not limited to text searches. The same approach can also be used to search anything from images to DNA. Much of the software involved is open source, so it functions transparently and users can customize it to meet their specific needs. Clients can retain control of their data, keeping it safely behind their own firewalls."], "query": "What is the \"third wave\" of AI-first database technology about?"}
{"relevant_passages": ["The next natural question is: How do we compress vectors? ## How to Efficiently Compress Vectors\n\nThe main idea behind vector compression is to have a \u201cgood-enough\u201d representation of the vectors (as opposed to a perfect representation) so they take up less space in memory while still allowing us to calculate the distance between them in a performant and accurate way. Compression could come from different sources. We could, for example, aim to reduce redundant data to store information more efficiently. We could also sacrifice accuracy in the data in favor of space."], "query": "What is the main purpose of compressing vectors in data processing?"}
{"relevant_passages": ["He developed an Autonomous Testing Agent to enhance software testing efficiency, harnessing the power of [SuperAGI](https://www.linkedin.com/company/superagi/) and Weaviate. Meanwhile, [BYTE](https://lablab.ai/event/cohere-coral-hackathon/byte/byte-ai-based-nutrition-app), an AI-based nutrition app, clinched the top spot at the [Coral Cohere Hackathon](https://lablab.ai/event/cohere-coral-hackathon)! Ayesha and Moneebah built this project to transform and personalize nutrition advice. They used Weaviate\u2019s vector database for search and recommendation and multi-tenancy for data security. These projects offer just a glimpse of the boundless possibilities within the AI realm, pointing the way to a future where AI is more accessible, formidable, and transformative. So, what are you waiting for if you haven't already started building with Weaviate?"], "query": "What AI-based nutrition app won the top spot at the Coral Cohere Hackathon?"}
{"relevant_passages": ["For sure it might not be for everybody and every use case. But if you are using Weaviate at scale, in production, we believe enabling it will add significant value and encourage you to consider its use."], "query": "Should I enable a specific feature when using Weaviate in production at scale?"}
{"relevant_passages": ["---\ntitle: Hacktoberfest 2023 - Celebrating Open Source with Weaviate\nslug: hacktoberfest-2023\nauthors: [leonie]\ndate: 2023-10-02\ntags: []\nimage: ./img/weaviate-hacktoberfest-2023.png\ndescription: \"Join us in celebrating Hacktoberfest, a month-long celebration of open source!\"\n---\nimport hacktober_demo from './img/hacktoberfest_2023_demo.mp4';\n\n![Celebrating Hacktoberfest 2023 with Weaviate](./img/weaviate-hacktoberfest-2023.png)\n\nAt [Weaviate](https://weaviate.io/), we love open source! Join us in celebrating [Hacktoberfest](https://hacktoberfest.com/), a month-long celebration of open source!\nParticipants with four pull requests (PR) accepted between **October 1 - 31, 2023** will receive a unique digital reward [from Hacktoberfest](https://hacktoberfest.com/participation/). ## The Task\n\nWelcome to our demo sprint!\n\nIt is aimed at onboarding engineers and machine learning practitioners to open-source. In our [example use cases and demos](https://weaviate.io/developers/weaviate/more-resources/example-use-cases) page, we showcase what you can do with a [vector database](https://weaviate.io/blog/what-is-a-vector-database). Some of these demos have received more love than others recently, and we want to give them a little polish. ![Untitled](./img/weaviate-demos.png)\n\nThe gallery contains demos using Weaviate in different states."], "query": "What are the requirements to receive a digital reward during Weaviate's Hacktoberfest 2023 celebration?"}
{"relevant_passages": ["Use the hashtag #hacktoberfest2023 for increased visibility. :::\n\n\n## FAQ\n\n- **Will this count towards Hacktoberfest?** Yes, it definitely does! If your PR/MR is created between **October 1** and **October 31** (in any time zone, UTC-12 thru UTC+14), we will add the \"HACKTOBERFEST-ACCEPTED\" label to it. - **Where do I get help?** For any questions or assistance, contact us on our [Discourse](https://forum.weaviate.io/) and [Slack](https://weaviate.slack.com/) channels. - **I have a cool contribution idea. Can I still participate?** Awesome! Connect with us on our [Discourse](https://forum.weaviate.io/) or [Slack](https://weaviate.slack.com/) channels and we will figure it out."], "query": "Does a contribution count towards Hacktoberfest 2023 if it is submitted on October 1st?"}
{"relevant_passages": ["- **I don\u2019t know how to write code. Can I still contribute?** Yes, of course! You can make no-code contributions, e.g., by updating the README.md files. If you want to learn how to write code with a concrete example, we can help you find a good issue. Just ping us on our [Discourse](https://forum.weaviate.io/) or [Slack](https://weaviate.slack.com/) channels. ---\n\nHappy hacking, and let's make Hacktoberfest 2023 a memorable one together! \ud83d\ude80\n\nJump right in and have a look at our [example use cases and demos](https://weaviate.io/developers/weaviate/more-resources/example-use-cases) page."], "query": "Can I contribute to Hacktoberfest 2023 if I don't know how to code?"}
{"relevant_passages": ["We no longer talk about thousands of vectors but **hundred of millions** or **even billions**! Keeping all the vectors in memory and adding a graph representation of vector connections **requires a lot of RAM**. This sparked the emergence of a new set of algorithms that allow vectors to reside on disk instead of in memory whilst retaining high performance. Some prominent examples of disk-based solutions include [DiskANN](https://suhasjs.github.io/files/diskann_neurips19.pdf) and [SPANN](https://arxiv.org/abs/2111.08566). ## The future of Weaviate\nToday, users use Weaviate in production to serve large-scale use cases with single-digit millisecond latencies and massive throughput. But not every use case requires such a high throughput that the cost of keeping all indexes in memory is justified."], "query": "What are some examples of disk-based solutions for storing vectors that allow for high performance while not residing in memory?"}
{"relevant_passages": ["Here are the results:\n\n### Queries during maintenance - without replication\n\nThe below figure shows results from our setup with no replication. The area chart at the top shows how many requests failed, and the line graph shows pod readiness. We see immediately that over the course of the update time, just about one out of nine (11.5%) requests failed. ![Monitoring stats showing failures during restart](./img/queries_without_replication.png)\n\nAnd at times, it is even worse, with the failure rate being as high as one in three when the node is down and before it starts to load tenants. :::info\nThe failure rate here is less than one in three, as Weaviate is capable of loading tech tenant\u2019s data (i.e. shard) and making them available."], "query": "What was the failure rate of requests during maintenance without replication, and how does Weaviate perform with tech tenant's data in this scenario?"}
{"relevant_passages": ["<img\n  src={require('./img/knn-linear.png').default}\n  alt=\"kNN - O(n) complexity\"\n  style={{ maxWidth: \"50%\" }}\n/>\n\n*[Figure 2 - O(n) and O(log n) complexity]*\n\nIn summary, kNN search doesn't scale well, and it is hard to image using it with a large dataset in production. ## The answer - Approximate nearest neighbors (ANN)\nInstead of comparing vectors one by one, vector databases use [Approximate Nearest Neighbor (ANN) algorithms](https://en.wikipedia.org/wiki/Nearest_neighbor_search#Approximation_methods), which trade off a bit of accuracy (hence the A in the name) for a huge gain in speed. ANN algorithms may not return the true k nearest vectors, but they are very efficient. ANN algorithms maintain good performance (sublinear time, e.g. (poly)logarithmic complexity, see Figure 2) on very large-scale datasets. *Note that most vector databases allow you to configure how your ANN algorithm should behave."], "query": "Why is the k-nearest neighbors algorithm not suitable for large datasets, and what is the alternative approach to address this issue?"}
{"relevant_passages": ["So if you are a Weaviate user, we encourage you to update Weaviate to the latest release to take advantage of this improvement as well as many others. Thank you for reading, and see you next time!\n\n\nimport WhatNext from '/_includes/what-next.mdx'\n\n<WhatNext />"], "query": "Why should Weaviate users update to the latest release?"}
{"relevant_passages": ["---\ntitle: How to run an embedded vector database in 10 lines of code\nslug: embedded-local-weaviate\nauthors: [dan]\ndate: 2023-06-06\nimage: ./img/hero.png\ntags: ['how-to']\ndescription: \"The Weaviate server can be run locally directly from client code\"\n\n---\n\nimport Tabs from '@theme/Tabs';\nimport TabItem from '@theme/TabItem';\nimport FilteredTextBlock from '@site/src/components/Documentation/FilteredTextBlock';\nimport PyCode from '!!raw-loader!/_includes/code/embedded.py';\nimport TSCode from '!!raw-loader!/_includes/code/embedded.ts';\n\nYes, 10 Python lines of code, generously formatted with whitespace. Or 14 for TypeScript. Oh, and all your data stays private locally, and we don't charge you anything. We're also going to build a useful example, illustrating a testing scenario. Here's how."], "query": "How can I run a Weaviate vector database locally with just a few lines of Python or TypeScript code?"}
{"relevant_passages": ["Indexing, 2. Retrieval, and 3. Generation. RAG Evaluation is tricky because of the series of interacting components and the strain of collecting test data. This article will present an exciting development in using LLMs to produce evaluations and the state of RAG components."], "query": "What are the challenges and developments in evaluating RAG using LLMs?"}
{"relevant_passages": ["What I love about all this work, like self-ask, chain of thought, we're developing new querying languages. This is like us inventing SQL, except that we didn't design the database. The database came into being and we have to figure out how to interact with it. That example I mentioned about the IPython interaction, like that's a, again, it's a new querying language. And I honestly thought the most potent part of the self-ask wasn't even necessarily the self-ask part."], "query": "What does the document compare the development of new querying languages to?"}
{"relevant_passages": ["Charts to the left show Recall (vertical axis) Vs Heap usage (horizontal axis). Charts to the right show Heap usage (horizontal axis) Vs the different parameter sets. Parameter sets to achieve a larger graph (also producing a more accurate search) are charted from top down.*\n\nLet's sumamrize what we see in the above charts. We could index our data using high or low parameters set. Additionally, we could aim for different levels of compression."], "query": "How does heap usage affect recall and the accuracy of search results based on different parameter sets?"}
{"relevant_passages": ["First, modify the configuration file to enable a multi-node setup (e.g. 3), and add the `replicationConfig` setting to the collection definition like so:\n\n```json\n{\n  class: YOUR_CLASS_NAME,\n  ... replicationConfig: {\n    factor: 3\n  }\n}\n```\n\n:::tip Replication factor & nodes\nThe replication factor should be less than or equal to the number of nodes. :::\n\nOnce you\u2019ve modified the configuration file and set the replication factor, you should have a multi-node setup. If you are keen to try running a multi-node setup yourself, follow the optional guide here. Or you can read ahead ;)."], "query": "How do I configure a multi-node setup with a replication factor of 3?"}
{"relevant_passages": ["![vector embeddings example](./img/vector-embeddings-example.png)\n\nHow can computers mimic our understanding of language, and similarities of words or paragraphs? To tackle this problem, semantic search uses at its core a data structure called **vector embedding** (or simply, **vector** or **embedding**), which is an array of numbers. Here's how the semantic search above works, step by step:\n\n1. The [vector database](/blog/vector-library-vs-vector-database) computes a vector embedding for each data object as it is inserted or updated into the database, using a given model. 2."], "query": "What is vector embedding and how is it used in semantic search?"}
{"relevant_passages": ["You can reach us through:\n\n- Join our dedicated Hacktoberfest channel in our [Discourse community forum](https://forum.weaviate.io/), where we're ready to answer your questions. - Join our dedicated #hacktoberfest channel in our [Weaviate Community Slack](https://weaviate.slack.com/) channel to stay connected and receive real-time support. - Join our [newsletter](https://newsletter.weaviate.io/) and follow us on [Twitter](https://twitter.com/weaviate_io) and [Linkedin](https://www.linkedin.com/company/weaviate-io/mycompany/verification/) to receive updates. - Stay up to date with Weaviate's development by exploring the [Weaviate GitHub Repository](https://github.com/weaviate/weaviate). Don\u2019t forget to give us a \u2b50\ufe0f while you are there!\n\n:::info Pro Tip\nShare your process online and tag us on [Twitter](https://twitter.com/weaviate_io) and [LinkedIn](https://nl.linkedin.com/company/weaviate-io)."], "query": "How can I participate in the Weaviate community's Hacktoberfest event and stay updated on their developments?"}
{"relevant_passages": ["### The search part\nIn a similar fashion, whenever we run a query (like: \"What is the tallest building in Berlin?\"), a vector database can also convert it to a \"query\" vector. The task of a vector database is to identify and retrieve a list of vectors that are closest to the given vector of your query, using a distance metric and a search algorithm. This is a bit like a game of boules \u2013 where the small marker (jack) is the location of our query vector, and the balls (boules) are our data vectors \u2013 and we need to find the boules that are nearest to the marker. ## k-nearest neighbors (kNN)\nOne way to find similar vectors is with a simple [k-nearest neighbors (kNN) algorithm](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm), which returns the k nearest vectors, by comparing every data vector in the database to the query vector. In our boules example (as illustraded below), with 6 boules, the kNN algorithm would measure the distance between the jack and each of the 6 boules on the ground."], "query": "How does a vector database find the closest data vectors to a given query vector?"}
{"relevant_passages": ["We aim to do the latter in this post. Once we have the data compressed, we still need to be able to calculate distances. This can be accomplished in two ways: Either we compress vectors from the original space to a compressed space to store them and we decompress the vectors back to the original space when calculating the distance, or we define the distance function directly over the compressed space. Figures 1 and 2 graphically demonstrate the first and second options respectively. Notice the delta($\\delta$) term in the explanation."], "query": "What are the two methods for calculating distances with compressed data as demonstrated in Figures 1 and 2, and what does the delta ($\\delta$) term refer to in this context?"}
{"relevant_passages": ["Jay Alamar provided a helpful [visualization](https://jalammar.github.io/illustrated-word2vec/) around this equation. Several concepts (\u201cwoman\u201d, \u201cgirl\u201d, \u201cboy\u201d etc.) are vectorized into (represented by) an array of 50 numbers generated using the [GloVe model](https://en.wikipedia.org/wiki/GloVe). In [vector terminology](https://en.wikipedia.org/wiki/Vector_(mathematics)), the 50 numbers are called dimensions. The vectors are visualized using colors and arranged next to each word:\n\n![vector embeddings visualization](./img/vector-embeddings-visualization.png)\n*Credit: Jay Alamar*\n\nWe can see that all words share a dark blue column in one of the dimensions (though we can\u2019t quite tell what that represents), and the word \u201cwater\u201d _looks_ quite different from the rest, which makes sense given that the rest are people. Also, \u201cgirl\u201d and \u201cboy\u201d look more similar to each other than to \u201cking\u201d and \u201cqueen\u201d respectively, while \u201cking\u201d and \u201cqueen\u201d look similar to each other as well."], "query": "What is an example of a visualization that explains how words like \"woman\", \"girl\", \"boy\", \"king\", \"queen\", and \"water\" are represented in vector space using the GloVe model?"}
{"relevant_passages": ["The illustration below denotes how we see major components such as Planning, Memory, and Tools that jointly add significant power to your system, but also make it more difficult to evaluate. <img\n  src={require('./img/agents.png').default}\n  alt=\"Agents meme\"\n  style={{ maxWidth: \"60%\" }}\n/>\n\n\nA common next step for RAG applications is to add advanced query engines. For interested readers new to the concept, please check out [Episode 3](https://www.youtube.com/watch?v=Su-ROQMaiaw) of our LlamaIndex and Weaviate series that provides python code examples of how to get started. There are many different advanced query engines, such as the Sub-Question Query Engine, SQL Router, Self-Correcting Query Engine, and more. We are also considering how a promptToQuery API or Search Query Extractor could look like in Weaviate\u2019s modules."], "query": "What are some examples of advanced query engines mentioned for RAG applications?"}
{"relevant_passages": ["Embedding providers (e.g., Hugging Face or OpenAI)\n1. Neural framework (e.g., deepset or Jina)\n1. Feature stores (e.g., FeatureBase, FeatureForm or Tecton)\n1. Vector databases (e.g., Weaviate or Vertex)\n\nWhile the number of data that companies are collecting in their data warehouses keeps growing, the need for better, more efficient searches keeps growing too. The more data we collect, the more complex searching through it becomes."], "query": "What technologies are used to improve search capabilities in large data warehouses?"}
{"relevant_passages": ["---\ntitle: What to expect from Weaviate in 2023\nslug: what-to-expect-from-weaviate-in-2023\nauthors: [etienne]\ndate: 2023-02-14\ntags: ['engineering']\nimage: ./img/hero.png\ndescription: \"Learn about the six pillars outlining how Weaviate will evolve in 2023.\"\n---\n![What to expect from Weaviate in 2023](./img/hero.png)\n\nWithout a doubt, 2022 has been the most exciting year for Weaviate so far. The company and the product have grown tremendously, and we are incredibly excited about 2023. Weaviate\u2019s usage numbers are through the roof, and so are your feedback and requests for what you\u2019re still missing from Weaviate. <!-- truncate -->\n\nIn this blog post, I will introduce you to the six pillars outlining how Weaviate will get even better in 2023. Weaviate development is highly dynamic \u2013 we don\u2019t waterfall-plan for the entire year \u2013 but nevertheless, we want to give you the best possible overview of what to expect in the coming year."], "query": "What are the six pillars guiding the evolution of Weaviate in 2023?"}
{"relevant_passages": ["On the other hand, to use the Hugging Face module in Weaviate open source (`v1.15` or newer), you only need to set `text2vec-huggingface` as the default vectorizer. Like this:\n\n```yaml\nDEFAULT_VECTORIZER_MODULE: text2vec-huggingface\nENABLE_MODULES: text2vec-huggingface\n```\n\n## How to get started\n\n:::note\nThis article is not meant as a hands-on tutorial. For more detailed instructions please check the [documentation](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-huggingface). :::\n\nThe overall process to use a Hugging Face module with Weaviate is fairly straightforward. ![Recipe for using the Hugging Face module](./img/how-to-get-started-recipe.png)\nIf this was a cooking class and you were following a recipe."], "query": "How do you set the default vectorizer module to Hugging Face in Weaviate v1.15 or newer?"}
{"relevant_passages": ["## How are vector embeddings generated? The magic of vector search resides primarily in how the embeddings are generated for each entity and the query, and secondarily in how to efficiently search within very large datasets (see our [\u201cWhy is Vector Search so Fast\u201d](/blog/why-is-vector-search-so-fast) article for the latter). As we mentioned, vector embeddings can be generated for various media types such as text, images, audio and others. For text, vectorization techniques have evolved tremendously over the last decade, from the venerable [word2vec](https://en.wikipedia.org/wiki/Word2vec) ([2013](https://code.google.com/archive/p/word2vec/)), to the state-of-the-art transformer models era, spurred by the release of [BERT](https://en.wikipedia.org/wiki/BERT_(language_model)) in [2018](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html). ### Word-level dense vector models (word2vec, GloVe, etc.)\n[word2vec](https://wiki.pathmind.com/word2vec) is a [family of model architectures](https://www.tensorflow.org/tutorials/text/word2vec) that introduced the idea of \u201cdense\u201d vectors in language processing, in which all values are non-zero."], "query": "What are some significant milestones in the development of text vectorization techniques, and which model introduced the concept of dense vectors in language processing?"}
{"relevant_passages": ["The embeddings are placed into an index, so that the database can [quickly](/blog/why-is-vector-search-so-fast) perform searches. 3. For each query,\n    1. a vector embedding is computed using the same model that was used for the data objects. 2."], "query": "How does a database perform searches using vector embeddings?"}
{"relevant_passages": ["We could use regular HNSW to start building our index. Once we have added some vectors (a fifth of the total amount, for example) we could then compress the existing vectors and from this point on, we compress the vectors once they come into the index. The separation between loading data uncompressed, then compressing afterwards is necessary. The threshold of how much data needs to be loaded in prior to compressing (a fifth of the data) is not a rule though. The decision on when to compress should be taken keeping in mind that we need to send enough data for the Product Quantizer to infer the distribution of the data before actually compressing the vectors."], "query": "At what point in the index-building process using HNSW should vector compression begin, and is this threshold strict?"}
{"relevant_passages": ["And for more cost-sensitive applications, even 2 would introduce high availability and robustness to the system. As we've mentioned before - using Weaviate Cloud Services is a convenient way to set up a cluster with replication enabled. Set the `Enable High Availability` button to `Yes` at cluster creation time, and it will spin up a multi-node cluster with replication enabled, including the appropriate replication factor for each class. So there it is. We hope we\u2019ve convinced you of the benefits of using replication, and how easy it is to use."], "query": "How do you enable high availability for a Weaviate Cloud Services cluster during its creation?"}
{"relevant_passages": ["<br></br>\n\n### Cloud Operations & Scaling\n![cloud operations scaling](./img/cloud-operations-scaling.png)\n\nWhen we introduced [Replication](/developers/weaviate/concepts/replication-architecture) to Weaviate in late 2022, we celebrated a significant milestone. It\u2019s never been easier to achieve a highly available setup, and you can even dynamically scale your setup to increase throughput. 2023 is all about improving your cloud and operations experience. We will give you more control over [how to structure your workloads](https://github.com/weaviate/weaviate/issues/2586) in a distributed setup and more [flexibility to adapt to your ever-changing needs](https://github.com/weaviate/weaviate/issues/2228). And, of course, we\u2019re constantly working on making your distributed cluster [even more resilient](https://github.com/weaviate/weaviate/issues/2405)."], "query": "What feature was introduced to Weaviate in late 2022 to enhance scalability and availability?"}
{"relevant_passages": [":::\n\n## Enabling replication in Weaviate\n\nJust the simple act of enabling replication on a Weaviate cluster will provide huge benefits. Doing so might actually be simpler than you might imagine. ### How to enable replication on Weaviate Cloud Services (WCS)\n\nEnabling replication on a Weaviate Cloud Services cluster is as simple as selecting the `Enable High Availability` button at cluster creation time. (Not available on sandboxes.)\n\n\nThis will enable a multi-node configuration in Weaviate and ensures that each class is configured with the appropriate replication factor. ### How to enable replication on self-deployed Weaviate\n\nEnabling replication in a self-deployment setup such as a Docker or Kubernetes setup involves the following two steps."], "query": "How do you enable replication in a Weaviate Cloud Services cluster?"}
{"relevant_passages": ["- Topics varied from RAG and multimodal search to scaling AI applications from prototype into production. - We partnered with the [MLOps Community](https://mlops.community/), [AICamp](https://www.aicamp.ai/), and other companies across the AI ecosystem. The Weaviate World Tour will be continuing in 2024! \u2728 Check our [Weaviate Events & Webinars Page](https://weaviate.io/community/events) for upcoming conferences, meetups, webinars, and workshops. Subscribe to the [Weaviate Newsletter](https://newsletter.weaviate.io/) and contact us if you're interested in speaking or hosting. ![worldtour](img/Group_2597.png)\n\n### Weaviate Hero Program\n\nAs 2023 wraps up, we're thrilled to celebrate our amazing Weaviate Community! Introducing the *Weaviate Hero Program* to honor those who embody our values and actively support others with their knowledge and expertise."], "query": "What program was introduced to honor active supporters in the Weaviate Community as 2023 came to an end?"}
{"relevant_passages": ["\ud83e\udd17We hope to see many more big and small contributions in the coming months and years. **#CommunityRocks**\n\n## Cloud-native backups\n\n![Cloud-native backups](./img/cloud-native-backup.png)\n\nCreating and restoring database backups in Weaviate was one of the most requested features from the Weaviate community and customers. And, of course, database backups are not just about disaster recovery. Sometimes we need to migrate our data to a different environment. Maybe because our database grew and now we need more resources, or perhaps we need to set up a new developer environment."], "query": "What was one of the most requested features from the Weaviate community and customers related to database management?"}
{"relevant_passages": ["And it's a great question. I hadn't thought of it that way. I don't know where it is in the data and it may simply be that the model is mashing up all this other data that it has in an interesting way, the same way that you can get stable diffusion to give you like people sitting around a campfire on an airplane, I think I saw go by, or like salmon swimming in a stream where it was literal pieces of salmon. That was never anywhere in the training set, but it managed to mash it up and figure out what to do with it. And I wonder if something similar is happening here."], "query": "What can generative AI models do with data to create novel outputs?"}
{"relevant_passages": ["Our new schema is below - note the commented lines:\n\n```python\nquestion_class = {\n    \"class\": \"Question\",\n    \"description\": \"Details of a Jeopardy! question\",\n    \"moduleConfig\": {\n        \"text2vec-cohere\": {  # The vectorizer name - must match the vectorizer used\n            \"vectorizeClassName\": False,  # Ignore class name\n        },\n    },\n    \"properties\": [\n        {\n            \"name\": \"answer\",\n            \"description\": \"What the host prompts the contestants with.\",\n            \"dataType\": [\"string\"],\n            \"moduleConfig\": {\n                \"text2vec-cohere\": {\n                    \"skip\": False,  # Do not skip class\n                    \"vectorizePropertyName\": False  # Ignore property name\n                }\n            }\n        },\n        {\n            \"name\": \"question\",\n            \"description\": \"What the contestant is to provide.\",\n            \"dataType\": [\"string\"],\n            \"moduleConfig\": {\n                \"text2vec-cohere\": {\n                    \"skip\": False,  # Do not skip class\n                    \"vectorizePropertyName\": True  # Do not ignore property name\n                }\n            }\n        },\n    ]\n}\nclient.schema.create_class(question_class)\n```\n\nThe schema is defined such that at least some of the options, such as `moduleConfig`/`text2vec-cohere` /`vectorizeClassName` and `properties`/`moduleConfig`/`text2vec-cohere`/`vectorizePropertyName` differ from their defaults. And as a result, a `nearVector` search with the previously-matching Cohere API vector is now at a distance of `0.00395`. To get this back down to zero, we must revise the text generation pipeline to match the schema. Once we've done that, which looks like this:\n\n```python\nstr_in = ''\nfor k in sorted(input_props.keys()):\n    v = input_props[k]\n    if type(v) == str:\n        if k == 'question':\n            str_in += k + ' '\n        str_in += v + ' '\nstr_in = str_in.lower().strip()\n```\n\nSearching with the vector generated from this input, the closest matching object in Weaviate once again has a distance of zero. We've come full circle \ud83d\ude42."], "query": "What changes were made to the `text2vec-cohere` vectorization settings in the new \"Question\" class schema, and how was the text generation pipeline adjusted to fix the `nearVector` search distance issue?"}
{"relevant_passages": ["![workshops](img/workshops_3.png)\n\n## Building a Global Community\n\nThe heart of Weaviate lies in its **community**. Our strength comes from the combined efforts of our team and users, whether they're engaging in the [Community Slack](https://weaviate.slack.com/), participating in meetups, or using Weaviate in their projects. This year, we introduced the **Weaviate Hero Program**, an initiative by [Marion](https://www.linkedin.com/in/marionnehring/), our Community Manager, to honor those significantly contributing to our community's growth and success. ### Meetups and Events\n\n**Weaviate World Tour - End of Year Edition:** To ensure everyone can benefit from the knowledge, we decided to share our knowledge and connect, collaborate, and network with community members around the globe. We introduced our Weaviate World Tour: Year-End Special Edition! bringing tech experts to community events in Amsterdam, Berlin, London, San Francisco, and New York to hundreds of developers, data scientists, and AI enthusiasts."], "query": "What is the name of the program introduced by Weaviate to honor significant contributors to its community?"}
{"relevant_passages": ["For one, what does each number represent? That depends on the machine learning model that generated the vectors, and isn\u2019t necessarily clear, at least in terms of our human conception of language and meaning. But we can sometimes gain a rough idea by correlating vectors to words with which we are familiar. Vector-based representation of meaning caused quite a [stir](https://www.ed.ac.uk/informatics/news-events/stories/2019/king-man-woman-queen-the-hidden-algebraic-struct) a few years back, with the revelation of mathematical operations between words. Perhaps *the* most famous result was that of\n\n    \u201cking \u2212 man + woman \u2248 queen\u201d\n\nIt indicated that the difference between \u201cking\u201d and \u201cman\u201d was some sort of \u201croyalty\u201d, which was analogously and mathematically applicable to \u201cqueen\u201d minus \u201cwoman\u201d."], "query": "What is the famous result demonstrating the algebraic structure of word vectors in machine learning?"}
{"relevant_passages": ["Evaluating the performance of multi-hop query engines can be done by observing the sub questions. It is important that the LLM is creating relevant sub questions, answering each accurately, and combining the two answers to provide a factual and relevant output. Additionally, if you\u2019re asking complex questions, it is probably best to utilize the multi-hop query engine. A multi-hop question is firstly dependent on the accuracy of the sub-questions. We can imagine a similar LLM Evaluation used here with the prompt: \u201cGiven the question: {query}."], "query": "What is a multi-hop query engine and how is its performance evaluated?"}
{"relevant_passages": ["One of the primary reasons is the computational cost associated with processing and storing longer sequences of tokens. The longer the sequence, the more memory and processing power required to operate on it, which can be a significant challenge for even the most powerful computing systems. The relatively long input window of LLMs is what drives the integration with semantic search. For example, we can use this entire blog post as input in addition to questions if we want the LLM to answer questions such as \u201cWhat are LLM Chains?\u201d However, when we want to give the LLM an entire book or pages of search results, we need more clever techniques to decompose this task. This is where the `CombineDocuments` chain comes to play! Note, that one method is not better than another, and the performance is dependent solely on your application."], "query": "Why might the `CombineDocuments` chain be used in the context of large language models (LLMs)?"}
{"relevant_passages": ["* Making the copy function from Go to move full memory sections. ![Recall vs. Latency when retrieving the ten approximate nearest neighbors](./img/fig-3.png)\n*Fig. 3: Recall vs. Latency when retrieving the ten approximate nearest neighbors.*\n\n![Recall vs."], "query": "What is the relationship between recall and latency in the context of retrieving the ten approximate nearest neighbors according to Fig. 3?"}
{"relevant_passages": ["In [ARES](https://arxiv.org/abs/2311.09476): An Automated Evaluation Framework for Retrieval-Augmented Generation Systems, Saad-Falcon et al. found that training your own LLM evaluator can have a better performance than zero-shot prompting. To begin, \u201cARES requires three inputs for the pipeline: a set of passages from the target corpus, a human preference validation set of 150 annotated datapoints or more, and five few-shot examples of in-domain queries.\u201d ARES then uses the few-shot examples of queries to generate a large dataset of synthetic queries. These queries are then filtered using the roundtrip consistency principle: Can we retrieve the document that produced the synthetic query when searching with the synthetic query? In addition to the positive chunk that was used to create the synthetic query, ARES adds weak negatives by randomly sampling other chunks from other documents in the corpus and strong negatives by either looking for a chunk in the same document as the one used to produce the query, or if unavailable, using one of the top-10 results from a BM25 query."], "query": "What are the inputs and steps involved in the ARES framework for evaluating retrieval-augmented generation systems?"}
{"relevant_passages": ["Due to the large dataset size mentioned earlier, there can be millions or even billions of objects with unique UUIDs in Weaviate, and creating a lock for each of them would require a lot of memory. We found an elegant solution that is in-between both of the solutions above - a **lock striping** pattern. ## Solving both challenges\nBased on the UUID we assign each object to one of the 128 locks. This process is deterministic so objects with an identical UUID will always use the same lock. This gives us the best of both worlds: we have a small, fixed number of locks, but it still guarantees that two objects with the same UUID are never processed concurrently."], "query": "What lock management strategy does Weaviate use to handle millions of objects with unique UUIDs efficiently?"}
{"relevant_passages": ["- How much do we save in memory requirements? ### Performance Results\n\nIn the following figures we show the performance of HNSW+PQ on the three databases used above. Notice how compressing with KMeans keeps the recall closer to the uncompressed results. Compressing too aggressively (KMeans with a few dimensions per segment) improves memory, indexing and latency performance but it rapidly destroys the recall so we recommend using it discreetly. Notice also that KMeans encoding with as many segments as dimensions ensures a 4 to 1 compression ratio."], "query": "What is the compression ratio when using KMeans encoding with as many segments as dimensions?"}
{"relevant_passages": ["This will give us two important results, the time it would take to fit the data with KMeans clustering and compress the vectors (which will be needed at some step by the indexing algorithm) and the distortion introduced by reconstructing the compressed vectors to calculate the distance. This distortion is measured in terms of a drop in recall. We considered three datasets for this study: Sift1M, Gist1M and DeepImage 96. Table 1, shown above, summarizes these datasets. To avoid overfitting, and because this is how we intend to use it in combination with the indexing algorithm, we use only 200,000 vectors to fit KMeans and the complete data to calculate the recall."], "query": "What were the results of the study on the time and distortion of KMeans clustering with Sift1M, Gist1M, and DeepImage 96 datasets?"}
{"relevant_passages": ["* Advanced monitoring and replication capabilities. * High query throughput at scale. * Unique search features to drive performance and efficiency (using important aspects of both keyword and vector search). > \u201cWeaviate was exactly what we needed. Within a couple of weeks we had a production-ready AI-powered search engine."], "query": "What database technology offers advanced monitoring, replication, high query throughput, and unique search features for AI-powered search engines?"}
{"relevant_passages": ["This is bad for three reasons:\n* Grabbing a new plate for each cake and then washing it takes time - **higher CPU use** with many GC cycles. * We would pile up many plates between each wash - **high memory consumption** between each GC cycle. * We might run out of clean plates - **OOM crash** if we **run out of RAM**. ### The solution\nTo solve the problem, we implemented two solutions:\n* We created our own library for reading binary data optimized for the Weaviate-specific needs. It makes fewer temporary memory allocations."], "query": "What problems does the custom library for reading binary data solve in the context of Weaviate?"}
{"relevant_passages": ["That \"wisdom of the crowd\" approach worked, but to improve the quality of the results it returned, Google needed RankBrain to \"understand\" the text it searched through. So, it used machine learning to vectorize (the process happening inside machine learning models such as transformers) text on sites and in links. Returning to the grocery store for a moment, the challenge comes because a grocery store is a three-dimensional space, but every significant word in unstructured text data needs to be related to hundreds of other words that it is frequently associated with. So, machine learning systems automatically classify text in hypervolumes\u2014imaginary spaces with hundreds or even thousands of dimensions. For any given item in a database, those vectors form what's known as a \"representation\" of the item."], "query": "How does Google's RankBrain use machine learning to understand and represent text?"}
{"relevant_passages": ["So we can *see* that these vector embeddings of words align with our intuitive understanding of meaning. And even more amazingly, vector embeddings are not limited to representing meanings of words. In fact, effective vector embeddings can be generated from any kind of data object. Text is the most common, followed by images, then audio (this is how Shazam recognizes songs based on a short and even noisy audio clip), but also time series data, 3D models, video, molecules etc. Embeddings are generated such that two objects with similar semantics will have vectors that are \"close\" to each other, i.e. that have a \"small\" distance between them in [vector space](https://en.wikipedia.org/wiki/Vector_space_model)."], "query": "What are vector embeddings and what types of data can they be applied to?"}
{"relevant_passages": ["It takes one input/output and then uses the output for the next step. Let\u2019s look at an example of sequential chains in a conversation between the bot and me. This visualization shows an example of a step-by-step reasoning to fact check an LLM tasked with question answering:\n\n<img\n    src={require('./img/sequential-chains.gif').default}\n    alt=\"alt\"\n    style={{ width: \"100%\" }}\n/>\n\n<div style = {{textAlign: \"center\"}}>\n\n*All credit to [jagilley](https://github.com/jagilley/fact-checker) for creating this awesome example*\n\n</div>\n\nWhen we ask an LLM, \u201cWhat type of mammal lays the biggest eggs?\u201d, it initially answers \u201cThe biggest eggs laid by any mammal belong to the elephant.\u201d This is a clear example of hallucination, where the response is misleading and incorrect. By adding in a simple chain of asking the LLM to reason about its assumptions, we are able to fix the hallucination problem. ## CombineDocuments\nLLMs have a limited token length due to various practical and technical constraints."], "query": "How can sequential chains be used to correct hallucinations in language model responses?"}
{"relevant_passages": ["\ud83d\ude01\n:::\n\n## Videos\n\n### Introduction to authentication in Weaviate\n\n<ReactPlayer className=\"react-player\" url='https://youtu.be/5xB5cRUFe2M' controls=\"true\"/>\n<br/>\n\n#### <i class=\"fa-solid fa-video\"></i> Timestamps\n\n- 0:00 Overview\n- 0:13 What is authentication? & Key considerations\n- 0:58 Overview of available authentication options in Weaviate (anonymous / API key authentication / OIDC authentication)\n- 1:54 General Recommendations\n\n### Authentication: A client-side perspective\n\n<ReactPlayer className=\"react-player\" url='https://youtu.be/I3o93Co1Ezg' width=\"100%\" controls=\"true\"/>\n<br/>\n\n#### <i class=\"fa-solid fa-video\"></i> Timestamps\n\n- 0:00 Overview\n- 0:28 Getting authentication information from WCS\n- 2:10 Authenticating against Weaviate\n- 2:28 Anonymous access\n- 3:01 API key authentication\n- 3:45 OIDC (username+password) authentication\n- 4:21 Read-only key\n- 4:38 Authentication in action\n- 5:36 Wrap-up\n\n### Authentication: Key concepts\n\n<ReactPlayer className=\"react-player\" url='https://youtu.be/Ok9AcWK0R38' width=\"100%\" controls=\"true\"/>\n<br/>\n\n#### <i class=\"fa-solid fa-video\"></i> Timestamps\n\n- 0:00 Overview\n- 0:31 Anonymous access\n- 0:46 Authentication\n- 0:58 API key authentication\n- 1:04 OIDC authentication\n- 1:36 Authentication & Authorization\n- 1:52 A comparison of options\n- 2:09 Additional complexities in OIDC\n- 2:54 summary\n\n### Authentication: A server-side perspective\n\n<ReactPlayer className=\"react-player\" url='https://youtu.be/0oxL1J0W-Hs' width=\"100%\" controls=\"true\"/>\n<br/>\n\n#### <i class=\"fa-solid fa-video\"></i> Timestamps\n\n- 0:00 Overview\n- 0:35 Weaviate without authentication\n- 1:39 Setting up API key access\n- 2:32 Enabling authorization (tiered access)\n- 3:46 Setting up OIDC access\n- 5:30 Enabling authorization with OIDC\n- 5:54 Summary\n- 6:02 Relevant environment variables\n\n## Read more:\n\n- [How to configure authentication](/developers/weaviate/configuration/authentication)\n- [How to configure authorization](/developers/weaviate/configuration/authorization)\n- [References: Environment variables](/developers/weaviate/config-refs/env-vars)\n- [Weaviate clients](/developers/weaviate/client-libraries/)\n\nimport WhatNext from '/_includes/what-next.mdx'\n\n<WhatNext />"], "query": "What are the available authentication options in Weaviate as discussed in the \"Introduction to authentication in Weaviate\" video?"}
{"relevant_passages": ["Fortunately, there are emerging technologies that help solve this limitation. <!-- truncate -->\n\n[LangChain](https://langchain.readthedocs.io/en/latest/) is one of the most exciting new tools in AI. LangChain helps overcome many limitations of LLMs such as hallucination and limited input lengths. Hallucination refers to where the LLM generates a response that is not supported by the input or context \u2013 meaning it will output text that is irrelevant, inconsistent, or misleading. As you can imagine, this is a huge problem in many applications."], "query": "What is LangChain, and what limitations of LLMs does it help to overcome?"}
{"relevant_passages": ["In the future, we expect further optionality ablating the capacity of the model due to inherent trade-offs of performance and latency that may make sense for some applications but not others. Discovering jointly which capacity re-ranker is needed and how many retrieved results to re-rank is another challenge for tuning the knobs in retrieval. This is also one of the lowest hanging fruit opportunities for fine-tuning custom models in the RAG stack, which we will discuss further in \u201cTuning Orchestration\u201d. Another interesting knob to tune is Multi-Index Search. Similar to our discussion on chunking, this is a tricky one that may involve structural changes to the database."], "query": "What are some of the challenges and opportunities in fine-tuning retrieval systems in the RAG stack?"}
{"relevant_passages": ["HNSW, on the other hand, implements the same idea a bit differently. Instead of having all information together on a flat graph, it has a hierarchical representation distributed across multiple layers. The top layers only contain long-range connections, and as you dive deeper into the layers, your query is routed to the appropriate region where you can look more locally for your answer. So your search starts making only big jumps across the top layers until it finally looks for the closest points locally in the bottom layers. ## Performance comparison\nSo, how do they perform?"], "query": "How does the HNSW algorithm structure its graph and conduct searches differently from a flat graph approach?"}
{"relevant_passages": ["Latency when retrieving the hundred approximate nearest neighbors](./img/fig-4.png)\n*Fig. 4: Recall vs. Latency when retrieving the hundred approximate nearest neighbors.*\n\n## So, when do SSDs come into play? But wait, is this the end of it? Was moving to disk, not the final goal?"], "query": "What is the relationship between recall and latency in retrieving the hundred approximate nearest neighbors using SSDs?"}
{"relevant_passages": ["LLMs have a limited input length when referring to the scale of inputting a book or pages of search results. LangChain has various techniques implemented to solve this problem. This blog post will begin by explaining some of the key concepts introduced in LangChain and end with a demo. The demo will show you how to combine LangChain and Weaviate to build a custom LLM chatbot powered with semantic search!\n\n## Sequential Chains\n[Chains](https://python.langchain.com/docs/modules/chains/) enable us to combine multiple LLM inferences together. As you can guess from the name, sequential chains execute their links in a sequential order."], "query": "What is the purpose of Sequential Chains in LangChain?"}
{"relevant_passages": ["Then, you just run the queries, as per usual:\n```javascript\nnearText = {\n    \"concepts\": [\"How to use Hugging Face modules with Weaviate?\"],\n    \"distance\": 0.6,\n}\n\nresult = (\n    client.query\n    .get(\"Notes\", [\n        \"name\",\n        \"comment\",\n        \"_additional {certainty distance} \"])\n    .with_near_text(nearText)\n    .do()\n)\n```\n\n## Summary\n> Now you can use [Hugging Face](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-huggingface) or [OpenAI](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-openai) modules in Weaviate to delegate model inference out. Just pick the model, provide your API key and start working with your data. Weaviate optimizes the communication process with the Inference API for you, so that you can focus on the challenges and requirements of your applications. No need to run the Inference API yourself. ## What next\nCheck out the [text2vec-huggingface](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-huggingface) documentation to learn more about the new module."], "query": "How can I use Hugging Face modules to run queries in Weaviate without managing the Inference API myself?"}
{"relevant_passages": ["Add any ordered input, for example, 1, 2, 3, 4 and see how the tree stays balanced. ### Results\nWe've run a few local tests to paint a better picture of what you could expect. First, we saw that the RB-Tree is a factor of 20 faster than the binary tree when adding objects with sequential keys (just the tree, without anything else). With a full import test, we saw a **3x performance improvement** \ud83d\ude80. * Weaviate `1.14.1` - import time **~38 minutes**\n* Weaviate `1.15.0` - import time **~13 minutes** \ud83d\udd25\n\n## More efficient filtered aggregations\n\n![More efficient filtered aggregations](./img/filtered-aggregation.png)\n\nRecently we've been working with a customer who was running multiple filtered aggregations on a large dataset."], "query": "How much did the import time improve from Weaviate version 1.14.1 to 1.15.0?"}
{"relevant_passages": ["- 4th October 2023 (20:00-21:00 UTC+2): [Introduction to Weaviate](https://form.jotform.com/232574048361254) with Zain Hasan, Senior Dev Advocate @ Weaviate\n- 5th October 2023 (17:00-18:00 UTC+2): [NEW Python `Collections` Client API Preview](https://form.jotform.com/232683153137859) with JP Hwang - Educator @ Weaviate\n- 18th October 2023 (15:00-16:00 UTC+2): [Introduction to Weaviate](https://form.jotform.com/232602295283859) with JP Hwang - Educator @ Weaviate\n\nExpand your knowledge with these supplementary resources:\n\n- [Weaviate YouTube Channel](https://www.youtube.com/@Weaviate) \ud83d\udcfa\n- [Weaviate Blog](https://weaviate.io/blog) \ud83d\udcf0\n- [Weaviate Recipes](https://github.com/weaviate/recipes) \ud83c\udf73\n\n\n## What's in it for you? The repositories you contribute to participate in [Hacktoberfest](https://hacktoberfest.com/) and are a great opportunity for your first-ever Hacktoberfest PR. We will also assist you throughout the whole process. You might even receive some swag in the end. ## Connect with the Weaviate Community!\n\nTo make your Hacktoberfest experience successful, connect with the Weaviate community for collaboration and assistance."], "query": "Who is hosting the 'Introduction to Weaviate' webinar on 4th October 2023?"}
{"relevant_passages": ["Then we take this new query and hit the Weaviate vector database to get context to answer the question. The ChatVectorDB chain we use has a default value of k = 4 search results, if we use longer search results we will need another CombineDocuments chain here as well! With the 4 search results, we answer the question with this final prompt:\n\n```python\nPrompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. {context}\nQuestion: {question}\nHelpful Answer:\"\"\"\n```\n\nHopefully this was a nice look under the hood of how the ChatVectorDB chain works. Let\u2019s get into how we can use this with Weaviate!\n\n### The Code\n\nIf this is your first time using Weaviate, please check out the [Quickstart tutorial](/developers/weaviate/quickstart)."], "query": "How does the ChatVectorDB chain in Weaviate use context to answer questions?"}
{"relevant_passages": ["5**: *Average time (microseconds) to calculate distances from query vectors to all 1,000,000 vectors compared to the recall achieved. The different curves are obtained varying the segment length (shown in the legend) from 1 to 8 dimensions per segment. The points in the curve are obtained varying the amount of centroids. The \u201cbase\u201d curve refers to the fixed average time to calculate distances between uncompressed vectors.*\n\nNotice some important findings from the experiments. Latency could vary significantly depending on the settings we use."], "query": "What is the relationship between segment length and recall in the context of calculating distances from query vectors to a dataset of 1,000,000 vectors?"}
{"relevant_passages": ["This demo is built off of Connor Shorten\u2019s [Podcast Search](https://github.com/weaviate/weaviate-podcast-search) demo. We are connecting to our Weaviate instance and specifying what we want LangChain to see in the `vectorstore`. `PodClip` is our class and we want to use the `content` property, which contains the transcriptions of the podcasts. Next in `qa` we will specify the OpenAI model. ```python\nfrom langchain.vectorstores.weaviate import Weaviate\nfrom langchain.llms import OpenAI\nfrom langchain.chains import ChatVectorDBChain\nimport weaviate\n\nclient = weaviate.Client(\"http://localhost:8080\")\n\nvectorstore = Weaviate(client, \"PodClip\", \"content\")\n\nMyOpenAI = OpenAI(temperature=0.2,\n    openai_api_key=\"sk-key\")\n\nqa = ChatVectorDBChain.from_llm(MyOpenAI, vectorstore)\n\nchat_history = []\n\nprint(\"Welcome to the Weaviate ChatVectorDBChain Demo!\")\nprint(\"Please enter a question or dialogue to get started!\")\n\nwhile True:\n    query = input(\"\")\n    result = qa({\"question\": query, \"chat_history\": chat_history})\n    print(result[\"answer\"])\n    chat_history = [(query, result[\"answer\"])]\n```\nAnd just like that, you have built an application using LangChain and Weaviate."], "query": "How do you create a chatbot using LangChain and Weaviate that answers questions based on podcast transcriptions?"}
{"relevant_passages": ["The above visual illustrates converting a research paper into chunks based on the heading. For example, chunk 1 is the abstract, chunk 2 is the introduction, and so on. Additionally, there are methods to combine chunks and have an overlap. Including a rolling window takes tokens from the previous chunk and begins the next chunk with it. The slight overlap of chunks can improve the search since the retriever will understand the previous context/chunk."], "query": "How does the rolling window method improve search functionality in the context of chunking a research paper?"}
{"relevant_passages": ["[Here](https://github.com/databyjp/distyll) is an example of a demo project in a good state. While some may only need a little polish of the description (README.md file), others are e.g., a little older or can use a little makeover for the user interface. The steps to your first Hacktoberfest PR are simple:\n\n1. Find an issue you're interested in: Go to the [example use cases and demos](https://weaviate.io/developers/weaviate/more-resources/example-use-cases) page and select a project marked for Hacktober with a \ud83c\udf83\u00a0sign. Click on \u201ccode\u201d to get to the repository and have a look at its issues."], "query": "How can I contribute to a Hacktoberfest project according to the instructions provided in the linked document?"}
{"relevant_passages": ["![Conceptual diagram of sending a request with authentication credentials](./img/auth_light.png#gh-light-mode-only)\n![Conceptual diagram of sending a request with authentication credentials](./img/auth_dark.png#gh-dark-mode-only)\n\nIn other words, the server can provide as much access as the particular user is allowed. But balancing security with usability can be a tricky line to draw, as everybody has different needs and often use different systems. So, we thought that this might be a good time to provide an overview of all things authentication in Weaviate. Also, we've recently introduced an API key-based authentication method, which we think might be a good balance of security and usability for many of you. Please check them out below."], "query": "What new authentication method has Weaviate recently introduced to balance security and usability?"}
{"relevant_passages": ["The points in the curve are obtained varying the amount of centroids. The \u201cbase\u201d curve refers to the fixed average time to calculate distances between uncompressed vectors.*\n\nAs we should expect, Product Quantization is very useful for saving memory. It comes at a cost though. The most expensive part is fitting the KMeans clustering algorithm. To remedy this we could use a different encoder based on the distribution of the data, however that is a topic we shall save for later!\n\nA final word on performance, not all applications require a high recall."], "query": "What is the trade-off of using Product Quantization for saving memory in terms of computational cost, and how does it affect recall in applications?"}
{"relevant_passages": ["However, this can be done by aggregating vectors of constituent words, which is often done by incorporating weightings such that certain words are weighted more heavily than others. However, word2vec still suffers from important limitations:\n* it doesn\u2019t address words with multiple meanings (polysemantic): \u201crun\u201d, \u201cset\u201d, \u201cgo\u201d, or \u201ctake\u201d each have [over 300 meanings](https://www.insider.com/words-with-the-most-definitions-2019-1) (!)\n* it doesn\u2019t address words with ambiguous meanings: \u201cto consult\u201d can be its own antonym, like [many other words](http://www.dailywritingtips.com/75-contronyms-words-with-contradictory-meanings/)\n\nWhich takes us to the next, state-of-the-art, models. ### Transformer models (BERT, ELMo, and others)\nThe current state-of-the-art models are based on what\u2019s called a \u201ctransformer\u201d architecture as introduced in [this paper](https://arxiv.org/abs/1706.03762). [Transformer models](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)) such as BERT and its successors improve search accuracy, [precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall) by looking at every word\u2019s context to create full contextual embeddings (though [the exact mechanism of BERT\u2019s success is not fully understood](https://aclanthology.org/D19-1445/)). Unlike word2vec embeddings which are context-agnostic, transformer-generated embeddings take the entire input text into account\u2014each occurrence of a word has its own embedding that is modified by the surrounding text."], "query": "What are the limitations of word2vec in handling words with multiple meanings, and how do transformer models like BERT address this issue?"}
{"relevant_passages": ["Similarly, we want to give you more flexibility during ingestion time: how about [extracting PDFs](https://github.com/weaviate/weaviate/issues/2509) or applying [stemming](https://github.com/weaviate/weaviate/issues/2439) to your BM25 and hybrid search? <br></br>\n\n### Beyond Billion Scale: Large-Scale Performance\n![Billion Scale](./img/billion-scale.png)\n\nIn 2022, we published the [Sphere Demo Dataset for Weaviate](/blog/sphere-dataset-in-weaviate). This marked the first time (to our knowledge) that more than a billion objects and vectors were imported into Weaviate. Dealing with ever-growing datasets is not only about being able to handle their size. Our users run complex queries in production and often have strict latency requirements."], "query": "What are the new ingestion time flexibility features for Weaviate, and what was the significance of the Sphere Demo Dataset published in 2022?"}
{"relevant_passages": ["The compression is carried out by using predefined centers, which we will explain shortly. If we aim to compress each segment down to 8 bits (one byte) of memory, we might have 256 (total combinations with 8 bits) predefined centers per segment. When compressing a vector we would go segment-by-segment assigning a byte representing the index of the predefined center. The segmentation and compression process is demonstrated in Figure 3 below. ![pq](./img/image3.jpg)\n**Fig."], "query": "How many predefined centers are used per segment in the 8-bit compression method described?"}
{"relevant_passages": ["*import-ance* (sorry) \ud83e\udd41. Weaviate offers and strongly recommends the [batch import feature](/developers/weaviate/tutorials/import#import-setup) for adding data objects in bulk. To further speed up the import process, you can use parallelization, which lets you run multiple batches concurrently. Each object in these batches is then checked for duplicates and assigned a unique internal DocID used by Weaviate to access objects. We uncovered that there could be a race condition in this process."], "query": "What is the recommended method for bulk data import in Weaviate, and how does it handle object uniqueness and potential race conditions?"}
{"relevant_passages": ["### The elephant in the room\nRunning model inference in production is hard. * It requires expensive specialized hardware. * You need a lot more computing power during the initial data import. * Hardware tends to be underutilized once the bulk of the heavy work is done. * Sharing and prioritizing resources with other teams is hard."], "query": "What are the challenges of running model inference in production?"}
{"relevant_passages": ["### Concluding thoughts on RAG Metrics\nIn conclusion, we have presented metrics used to evaluate indexing, retrieval, and generation:\n* Generation: Faithfulness and answer relevance, and the evolution from a massive focus on detecting hallucinations and other metrics such as Sensibleness and Specificity Average (SSA). * Retrieval: New opportunities with LLM rated context precision and context recall, as well as an overview of how human labeling has been used to measure recall, precision, and nDCG. * Indexing: Measuring recall as the number of ground truth nearest neighbors returned from the vector search algorithm. We believe the key question here is: *When do ANN errors seep into IR errors*? All components generally have an option to trade-off performance for latency or cost."], "query": "What metrics are used to evaluate the generation, retrieval, and indexing aspects of information retrieval systems, and what is the key question regarding ANN errors?"}
{"relevant_passages": ["## LLM Evaluations\nLet\u2019s start with the newest and most exciting component of all this, LLM evaluations! The history of machine learning has been heavily driven by the manual labor of labeling data, such as whether a Yelp review is positive or negative, or whether an article about nutritional supplements is related to the query, \u201cWho is the head coach of the Boston Celtics?\u201d. LLMs are becoming highly effective at data annotation with less manual effort. This is the key **\u201cwhat\u2019s new\u201d** development accelerating the development of RAG applications. The most common technique pioneered by frameworks, like [Ragas](https://docs.ragas.io/en/latest/), are Zero-Shot LLM Evaluations. Zero-Shot LLM Evaluation describes prompting a Large Language Model with a prompt template such as: \u201cPlease provide a rating on a scale of 1 to 10 of whether these search results are relevant to the query."], "query": "What is the new development in LLM evaluations that is reducing the need for manual data labeling?"}
{"relevant_passages": ["However, we did not explain the underlying motivation of moving vectors to disk. In this post we explore:\n\n- What kind of information we need to move to disk\n- The challenges of moving data to disk\n- What the implications of moving information to disk are\n- Introduce and test a full first solution offered by the new HNSW+PQ feature in Weaviate v1.18\n\n## What Information to Move to disk\n\nWhen indexing, there exist two big chunks of information that utilize massive amounts of memory: vectors and neighborhood graphs. Weaviate currently supports vectors of `float32`. This means we need to allocate 4 bytes, or 32 bits, per stored vector dimension. A database like Sift1M contains 1 million vectors of 128 dimensions each."], "query": "What is the size in bytes of each vector dimension stored in Weaviate v1.18, and how many vectors of what dimension does the Sift1M database contain?"}
{"relevant_passages": ["#### Background\nWe found a critical error in the compactioniong logic that could lead to the compaction operation either corrupting or completely losing data elements. This could be obsereved through a variety of symptoms:\n  * Retrieving an object by it's ID would lead to a different result than retrieving the object using a filter on the id property\n  * Filters that should match a specific number of objects matched fewer objects than expected\n  * Objects missing completely\n  * Filters with `limit=1` would not return any results when there should be exactly one element, but increasing the limit would then include the object\n  * Filters would return results with `null` ids\n\n#### Example\nIn the first case, if you had an object with id: **my-id-123456**. Calling the following GraphQL API with a filter on id would return the expected object. ```graphql\n{\n  Get {\n    Article(where: {\n        path: [\"id\"],\n        operator: Equal,\n        valueText: \"my-id-123456\"\n      }) {\n      title\n    }\n  }\n}\n```\n\nHowever, calling the following REST API with the same id wouldn't get the object back. ```\nGET /v1/objects/{my-id-123456}\n```\n\n#### The problem\nSo, if your data manipulation logic depended on the above operations to perform as expected, you update and delete operations might have been issued incorrectly."], "query": "What error in the compaction logic could cause data retrieval issues and affect update and delete operations?"}
{"relevant_passages": ["I am proud of you all and highly excited about the future. Thank you all, and let\u2019s make 2023 the most exciting year for Weaviate users so far!\n\n<br></br>\n\nimport WhatNext from '/_includes/what-next.mdx'\n\n<WhatNext />"], "query": "Who is the speaker expressing pride and excitement for in the year 2023, and what code component is being imported in the message?"}
{"relevant_passages": ["To end this article, let\u2019s discuss a little further why ranking is so exciting for the most hyped pairing of LLMs and Search: Retrieval-Augmented Generation. ## Ranking for Retrieval-Augmented Generation\nA lot of the recent successes of vector search can be attributed to their effectiveness as a tool for Large Language Models. So whereas the speed trade-off with rankers may be a major bottleneck for how humans use search, it might not be as much of a problem for how LLMs use search. Of course fast generation is preferred, but if you are paying for the result, quality may be more important than speed. Shi et al."], "query": "Why is ranking considered exciting in the context of Retrieval-Augmented Generation for Large Language Models?"}
{"relevant_passages": ["Keep your eyes peeled for more details on this soon on [our blog](/blog)! We will share our insights as we go. \ud83d\ude00\n\n\nimport WhatNext from '/_includes/what-next.mdx'\n\n<WhatNext />"], "query": "Where can I find upcoming details and insights that are mentioned in the document?"}
{"relevant_passages": ["The compression rate will depend on the amount of centroids. If we use 256 centroids we only need one byte per segment. Additionally, using an amount in the range 257 to 65536 will require two bytes per segment. ### Sift\n\nFirst, we show results on Sift1M. We vary the amount of centroids starting with 256 and increase the number until it performs poorly compared to the next curve."], "query": "How many bytes per segment are needed when using 256 centroids for compression on the Sift1M dataset?"}
{"relevant_passages": ["#### Problem\nIf Weaviate encounters an unexpected crash, no data will be lost. To provide this guarantee, a Write-Ahead Log (WAL) is in place. If a crash had occurred, the WAL is parsed at startup, and all previously unfinished operations are recovered, even if they were part of in-memory structures that had not yet been flushed. While this system is very safe, the recovery could be slow for several reasons:\n\n- Unflushed memtables could become very large. This would lead to a lot of data that needs to be recovered after a crash\n- The recovery process was single-threaded."], "query": "How does Weaviate ensure data recovery after an unexpected crash?"}
{"relevant_passages": ["We\u2019re confident in knowing we don\u2019t have to trade performance for scale.\u201d\n\n\u2013 David Wood, CTO and co-founder, Moonsift\n\n## Next steps \nMoonsift is now getting ready to launch their [AI Copilot](https://www.moonsift.com/copilot) to the world. They\u2019re seeing early results of the power of its ability to understand user intent and serve intelligent results. Some fun examples include \u201cshirt that looks like a Caipirinha\u201d or \u201cskirt with a pattern inspired by ocean waves\u201d. ![image](img/image1.png)\n\nAs Moonsift prepares for the public launch of its shopping Copilot, the team is continuing to explore ways to optimize the cost, performance, and scale of their system. They are looking into Weaviate\u2019s new feature, [Product Quantization (PQ)](/blog/pq-rescoring), which helps reduce the memory footprint of their system, by compressing vectors and performing rescoring, while retaining search relevance."], "query": "What examples of user intent can Moonsift's AI Copilot understand according to their CTO, David Wood?"}
{"relevant_passages": ["---\ntitle: Vamana vs. HNSW - Exploring ANN algorithms Part 1\nslug: ann-algorithms-vamana-vs-hnsw\nauthors: [abdel]\ndate: 2022-10-11\ntags: ['research']\nimage: ./img/hero.png\ndescription: \"Vector search on disks: How does Vamana compare to HNSW?\"\n---\n![Vamana vs HNSW - Exploring ANN algorithms Part 1](./img/hero.png)\n\n<!-- truncate -->\n\nVector databases must be able to search through a vast number of vectors at speed. This is a huge technical challenge that is only becoming more difficult over time as the vector dimensions and dataset sizes increase. Like many others, our current prevailing solution is to use Approximate Nearest Neighbor (ANN) algorithms to power Weaviate. But the key question is - which ones to use?"], "query": "What are the differences between Vamana and HNSW algorithms in vector search on disks?"}
{"relevant_passages": ["We found that the data structures relied on dynamic allocations. So, even if we knew that an array would never be longer than 64 elements, the Go runtime could still decide to allocate an array[100] in the background when the array reaches 51 elements. To fix that, we switched to static allocations, and Weaviate instructs the Go runtime to allocate the exact number of elements. This reduced **static** memory usage even when idle. ### Results\n\n\ud83c\udf89 Between these two major updates, plus some smaller ones, we saw a **significant reduction in memory usage of 10-30%**\ud83d\ude80."], "query": "What was the impact on memory usage after switching from dynamic to static allocations in Go data structures according to the document?"}
{"relevant_passages": ["If we compress too soon, when too little data is present in uncompressed form, the centroids will be underfit and won't capture the underlying data distribution. Alternatively, compressing too late will take up unnecessary ammounts of memory prior to compression. Keep in mind that the uncompressed vectors will require more memory so if we send the entire dataset and compress only at the end we will need to host all of these vectors in memory at some point prior to compressing after which we can free that memory. Depending on the size of your vectors the compressing time could be optimally calculated but this is not a big issue either. Figure 10 shows a profile of the memory used while loading Sift1M."], "query": "What are the trade-offs of compressing data too early or too late in terms of memory usage and centroid quality?"}
{"relevant_passages": ["on Article {\n          title\n        }\n      }\n    }\n  }\n}\n```\n\n\ud83d\udca1 LIVE \u2014 [try out this query](https://console.weaviate.io/console/query#weaviate_uri=http://semantic-search-wikipedia-with-weaviate.api.vectors.network:8080&graphql_query=%23%23%0A%23%20Mixing%20scalar%20queries%20and%20semantic%20search%20queries%0A%23%23%0A%7B%0A%20%20Get%20%7B%0A%20%20%20%20Paragraph(%0A%20%20%20%20%20%20ask%3A%20%7B%0A%20%20%20%20%20%20%20%20question%3A%20%22What%20was%20Michael%20Brecker's%20first%20saxophone%3F%22%0A%20%20%20%20%20%20%20%20properties%3A%20%5B%22content%22%5D%0A%20%20%20%20%20%20%7D%0A%20%20%20%20%20%20where%3A%20%7B%0A%20%20%20%20%20%20%20%20operator%3A%20Equal%0A%20%20%20%20%20%20%20%20path%3A%20%5B%22inArticle%22%2C%20%22Article%22%2C%20%22title%22%5D%0A%20%20%20%20%20%20%20%20valueText%3A%20%22Michael%20Brecker%22%0A%20%20%20%20%20%20%7D%0A%20%20%20%20%20%20limit%3A%201%0A%20%20%20%20)%20%7B%0A%20%20%20%20%20%20_additional%20%7B%0A%20%20%20%20%20%20%20%20answer%20%7B%0A%20%20%20%20%20%20%20%20%20%20result%0A%20%20%20%20%20%20%20%20%7D%0A%20%20%20%20%20%20%7D%0A%20%20%20%20%20%20content%0A%20%20%20%20%20%20order%0A%20%20%20%20%20%20title%0A%20%20%20%20%20%20inArticle%20%7B%0A%20%20%20%20%20%20%20%20...%20on%20Article%20%7B%0A%20%20%20%20%20%20%20%20%20%20title%0A%20%20%20%20%20%20%20%20%7D%0A%20%20%20%20%20%20%7D%0A%20%20%20%20%7D%0A%20%20%7D%0A%7D)\n\n### Example 4 \u2014 mix generic concept search with graph relations\nWith Weaviate you can also use the GraphQL interface to make graph relations like -in the case of Wikipedia- links between different articles. In this overview we connect the paragraphs to the articles and show the linking articles. ```graphql\n{\n  Get {\n    Paragraph(\n      nearText: {\n        concepts: [\"jazz saxophone players\"]\n      }\n      limit: 25\n    ) {\n      content\n      order\n      title\n      inArticle {\n        ... on Article { # <== Graph connection I\n          title\n          hasParagraphs { # <== Graph connection II\n            ... on Paragraph {\n              title\n            }\n          }\n        }\n      }\n    }\n  }\n}\n```\n\n\ud83d\udca1 LIVE \u2014 [try out this query](https://console.weaviate.io/console/query#weaviate_uri=http://semantic-search-wikipedia-with-weaviate.api.vectors.network:8080&graphql_query=%23%23%0A%23%20Using%20the%20Q%26A%20module%20I%0A%23%23%0A%7B%0A%20%20Get%20%7B%0A%20%20%20%20Paragraph(%0A%20%20%20%20%20%20ask%3A%20%7B%0A%20%20%20%20%20%20%20%20question%3A%20%22Where%20is%20the%20States%20General%20of%20The%20Netherlands%20located%3F%22%0A%20%20%20%20%20%20%20%20properties%3A%20%5B%22content%22%5D%0A%20%20%20%20%20%20%7D%0A%20%20%20%20%20%20limit%3A%201%0A%20%20%20%20)%20%7B%0A%20%20%20%20%20%20_additional%20%7B%0A%20%20%20%20%20%20%20%20answer%20%7B%0A%20%20%20%20%20%20%20%20%20%20result%0A%20%20%20%20%20%20%20%20%20%20certainty%0A%20%20%20%20%20%20%20%20%7D%0A%20%20%20%20%20%20%7D%0A%20%20%20%20%20%20content%0A%20%20%20%20%20%20title%0A%20%20%20%20%7D%0A%20%20%7D%0A%7D)\n\n## Implementation Strategies \u2014 Bringing Semantic Search to Production\nThe goal of Weaviate is to allow you to bring large ML-first applications to production."], "query": "How can I perform a semantic search for jazz saxophone players using GraphQL?"}
{"relevant_passages": ["We stop searching when all points in the current set are visited and return the top k results. On the other hand, the robust prune method optimizes the graph on a particular point p so that the greedy search will run faster. For that, it checks a list of visited nodes and selects on each iteration a point that minimizes the distance from p (the node for which we are currently optimizing the graph) and adds it to the out neighbors of the node p. It also removes the nodes that could be reached from the newly added node with a shorter distance than from p (amplifying the former distance by alpha to keep long-range connections). Together, these two algorithms should enable fast searches with high recall."], "query": "What are the two algorithms described for optimizing graph searches, and how does the robust prune method work?"}
{"relevant_passages": ["They discovered the influence of prompt phrasing on responses, leading to a growing interest in \"prompt engineering\" and its best practices within the AI community. The emphasis on scalable, open-source vector databases to manage data from AI models aligns perfectly with the goal of making generative AI accessible to all. ### In-house open source projects\n\nOur in-house projects, [Verba, an open-source RAG app](https://github.com/weaviate/Verba) and [Health Search](https://github.com/weaviate/healthsearch-demo) built by [Edward Schmuhl](https://www.linkedin.com/in/edwardschmuhl/), and other projects like [Leonie Monigatti's](https://www.linkedin.com/in/804250ab/) [Awesome-Moviate](https://github.com/weaviate-tutorials/awesome-moviate) and [Adam Chan](https://www.linkedin.com/in/itsajchan/)'s [Recommendation System](https://github.com/itsajchan/Create-JSON-Data-with-AI) have pushed the boundaries of what's possible with vector databases applicable to different industry use cases. ![build with weaviate](img/Group_2596_(1).png)\n\n### Community projects\n\nIncredible projects came to life through Weaviate in 2023! One standout project is [Basepilot](https://www.basepilot.com/), a project by [Ken Hendricks](https://www.linkedin.com/in/ken-hendricks-96181611a/) and [Pascal Wieler](https://www.linkedin.com/in/pascal-wieler/) that enables businesses to create their own product copilots easily. Their embedded copilot allows product users to find information and get tasks done just by chatting to it."], "query": "Who created the open-source RAG app called Verba?"}
{"relevant_passages": ["Each query engine has its own strengths in the information retrieval process, let\u2019s dive into a couple of them and how we might think about evaluation. <img\n  src={require('./img/sub-question.png').default}\n  alt=\"Sub Question Query Engine\"\n  style={{ maxWidth: \"60%\" }}\n/>\n\nMulti-hop query engines (otherwise known as [sub question query engine](https://gpt-index.readthedocs.io/en/latest/examples/query_engine/sub_question_query_engine.html)) are great at breaking down complex questions into sub-questions. In the visual above, we have the query \u201cWhat is Ref2Vec in Weaviate?\u201d To answer this question, you need to know what Ref2Vec and Weaviate are separately. Therefore, two calls will need to be made to your database to retrieve relevant context based on the two questions. The two answers are then combined to generate one output."], "query": "How do multi-hop query engines handle complex questions?"}
{"relevant_passages": ["If we compress the vectors then the memory requirements goes down to the 1572 MB to 2129 MB range. After compression, recall drops to values ranging from 0.7337 to 0.9545. Latency rises up to the 7521 to 37402 microsends range. A summary is shown in Table 3 below. |                       |              | Recall100@100 | Latency ($\\mu s$)         | Memory required (MB)         |\n|-----------------------|--------------|---------------|---------------------------|------------------------------|\n| Sift1M Low params     | Uncompressed | 0.91561       | 293                       | 1277                         |\n|                       | Compressed   | 0.91361       | 401               (x1.36) | 610                 (47.76%) |\n| Sift1M High params    | Uncompressed | 0.99974       | 1772                      | 1674                         |\n|                       | Compressed   | 0.99658       | 1937             (x1.09)  | 1478               (88.29%)  |\n| DeepImage Low params  | Uncompressed | 0.8644        | 827                       | 9420                         |\n|                       | Compressed   | 0.85666       | 1039             (x1.25)  | 4730               (50.21%)  |\n| DeepImage High params | Uncompressed | 0.99757       | 2601                      | 15226                        |\n|                       | Compressed   | 0.97023       | 2708             (x1.04)  | 12367             (81.22%)   |\n| Gist Low params       | Uncompressed | 0.74461       | 2133                      | 4218                         |\n|                       | Compressed   | 0.73376       | 7521             (x3.52)  | 1572              (37.26%)   |\n| Gist High params      | Uncompressed | 0.99628       | 15539                     | 5103                         |\n|                       | Compressed   | 0.95455       | 37402           (x2.40)   | 2129               (41.72%)  |\n\n**Tab."], "query": "What is the percentage reduction in memory requirements for the Gist dataset with low parameters after compression?"}
{"relevant_passages": ["If we make one final trip to the supermarket to understand what a guava is, we could look at the things around it\u2014other fruit. Somewhat further away we might find guava juice or tinned guavas, but there's really no reason to look four aisles over for guava-flavored cat food. Using ANN allowed high-dimensional searches with near-perfect accuracy to be returned in milliseconds instead of hours. To be practical, vector databases also needed something prosaically called CRUD support. That stands for \"create, read, update and delete,\" and solving that technical challenge meant that the complex process of indexing the database could be done once, rather than being repeated from scratch whenever the database was updated."], "query": "What does CRUD stand for in the context of vector databases?"}
{"relevant_passages": ["The database backups include data objects, plus their vectors and indexes. This way, restoring a backup is a straight copy of all the required elements without the need to re-create vectors or rebuild the indexes. (Read, this is going to be fast)\n\n### Backup modules\nCloud-native backups in Weaviate are handled with the addition of the new **backup modules**:\n\n* `backup-s3` - for S3\n* `backup-gcs` - for GCS\n* `backup-fs` - for local filesystem\n\nWithout getting into too many details (see the [docs for more precise instructions](/developers/weaviate/configuration/backups)), each module requires a different set of settings. For S3 and GCS, you need your cloud bucket name, authentication details and some extra details like a project name or the cloud region. > For S3 authentication you can use access keys or IAM with role ARN's."], "query": "What are the components included in Weaviate database backups, and what modules are available for cloud-native backups?"}
{"relevant_passages": ["But like with any technology, it is not a silver bullet and success depends on your implementation. ### Scalability\nThe demo dataset runs on a Docker setup on a single machine, you can easily spin up a Kubernetes cluster if you want to use the Weaviate dataset in production. How to do this, is outlined [here](/developers/weaviate/concepts/cluster). ## Conclusion\nTo bring semantic search solutions to production, you need three things:\n\n1. Data\n1."], "query": "How can I scale the Weaviate dataset for production use?"}
{"relevant_passages": ["The temperature setting controls the amount of randomness in the output. A temperature of 0 means that the response is more predictable and will vary less. A temperature of 1 gives the model the ability to introduce randomness and creativity into its responses. Therefore, if you\u2019re running the generative model more than once and it has a temperature of 1, the responses can vary after each rerun. Long context models are an emerging direction for choosing the LLM for your application."], "query": "How does the temperature setting affect the variability of a generative model's responses?"}
{"relevant_passages": ["This makes the restarting node \u2018partly\u2019 available. :::\n\n### Queries during maintenance - with replication\n\nOn the other hand, this figure shows results from a scenario with replication configured with a factor of 3. ![Monitoring stats showing no failures during restart](./img/queries_with_replication.png)\n\nWe see that a grand total of zero queries failed here over the course of 8-9 minutes, even though individual pods did go down as they did before. In other words, the end users wouldn\u2019t even have noticed that a new version was rolled out, as node-level downtime did not lead to system-level downtime. Did we mention that the only change between the two was setting the replication factor?"], "query": "How does configuring replication with a factor of 3 affect query failures during node restarts in a maintenance scenario?"}
{"relevant_passages": ["## Step 2.2 \u2014 Import the Data\nBecause we are going to vectorize a lot of data. We will be using the same machine as mentioned in the opening but with 4 instead of 1 GPU. ![Google Cloud GPU setup with a Weaviate load balancer](./img/load-balancer.png)\n*Google Cloud GPU setup with a Weaviate load balancer*\n\nThe load balancer will redirect the traffic to available Weaviate transformer modules so that the import speed significantly increases. In the section: *Implementation Strategies \u2014 Bringing Semantic Search to Production* below you'll find more info about how you can run this in production. Most critically, we are going to set an external volume in the Docker Compose file to make sure that we store the data outside the container."], "query": "What hardware change is made to the machine setup in Step 2.2 of the data import process to increase import speed?"}
{"relevant_passages": ["Let\u2019s now dive one layer deeper to understand how vector indexes are compared with one another. ### Indexing Metrics\nTenured Weaviate users are likely familiar with the [ANN Benchmarks](https://github.com/erikbern/ann-benchmarks/tree/main), which for example inspired the development of the [gRPC API in Weaviate 1.19](https://weaviate.io/blog/weaviate-1-19-release#grpc-api-support-experimental). The ANN Benchmarks measure Queries Per Second versus Recall, with additional nuances on single-threaded restrictions and so on. Databases are typically evaluated based on latency and storage cost, stochastic vector indexes place additional emphasis on accuracy measurement. There is some analog with approximation in [SQL select statements](https://learn.microsoft.com/en-us/sql/t-sql/functions/approx-count-distinct-transact-sql?view=sql-server-ver16), but we predict that error caused by approximation will have an even larger emphasis with the rising popularity of vector indexes."], "query": "What inspired the development of the gRPC API in Weaviate 1.19?"}
{"relevant_passages": ["Broadly there is the question of: **When does it make sense to use separate collections instead of filters?** Should `blogs` and `documentation` be separated into two collections or jointly housed in a `Document` class with a `source` property? ![multi-index](img/multi-index.png)\n\nUsing filters gives us a quick way to test the utility of these labels, because we can add more than one tag to each chunk and then ablate how well the classifiers use the labels. There are many interesting ideas here such as explicitly annotating where the context came from in the input to the LLM, such as \u201cHere are search results from blogs {search_results}. Here are search results from documentation {documentation}\u201d. As LLMs are able to process longer inputs, we expect that context fusion between multiple data sources will become more popular and thus, an associated hyperparameter emerges of how many documents to retrieve from each index or filter."], "query": "When should separate collections be used instead of filters for organizing data in the context of LLMs and context fusion?"}
{"relevant_passages": ["Head to the [documentation](/developers/weaviate/configuration/backups) for a more in-depth overview and instructions. ## Reduced memory usage\n\n![Reduced memory usage](./img/reduced-memory-usage.jpg)\n\nAs part of the continuous effort to make Weaviate faster, leaner and more powerful, we introduced new optimizations to use less RAM without sacrificing performance. ### Thread pooling optimization\n\nFirst, we set our sights on parallel imports, where we introduced thread pooling to reduce memory spikes while importing data. Previously if you had, e.g., 8 CPUs and would import from 4 client threads, each client request would run with a parallelization factor of 8 (one per CPU core). So, in the worst case, you could end up with 32 parallel imports (on a machine with \"only\" 8 CPUs)."], "query": "What optimization was introduced in Weaviate to reduce memory usage during parallel imports?"}
{"relevant_passages": ["The main metric of concern in this setting is the latency with which we can obtain our replies. Product Quantization not only helps with reducing memory requirements in this case but also with cutting down on latency. The following table compares improvement in latency that can be achieved using PQ aggressively. | Dataset   | Segments | Centroids | Compression |              | Latency (ms) |\n|-----------|----------|-----------|-------------|--------------|--------------|\n| Sift      | 8        | 256       | x64         | Compressed   | 46 (x12)     |\n|           |          |           |             | Uncompressed | 547          |\n| DeepImage | 8        | 256       | x48         | Compressed   | 468 (x8.5)   |\n|           |          |           |             | Uncompressed | 3990         |\n| Gist      | 48       | 256       | x80         | Compressed   | 221 (x17.5)  |\n|           |          |           |             | Uncompressed | 3889         |\n\n**Tab. 2**: *Brute force search latency with high compression ratio.*\n\n## HNSW+PQ\n\nOur complete implementation of [FreshDiskANN](https://arxiv.org/abs/2105.09613) still requires a few key pieces, however at this point we have released the HNSW+PQ implementation with v1.18 for our users to take advantage of."], "query": "What improvements in latency can be achieved by using Product Quantization for different datasets, and in which version was the HNSW+PQ implementation released?"}
{"relevant_passages": ["[Two new distance metrics](#new-distance-metrics) - with the addition of Hamming and Manhattan distance metrics, you can choose the metric (or a combination of) to best suit your data and use case. 1. [Two new Weaviate modules](#new-weaviate-modules) - with the Summarization module, you can summarize any text on the fly, while with the HuggingFace module, you can use compatible transformers from the HuggingFace\n1. [Other improvements and bug fixes](#other-improvements-and-bug-fixes) - it goes without saying that with every Weaviate release, we strive to make Weaviate more stable - through bug fixes - and more efficient - through many optimizations. Read below to learn more about each of these points in more detail."], "query": "What new distance metrics were added in the latest Weaviate release?"}
{"relevant_passages": ["## RAG Metrics\nWe are presenting RAG metrics from a top-down view from generation, to retrieval, and then indexing. We then present the RAG knobs to tune from a bottom-up perspective of building an index, tuning how to retrieve, and then options for generation. Another reason to present RAG Metrics from a top-down view is because errors from Indexing will bubble up to Search and then Generation, but errors in Generation (as we have defined the stack) have no impact on errors in Indexing. In the current state of RAG evaluation, it is uncommon to evaluate the RAG stack end-to-end, rather **oracle context**, or **controlled distractors** (such as the Lost in the Middle experiments) are assumed when determining faithfulness and answer relevancy in generation. Similarly, embeddings are typically evaluated with brute force indexing that doesn\u2019t account for approximate nearest neighbor errors."], "query": "What is the reason for presenting RAG metrics from a top-down perspective, and how does error propagation affect the different layers of the RAG stack?"}
{"relevant_passages": ["Back in 2020, this required humans to have conversations with the chatbot and manually assign these ratings. While it is good to avoid vague responses, it is equally important to avoid the LLM from **hallucinating**. Hallucination refers to the LLM generating a response that is not grounded in actual facts or the provided context. [LlamaIndex](https://docs.llamaindex.ai/en/latest/examples/evaluation/faithfulness_eval.html) measures this with a `FaithfulnessEvaluator` metric. The score is based on whether the response matches the retrieved context."], "query": "What is hallucination in language models and how is it measured by LlamaIndex?"}
{"relevant_passages": [":::\n\n## Implications for database maintenance\n\nIn production, this can dramatically reduce the critical downtime. Let\u2019s take an example three-pod Kubernetes setup with 10,000 tenants, and see how replication affects availability during a rolling update of Weaviate versions. Each Weaviate pod will restart one by one, as demonstrated in the example below (from `kubectl get pods`), which shows `weaviate-2` as having been recently restarted and ready, while `weaviate-1` is just restarting. ![Node statuses showing restarts](./img/node_statuses.png)\n\nWhat will the cluster availability be like during this period? We performed an experiment simulating non-trivial load with ~3,800 queries per second to approximate a real-life scenario."], "query": "How does replication affect database availability during a rolling update in a Kubernetes environment?"}
{"relevant_passages": ["The next segment amounts ensure 8, 16 and 32 to 1 compression ratio (in the case of sift which uses 2, 4 and 8 dimensions per segment). All experiments were performed adding 200,000 vectors using uncompressed behavior, then compressing and adding the rest of the data. We do not include the same charts for DeepImage but results are similar to those obtained over Sift1M. ![perf1](./img/image12.png)\n**Fig. 11**: *The chart shows Recall (vertical axis) Vs Latency (in microseconds, on the horizontal axis)."], "query": "What compression ratios were tested in the experiments with Sift1M, and how many vectors were initially added using uncompressed behavior?"}
{"relevant_passages": ["Typically this is in the context of recommendation in which we have metadata about users, as well as the documents or items. So for example, say we have features that describe a Users looking for Movies such as:\n\nUser Features - (Age, Gender, Location, Occupation, Preferences)\nMovie Features - (Release Year, Genre, Box Office, Duration). So together, the Metadata ranker takes as input something like: [Age, Gender, Location, Occupation, Preferences, Release year, Genre, Box Office, Duration] and predicts a score of how much this User will like the movie. We can fix the User features and rotate in each Document to get a score for each of the candidate movies (retrieved with something like ref2vec) to rank with. In addition to vectors, Weaviate also enables storing metadata features about objects such as `price`, or `color`."], "query": "How does a metadata ranker use user and movie features to recommend movies?"}
{"relevant_passages": ["The past for vector searching definitely was not a \u201csimpler time\u201d, and the appeal of modern vector databases like Weaviate is pretty clear given this context. But while the future is here, it isn't yet perfect. Tools like Weaviate can seem like a magician's mystery box. Our users in turn ask us *exactly* how Weaviate does its magic; how it turns all of that data into vectors, and how to control the process. So let's take a look inside the magic box together in this post."], "query": "How does Weaviate convert data into vectors and allow users to control the process?"}
{"relevant_passages": ["The first is to clean the data set and the second one is to import the data. ### Step 1 \u2013 Cleaning the Data\nThe first step is pretty straightforward, we will clean the data and create a [JSON Lines](https://jsonlines.org/) file to iterate over during import. You can run this process yourself or download the proceed file following [this](https://github.com/weaviate/semantic-search-through-wikipedia-with-weaviate#step-1-process-the-wikipedia-dump) link. ### Step 2 \u2014 Importing the Data\nThis is where the heavy lifting happens because all paragraphs need to be vectorized we are going to use Weaviate's modular setup to use multiple GPUs that we will stuff with models, but before we do this we need to create a Weaviate schema that represents our use case. ### Step 2.1 \u2014 Create a Weaviate Schema\nWithin Weaviate we will be using a schema that determines how we want to query the data in GraphQL and which parts we want to vectorize."], "query": "What are the two main steps described for processing data in the provided document?"}
{"relevant_passages": ["These kinds of models are increasingly being used as guardrails for generative models. For example, a harmful or NSFW content detector can prevent these generations from making it through the search pipeline. An interesting idea I recently heard from Eddie Zhou on Jerry Liu\u2019s Llama Index Fireside Chat is the idea of using Natural Language Inference models to prevent hallucination by predicting the entailment or contradiction taking as the [retrieved context, generated output] as input. Because large language models are stochastic models, we can sample several candidate generations and filter them through score rankers like these. ## A Recap of the Ranking Models\n* **Cross Encoders** are content-based re-ranking models that utilize pre-trained models, such as those available on Sentence Transformers, to rank the relevance of documents."], "query": "What is the role of Natural Language Inference models in preventing hallucination in generative models?"}
{"relevant_passages": ["|\n\nBy re-ranking the results we are able to get the clip where Jonathan Frankle describes the benchmarks created by Ofir Press et al. in the self-ask paper! This result was originally placed at #6 with Hybrid Search only. This is a great opportunity to preview the discussion of how LLMs use search versus humans. When humans search, we are used to scrolling through the results a bit to see the one that makes sense. In contrast, language models are constrained by input length; we can only give so many results to the input of the LLM."], "query": "What did Jonathan Frankle say about the benchmarks in the self-ask paper by Ofir Press et al. after re-ranking the search results?"}
{"relevant_passages": ["14**: *The chart shows Recall (vertical axis) Vs Indexing time (in minutes, on the horizontal axis). For this experiment we have added 200,000 vectors using the normal HNSW algorithm, then we switched to compressed and added the remaining 800,000 vectors.*\n\n### Memory compression results\n\nTo explore how the memory usage changes with the HNSW+PQ feature, we compare the two versions: uncompressed HNSW and HNSW plus compression using the KMeans encoder. We only compare KMeans using the same amount of segments as dimensions. All other settings could achieve a little bit of a higher compression rate but since we do not compress the graph, it is not significant for these datasets. Keep in mind that the whole graph built by HNSW is still hosted in memory."], "query": "How does the memory usage compare between uncompressed HNSW and HNSW with KMeans compression for the same number of segments as dimensions?"}
{"relevant_passages": ["We know that our users love multimodal models and we\u2019ve been cooking up more OOTB integrations with cutting-edge multimodal models so that it\u2019s easy for you to start using them in your vector search pipelines. Be on the lookout for new multimodal search capabilities soon, I \u201chear\u201d it's going to be amazing.\ud83d\ude09\n\nimport WhatNext from '/_includes/what-next.mdx'\n\n<WhatNext />"], "query": "What new capabilities involving multimodal models are being added to vector search pipelines?"}
{"relevant_passages": ["This way, you can make sure that your voice is heard, and we can see what all of you need the most. <br></br>\n\n## Conclusion: Proud of how far we\u2019ve come, excited about the future\nIn the beginning, I mentioned that not just the product but also the company grew significantly last year. I am incredibly proud of what we have achieved \u2013 both overall and in the past year. This wouldn\u2019t have been possible without an absolutely fantastic team. Everyone working on Weaviate \u2013 whether a full-time employee or open-source contributor \u2013 is doing a fantastic job."], "query": "What is the team working on that contributed to the company's significant growth last year?"}
{"relevant_passages": ["This feature allows HNSW to work directly with compressed vectors. This means using Product Quantization to compress vectors and calculate distances. As mentioned before, we could still store the complete representation of the vectors on disk and use them to correct the distances as we explore nodes during querying. For the time being, we are implementing only HNSW+PQ which means we do no correction to the distances. In the future we will explore adding such a correction and see the implications in recall and latency since we will have more accurate distances but also much more disk reads."], "query": "What is the current state of distance correction in the HNSW+PQ implementation?"}
{"relevant_passages": ["Recall measures how many of the positives were captured in the search results. Precision then measures how many of the search results are labeled as relevant. LLMs can thus calculate precision with the prompt: \u201cHow many of the following search results are relevant to the query {query}? {search_results}\u201d. A proxy measure for recall can also be achieved with an LLM prompt: \u201cDo these search results contain all the needed information to answer the query {query}?"], "query": "What are recall and precision in the context of search results, and how can they be measured using LLM prompts?"}
{"relevant_passages": ["Interestingly enough, it takes longer to render the results than it takes the vector database to find the answer. *Note, a semantic search is unlike a regular keywords search (which matches keywords like-for-like), but instead, we are searching for answers based on the semantic meaning of our query and data.*\n\nThe inevitable question that follows up this demonstration is always:\n\n> Why is this so incredibly fast? ## What is a vector search? To answer this question we need to look at how vector databases work. Vector databases index data, unlike other databases, based on data vectors (or vector embeddings)."], "query": "Why are vector databases faster at finding answers than rendering the results?"}
{"relevant_passages": ["Let's recap exactly what Weaviate does. ### Text vectorization in Weaviate\n\nimport VectorizationBehavior from '/_includes/vectorization.behavior.mdx';\n\n<VectorizationBehavior/>\n\nNow that we understand this, you might be asking - is it possible to customize the vectorization process? The answer is, yes, of course. ## Tweaking text2vec vectorization in Weaviate\n\nSome of you might have noticed that we have not done anything at all with the schema so far. This meant that the schema used is one generated by the auto-schema feature and thus the vectorizations were carried out using default options."], "query": "Can the text vectorization process in Weaviate be customized, and does it initially use a default schema for vectorization?"}
{"relevant_passages": ["These image representations are initialized with large-scale vision-language models such as CLIP, thereby leveraging the rich image and text representations of these models. Some of these alignments are explained directly (comparing a video of a person delivering a speech to audio of the speech) while other pairs of modalities are explained through their relation to an intermediate modality(understanding the relation between motion data and text data by seeing how they both relate to video/image data). With that said in other cases a logical explanation of why two examples are close together might not be possible without a close examination of the training sets. ### 4. Heterogeneity and Training Optimization\n\nHandling modality imbalance(we have a lot of image and text data and significantly less motion and tactile data) makes it very difficult to learn all modalities equally well; this takes careful tuning of the contribution of each modality during training and optimizing the fusion mechanism to prevent dominance by a single modality."], "query": "What challenges are associated with training optimization in multimodal learning systems that handle different types of data such as images, text, motion, and tactile data?"}
{"relevant_passages": ["Data imbalance is one consideration but even more important to consider is how much unique information a modality brings - this is known as heterogeneity. Assessing heterogeneity accurately allows you to decide which modalities are different enough to be separately processed and which modality pairs interact differently and thus should be differently fused. [HighMMT](https://arxiv.org/abs/2203.01311) has been proposed to handle high-modality scenarios involving a large set of diverse modalities. HighMMT uses two new information-theoretic metrics for heterogeneity quantification, enabling it to automatically prioritize the fusion of modalities that contain unique information or unique interactions. This results in a **single model** that scales up to 10 modalities and 15 tasks from 5 different research areas, demonstrating a crucial scaling behavior not found in ImageBind: performance continues to improve with each modality added, and it transfers to entirely new modalities and tasks during fine-tuning."], "query": "What is the advantage of HighMMT over ImageBind in terms of scaling behavior with added modalities?"}
{"relevant_passages": ["We listened to your feedback, suggestions and use cases! So we made it our mission for the `1.15` release to design and implement an **elegant solution** with a great **Developer Experience (DX)**, which you will love \ud83d\ude0d to use for years to come. ### Announcement\nIntroducing **Weaviate Cloud-native backups**. \ud83c\udf89\n\nIt allows you to make full database backups (or selected classes) straight to **S3**, **GCS** or the **local filesystem** with a single API call \ud83e\udd29; and restore the data to a Weaviate instance of your choice with another API call. What is really great about this implementation is that you can create a backup without downtime on a running instance. The database stays fully operational (including receiving writes) while the backup is transferred to the remote storage."], "query": "What new backup feature was introduced in Weaviate version 1.15 that allows for operational continuity?"}
{"relevant_passages": ["This would mean we need roughly 500 MB for the graph but nearly ten times more memory for the vectors. On the other hand, a database such as DeepImage96 would have 96 dimensions but almost 10,000,000 vectors, meaning, that we would need around 10 GB to hold the vectors and the graph, ~5 GB for each graph. Our final goal is to move both vectors and graphs to disk. However, we will only explore moving vectors to disk in this post. Storing vectors on disk is not too challenging."], "query": "How much memory is required to hold the vectors and the graph for the DeepImage96 database?"}
{"relevant_passages": ["Model Interpretability\n\nUnderstanding and interpreting the decisions made by multimodal models can be challenging. The fusion of different modalities may introduce complexities in interpreting the learned representations and attributing importance to each modality. Consider, for example, searching in the learned joint embedding space of a multimodal model what accelerometer motion data is closest to a person giving a speech about globalization. The lesson is that some modalities or data objects just don\u2019t pair together naturally. The ImageBind model handles this by using the image representation as ground truth and pulling all other concepts closer to image representations which then establishes a naturally learned alignment between other modalities."], "query": "What is the challenge of interpreting decisions made by multimodal models, and how does the ImageBind model address the alignment of different modalities?"}
{"relevant_passages": ["The main advantage of the hierarchical representation used in HNSW is that the traversal of the graph is accelerated. This is solved in the Vamana implementation by hosting long-range connections with a similar function. ### Traversing a graph\nTraversing a graph is a bit like planning international travel. First, we could take a long-distance flight (akin to a fast jump), taking us to a city closer to our destination. Then we could take a train (a lot better for the environment \ud83d\ude09) to get to the town of our choice."], "query": "What is the main advantage of the hierarchical representation in HNSW, and how does Vamana replicate this feature?"}
{"relevant_passages": ["---\ntitle: Achieve Zero-Downtime Upgrades with Weaviate\u2019s Multi-Node Setup\nslug: zero-downtime-upgrades\nauthors: [etienne,jp]\ndate: 2023-11-30\nimage: ./img/hero.png\ntags: ['concepts', 'engineering', 'how-to']\n# tags: ['replication']  # Added for further SEO without changing the original tags\ndescription: \"Learn about high-availability setups with Weaviate, which can allow upgrades and other maintenance with zero downtime. \"\n\n---\n\n![Image of Weaviate robots pointing at each other](./img/hero.png)\n\n## The tyranny of database downtime\n\nLike the old saying goes, a chain is only as strong as its weakest link. For tech infrastructure products, the weak link can often be its uptime. Think about how big a deal it is when social networks, web apps or databases are not available. This is why we at Weaviate really pride ourselves on having a robust, production-ready database that can scale as our users do."], "query": "How can Weaviate's multi-node setup enable zero-downtime upgrades?"}
{"relevant_passages": ["You can even run transformer models locally with [`text2vec-transformers`](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-transformers), and modules such as [`multi2vec-clip`](/developers/weaviate/modules/retriever-vectorizer-modules/multi2vec-clip) can convert images and text to vectors using a CLIP model. But they all perform the same core task\u2014which is to represent the \u201cmeaning\u201d of the original data as a set of numbers. And that\u2019s why semantic search works so well. import WhatNext from '/_includes/what-next.mdx'\n\n<WhatNext />"], "query": "What tools are mentioned for running transformer models locally and converting images and text to vectors?"}
{"relevant_passages": ["If you are new to Weaviate, check out the [getting started guide](/developers/weaviate/quickstart). Let us know if you found this article interesting or useful. We are always happy to receive constructive feedback. \ud83d\ude00\n\nWe are always working on new articles and are looking for new topics. Let us know if there is anything that you would like us to write about."], "query": "Where can I find a getting started guide for Weaviate?"}
{"relevant_passages": ["And yes, bug fixing is not always the most exciting topic, as we often get more excited about shiny new features. But for you to truly enjoy working with Weaviate, we need to make sure that no bugs are getting in the way. Check out [the changelog](https://github.com/weaviate/weaviate/releases/tag/v1.14.0) to see the complete list of features and over 25 bug fixes. ### Critical bug fix in compaction logic\nIn this release we fixed a critical bug, which in rare situations could result in data loss.<br/>\nThe bug affected environments with frequent updates and deletes. > This bug fix alone, makes it worth upgrading to Weaviate 1.14."], "query": "What critical bug was fixed in Weaviate version 1.14.0?"}
{"relevant_passages": ["Thus you could take any image from your training set, and step by step, add increasing levels of random noise to it and generate incrementally more noisy versions of that image as shown below. ![noising gif](./img/noise.gif)\n*[Source](https://yang-song.net/blog/2021/score/)*\n\n![noising images](./img/noisingimage.png)\n*[Source](https://huggingface.co/blog/annotated-diffusion)*\n\nThis \u201cnoising\u201d process, shown in the images above allows us to take training set images and add known quantities of noise to it until it becomes completely random noise. This process takes images from a state of having high probability of being found in the training set to having a low probability of existing in the training set. Once the \u201cnoising\u201d step is completed, then we can use these clean and noisy image combinations during the training phase of the diffusion model. In order to train a diffusion model we ask it to remove the noise from the noised images step by step until it recovers something as close as possible to the original image."], "query": "What is the process of adding noise to training set images to train a diffusion model?"}
{"relevant_passages": ["This means you could have a much faster reply, but there is no guarantee that you will actually have the closest element from your search. In the vector search space, we use [recall](https://en.wikipedia.org/wiki/Precision_and_recall) to measure the rate of the expected matches returned. The trade-off between recall and latency can be tuned by adjusting indexing parameters. Weaviate comes with reasonable defaults, but also it allows you to adjust build and query-time parameters to find the right balance. Weaviate incrementally builds up an index (graph representation of the vectors and their closest neighbors) with each incoming object."], "query": "How does Weaviate balance recall and latency in vector search?"}
{"relevant_passages": ["\ud83e\udd17\n\nimport ShareFeedback from '/_includes/share-feedback.md';\n\n<ShareFeedback />"], "query": "What is the purpose of the ShareFeedback component in the web application's codebase?"}
{"relevant_passages": ["[Better control over Garbage Collector](#better-control-over-garbage-collector) - with the introduction of GOMEMLIMIT we gained more control over the garbage collector, which significantly reduced the chances of OOM kills for your Weaviate setups. 1. [Faster imports for ordered data](#faster-imports-for-ordered-data) - by extending the Binary Search Tree structure with a self-balancing Red-black tree, we were able to speed up imports from O(n) to O(log n)\n1. [More efficient filtered aggregations](#more-efficient-filtered-aggregations) - thanks to optimization to a library reading binary data, filtered aggregations are now 10-20 faster and require a lot less memory. 1."], "query": "What feature was introduced to improve control over the garbage collector and reduce OOM kills in Weaviate setups?"}
{"relevant_passages": ["This topic is heavily related to our perspectives on continual optimization of Deep Learning models, discussed further in \u201cOrchestrating Tuning\u201d. Chunking your data is an important step before inserting your data into Weaviate. Chunking takes long documents and converts it into smaller sections. This enhances the retrieval since each chunk has an important nugget of information and this helps to stay within the LLMs token limit. There are quite a few strategies to parse documents."], "query": "What is the purpose of chunking data before inserting it into Weaviate, and how does it improve retrieval?"}
{"relevant_passages": ["For example, in the case of text data, \u201ccat\u201d and \u201ckitty\u201d have similar meaning, even though the _words_ \u201ccat\u201d and \u201ckitty\u201d are very different if compared letter by letter. For semantic search to work effectively, representations of \u201ccat\u201d and \u201ckitty\u201d must sufficiently capture their semantic similarity. This is where vector representations are used, and why their derivation is so important. In practice, vectors are arrays of real numbers, of a fixed length (typically from hundreds to thousands of elements), generated by machine learning models. The process of generating a vector for a data object is called vectorization."], "query": "What is the process called that generates vector representations for words to capture their semantic similarity in text data?"}
{"relevant_passages": ["Here is the example above as [Google Colab notebook](https://colab.research.google.com/drive/1XAJc9OvkKhsJRmheqWZmjYU707dqEIl8?usp=sharing). ![Colab screenshot](./img/colab.png)\n\n### Use Weaviate in CI/CD pipelines\n\nYou can use Embedded Weaviate in automated tests, where you can run integration tests without having to manage a separate server instance. Here is the example above slightly modified to perform similarity search and test that the added object was found. <Tabs groupId=\"languages\">\n  <TabItem value=\"py\" label=\"Python\">\n\n  Save as `embedded_test.py` and run `pytest`. (If you don't have pytest, run `pip install pytest`.)\n  <br/>\n\n  <FilteredTextBlock\n    text={PyCode}\n    startMarker=\"# START TestExample\"\n    endMarker=\"# END TestExample\"\n    language=\"py\"\n  />\n  </TabItem>\n\n  <TabItem value=\"js\" label=\"JavaScript/TypeScript\">\n\n  Save as `embedded_test.ts` and run `npx jest`:\n  <br/>\n\n  <FilteredTextBlock\n    text={TSCode}\n    startMarker=\"// START TestExample\"\n    endMarker=\"// END TestExample\"\n    language=\"js\"\n  />\n  </TabItem>\n</Tabs>\n\n\nHave you found other use cases for embedded Weaviate?"], "query": "How can Embedded Weaviate be used in CI/CD pipelines for integration testing?"}
{"relevant_passages": ["You would need the following ingredients:\n* Raw Data\n* Hugging Face API token \u2013 which you can request from [their website](https://huggingface.co/settings/tokens)\n* A working Weaviate instance with the `text2vec-huggingface` enabled\n\nThen you would follow these steps. ### Step 1 \u2013 initial preparation \u2013 create schema and select the hf models\nOnce you have a Weaviate instance up and running. Define your schema (standard stuff \u2013 pick a class name, select properties, and data types). As a part of the schema definition, you also need to provide, which Hugging Face model you want to use for each schema class. This is done by adding a `moduleConfig` property with the `model` name, to the schema definition, like this:\n```javascript\n{\n    \"class\": \"Notes\",\n    \"moduleConfig\": {\n        \"text2vec-huggingface\": {\n            \"model\": \"sentence-transformers/all-MiniLM-L6-v2\",  # model name\n            ..."], "query": "What are the prerequisites and initial steps for integrating the `text2vec-huggingface` module into a Weaviate instance?"}
{"relevant_passages": ["We instead may want to have another format, whether that be data stored in an S3 bucket or something else, that has the associated metadata with it, but provides a more economical way to experiment with this. On the other hand, we have model fine-tuning and continual learning with gradients, rather than data inserts or updates. The most common models used in RAG are embeddings, re-rankers, and of course, LLMs. Keeping machine learning models fresh with new data has been a longstanding focus of continual learning frameworks and MLops orchestration that manage the re-training and testing and deployment of new models. Starting with continual learning of LLMs, one of the biggest selling points of RAG systems is the ability to extend the \u201ccut-off\u201d date of the LLM\u2019s knowledge base, keeping it up to date with your data. Can the LLM do this directly?"], "query": "Can Large Language Models directly extend their knowledge base to stay current with new data in RAG systems?"}
{"relevant_passages": ["It then takes the summaries generated so far to influence the next output. It repeats this process until all documents have been processed. ### Map Rerank\n\n<img\n    src={require('./img/map-rerank.gif').default}\n    alt=\"alt\"\n    style={{ width: \"100%\" }}\n/>\n\nMap Rerank involves running an initial prompt that asks the model to give a relevance score. It is then passed through the language model and assigns a score based on the certainty of the answer. The documents are then ranked and the top two are stuffed to the language model to output a single response."], "query": "What is the Map Rerank process in document processing and how does it work?"}
{"relevant_passages": ["### Patch 1.15.1 note\nWe have published a patch release v1.15.1.<br/>\nTo learn more check the [Weaviate 1.15.1 patch release](/blog/weaviate-1-15-1-release) blog. ### Community effort\n![New Contributors](./img/new-contributors.jpg)\n\n\ud83d\ude00We are extremely happy about this release, as it includes two big community contributions from [Aakash Thatte](https://github.com/sky-2002) and [Dasith Edirisinghe](https://github.com/DasithEdirisinghe). Over the last few weeks, they collaborated with our engineers to make their contributions. \ud83d\ude80**Aakash** implemented the two **new distance metrics**, while **Dasith** contributed by implementing the two **new Weaviate modules**. \ud83d\udc55We will send some Weaviate t-shirts to Aakash and Dasith soon."], "query": "Who implemented the new distance metrics in Weaviate 1.15.1 patch release?"}
{"relevant_passages": ["A system decided to break it into the sub questions {sub_question_1} and {sub_question_2}. Does this decomposition of the question make sense?\u201d. We then have two separate RAG evaluations for each of the sub questions, and then an evaluation of whether the LLM was able to combine the answers from each question to answer the original question. As another example of evolving complexity from RAG to Agents, let\u2019s consider Routing Query Engines. The following visual illustrates an agent routing a query to either an SQL or Vector Database query."], "query": "How does a system decompose complex queries for evaluation and route them to the appropriate database?"}
{"relevant_passages": ["You can see how replication significantly improves availability. Weaviate provides further configurability and nuance for you in this area by way of a consistency guarantee setting. For example, a request made with a consistency level of QUORUM would require over half of the nodes which contain the data to be up, while a request with ONE consistency would only require one node to be up. :::note Notes\n- The replication algorithm makes sure that no node holds a tenant twice. Replication is always spread out across nodes."], "query": "How does Weaviate ensure high availability through its replication feature, and what are the different consistency levels it offers?"}
{"relevant_passages": ["A larger ef results in more distance comparisons done during the search, slowing it down significantly although producing a more accurate result. The next parameters to look at are the ones used in index building, efConstruction, the size of the queue when inserting data into the graph, and maxConnections, the number of edges per node, which also must be stored with each vector. Another new direction we are exploring is the impact of distribution shift on PQ centroids and the intersection with hybrid clustering and graph index algorithms such as [DiskANN](https://suhasjs.github.io/files/diskann_neurips19.pdf) or [IVFOADC+G+P](https://openaccess.thecvf.com/content_ECCV_2018/papers/Dmitry_Baranchuk_Revisiting_the_Inverted_ECCV_2018_paper.pdf). Using the Recall metric may be a good enough measure of this to trigger re-fitting the centroids, with the question then being: which subset of vectors to use in re-fitting. If we use the last 100K that may have caused the recall drop, we could risk overfitting to the new distribution, thus we likely want some hybrid sampling of the timeline of our data distribution when inserted into Weaviate."], "query": "What is the effect of increasing the `ef` parameter on the performance of a search algorithm?"}
{"relevant_passages": ["---\ntitle: Weaviate 1.15 release\nslug: weaviate-1-15-release\nauthors: [connor, erika, laura, sebastian]\ndate: 2022-09-07\ntags: ['release']\nimage: ./img/hero.png\ndescription: \"Weaviate 1.15 introduces Cloud-native Backups, Memory Optimizations, faster Filtered Aggregations and Ordered Imports, new Distance Metrics and new Weaviate modules.\"\n---\n![Weaviate 1.15 release](./img/hero.png)\n\n<!-- truncate -->\n\nWe are happy to announce the release of Weaviate 1.15, which is packed with great features, significant performance improvements, new distance metrics and modules, and many smaller improvements and fixes. ## The brief\n\nIf you like your content brief and to the point, here is the TL;DR of this release:\n1. [\u2601\ufe0fCloud-native backups](#cloud-native-backups) - allows you to configure your environment to create backups - of selected classes or the whole database - straight into AWS S3, GCS or local filesystem\n1. [Reduced memory usage](#reduced-memory-usage) - we found new ways to optimize memory usage, reducing RAM usage by 10-30%. 1."], "query": "What are the new features introduced in Weaviate 1.15?"}
{"relevant_passages": ["---\ntitle: Wikipedia and Weaviate\nslug: semantic-search-with-wikipedia-and-weaviate\nauthors: [bob]\ndate: 2021-11-25\ntags: ['how-to']\nimage: ./img/hero.jpg\n# canonical-url: https://towardsdatascience.com/semantic-search-through-wikipedia-with-weaviate-graphql-sentence-bert-and-bert-q-a-3c8a5edeacf6\n# canonical-name: Towards Data Science\ndescription: \"Semantic search on Wikipedia dataset with Weaviate \u2013 vector database.\"\n---\n![Wikipedia and Weaviate](./img/hero.jpg)\n\n<!-- truncate -->\n\nTo conduct semantic search queries on a large scale, one needs a vector database to search through the large number of vector representations that represent the data. To show you how this can be done, [we have open-sourced the complete English language Wikipedia corpus](https://github.com/weaviate/semantic-search-through-wikipedia-with-weaviate) backup in Weaviate. In this article, I will outline how we've created the dataset, show you how you can run the dataset yourself, and present search strategies on how to implement similar vector and semantic search solutions in your own projects and how to bring them to production. The Wikipedia dataset used is the \"truthy\" version of October 9th, 2021. After processing it contains 11.348.257 articles, 27.377.159 paragraphs, and 125.447.595 graph cross-references."], "query": "How can Weaviate be used for semantic search on the Wikipedia dataset?"}
{"relevant_passages": ["---\ntitle: Ingesting PDFs into Weaviate\nslug: ingesting-pdfs-into-weaviate\nauthors: [erika, shukri]\ndate: 2023-05-23\nimage: ./img/hero.png\ntags: ['integrations','how-to']\ndescription: \"Demo on how to ingest PDFs into Weaviate using Unstructured.\"\n\n---\n\n![PDFs to Weaviate](./img/hero.png)\n\n<!-- truncate -->\n\nSince the release of ChatGPT, and the subsequent realization of pairing Vector DBs with ChatGPT, one of the most compelling applications has been chatting with your PDFs (i.e. [ChatPDF](https://www.chatpdf.com/) or [ChatDOC](https://chatdoc.com/)). Why PDFs? PDFs are fairly universal for visual documents, encompassing research papers, resumes, powerpoints, letters, and many more. In our [latest Weaviate Podcast](https://www.youtube.com/watch?v=b84Q2cJ6po8) with Unstructured Founder Brian Raymond, Brian motivates this kind of data by saying \u201cImagine you have a non-disclosure agreement in a PDF and want to train a classifier\u201d. Although PDFs are great for human understanding, they have been very hard to process with computers."], "query": "What is the title of the document that explains how to ingest PDFs into Weaviate and discusses the application of ChatGPT with Vector DBs?"}
{"relevant_passages": ["They offer the advantage of further reasoning about the relevance of results without needing specialized training. Cross Encoders can be interfaced with Weaviate to re-rank search results, trading off performance for slower search speed. * **Metadata Rankers** are context-based re-rankers that use symbolic features to rank relevance. They take into account user and document features, such as age, gender, location, preferences, release year, genre, and box office, to predict the relevance of candidate documents. By incorporating metadata features, these rankers offer a more personalized and context-aware search experience."], "query": "What are Metadata Rankers and what features do they use to rank search results?"}
{"relevant_passages": ["---\ntitle: Combining LangChain and Weaviate\nslug: combining-langchain-and-weaviate\nauthors: [erika]\ndate: 2023-02-21\ntags: ['integrations']\nimage: ./img/hero.png\ndescription: \"LangChain is one of the most exciting new tools in AI. It helps overcome many limitations of LLMs, such as hallucination and limited input lengths.\"\n---\n![Combining LangChain and Weaviate](./img/hero.png)\n\nLarge Language Models (LLMs) have revolutionized the way we interact and communicate with computers. These machines can understand and generate human-like language on a massive scale. LLMs are a versatile tool that is seen in many applications like chatbots, content creation, and much more. Despite being a powerful tool, LLMs have the drawback of being too general."], "query": "What are the benefits of combining LangChain with Weaviate in the context of LLMs?"}
{"relevant_passages": ["Among a long list of capabilities, first- and second-wave databases have their strengths. For example, some are very good at finding every instance of a certain value in a database, and others are very good at storing time sequences. The third wave of database technologies focuses on data that is processed by a machine learning model first, where the AI models help in processing, storing and searching through the data as opposed to traditional ways. To better understand the concept, think of a supermarket with 50,000 items. Items on display are not organized alphabetically or by price, the way you'd expect a structured, digital system to do it; they're placed in context."], "query": "What is the unique feature of third-wave database technologies compared to first- and second-wave databases?"}
{"relevant_passages": ["The next step is the routing index used. For corpora of less than 10K vectors, RAG applications may be satisfied with a brute force index. However, with increased vectors brute force latency becomes far slower than Proximity Graph algorithms such as HNSW. As mentioned under RAG Metrics, HNSW performance is typically measured as a pareto-optimal point trading off queries per second with recall. This is done by varying the ef, or size of the search queue, used at inference time."], "query": "What indexing method is recommended for RAG applications with large corpora, and how is the performance of this method measured?"}
{"relevant_passages": ["Generative models are not limited to just generating images; they can also generate songs, written language or any other modality of data - however to make it easier for us to understand we will only consider generative models that for image data. The core idea behind all generative models is that they try to learn and understand what the training set \u201clooks\u201d like. In other words they try to learn the underlying distribution of the training set - which just means that they want to know how likely a datapoint is to be observed in the training set. For example, if you are training a generative model on images of beautiful landscapes then, for that generative model, images of trees and mountains are going to be much more common then images of someones kitchen. Furthering this line of reasoning, for that same generative model, an image of static noise would also be quite unlikely since we don\u2019t see that in the training set."], "query": "What is the core principle behind generative models in relation to their training data, and what types of data can they generate?"}
{"relevant_passages": ["\ud83e\udd14 With this, you can get more out of your existing setups and push your Weaviate instances to do more, or you could save on the resources. ## Better control over Garbage Collector\n\n![GOMEMLIMIT](./img/gomemlimit.jpg)\n\nWeaviate is built from the ground up in Go, which allows for building very performant and memory-safe applications. Go is a garbage-collected language. > *A quick refresher:*<br/>\n> In a garbage-collected language, such as Go, C#, or Java, the programmer doesn't have to deallocate objects manually after using them. Instead, a GC cycle runs periodically to collect memory no longer needed and ensure it can be assigned again."], "query": "What advantage does Weaviate have due to being built in Go related to garbage collection?"}
{"relevant_passages": ["Let's take a look in terms of speed as well as recall. The chart below illustrates a comparison of the C++ Vamana [reference code](https://github.com/microsoft/DiskANN) provided by Microsoft and our [HNSW implementation](https://github.com/weaviate/weaviate/tree/master/adapters/repos/db/vector/hnsw) when using Sift1M. Following Microsoft\u2019s experiments, we have used sift-query.fvecs (100,000 vectors sample) for building the index and sift-query.fvecs (1,000 vectors sample) for querying. We are retrieving 10 **(fig. 1)** and 100 **(fig."], "query": "How does the performance of the C++ Vamana reference code compare to the HNSW implementation when using the Sift1M dataset for indexing and querying?"}
{"relevant_passages": ["Further these approaches are well positioned to generalize to Recommendation. In Recommendation, instead of taking a [query, document] as input to a cross-encoder, we take as input a [user description, document] pair. For example, we can ask users to describe their preferences. Further, we could combine these in trios of [user description, query, item] for LLM, or more lightweight cross-encoder, ranking. There is a bonus 3rd idea where we use the log probabilities concatenating the query with the document."], "query": "What is the novel input method for cross-encoders in recommendation systems that involves user descriptions?"}
{"relevant_passages": [">\n> For GCS you can use a Google Application Credentials json file. Alternatively, you can configure backups with the **local filesystem**. All you need here is to provide the path to the backup folder. > Note, you can have multiple storage configurations - one for each S3, GCS and the local filesystem. ### Creating backups - API\nOnce you have the backup module up and running, you can create backups with a single `POST` command:\n\n```js\nPOST /v1/backups/{storage}/\n{\n  \"id\": \"backup_id\"\n}\n```\n\nThe `storage` values are `s3`, `gcs`, and `filesystem`."], "query": "How can I create backups using an API for different storage configurations such as S3, GCS, and the local filesystem?"}
{"relevant_passages": ["Our initial concatenation had the `question` text come first, so let's reverse it to:\n\n```text\n'McDonald\\'s In 1963, live on \"The Art Linkletter Show\", this company served its billionth burger '\n```\n\nThis lowers the distance to `0.0147`. Weaviate adds the class name to the text. So we will prepend the word `question` producing:\n\n```text\n'question McDonald\\'s In 1963, live on \"The Art Linkletter Show\", this company served its billionth burger'\n```\n\nFurther lowering the distance to `0.0079`. Then the remaining distance can be eliminated by converting the text to lowercase like so:\n\n```python\nstr_in = ''\nfor k in sorted(properties.keys()):\n    v = properties[k]\n    if type(v) == str:\n        str_in += v + ' '\nstr_in = str_in.lower().strip()  # remove trailing whitespace\nstr_in = 'question ' + str_in\n```\n\nProducing:\n\n```text\n'question mcdonald\\'s in 1963, live on \"the art linkletter show\", this company served its billionth burger'\n```\n\nPerforming the `nearVector` search again results in zero distance (`1.788e-07` - effectively zero)!\n\nIn other words - we have manually reproduced Weaviate's default vectorization process. It's not overly complex, but knowing it can certainly be helpful."], "query": "Which company served its billionth burger live on \"The Art Linkletter Show\" in 1963?"}
{"relevant_passages": ["Charts to the left show Recall (vertical axis) Vs Heap usage (horizontal axis). Charts to the right show Heap usage (horizontal axis) Vs the different parameter sets. Parameter sets to achieve a larger graph (also producing a more accurate search) are charted from top down.*\n\nLet's sumamrize what we see in the above charts. We could index our data using high or low parameters set. Additionally, we could aim for different levels of compression."], "query": "How does heap usage affect recall and the accuracy of search results based on different parameter sets?"}
{"relevant_passages": ["Since it conveys both content and context, such a representation obviously presents a more complete and nuanced data picture. The challenge comes from searching through myriad dimensions. Initially, this was done with a brute force approach, looking at every vector associated with every entry. Needless to say, that approach didn't scale. One breakthrough that helped third-wave vector databases to scale was an approach called \"approximate nearest neighbor\" (ANN) search."], "query": "What technique enabled third-wave vector databases to scale effectively?"}
{"relevant_passages": ["<!-- TODO: update with a link to the article once it is ready -->\n*We are working on an article that will guide you on how to create your own model and upload it to Hugging Face.*\n\n### Fully automated and optimized\nWeaviate manages the whole process for you. From the perspective of writing your code \u2013 once you have your schema configuration \u2013 you can almost forget that Hugging Face is involved at all. For example, when you import data into Weaviate, Weaviate will automatically extract the relevant text fields, send them Hugging Face to vectorize, and store the data with the new vectors in the database. ### Ready to use with a minimum of fuss\nEvery new Weaviate instance created with the [Weaviate Cloud Services](/pricing) has the Hugging Face module enabled out of the box. You don't need to update any configs or anything, it is there ready and waiting."], "query": "How does Weaviate automatically handle data vectorization with Hugging Face, and is the Hugging Face module enabled by default in new Weaviate Cloud Services instances?"}
{"relevant_passages": ["3**: *Summary of the previously presented results. We show results for the shortest and largest parameter sets for uncompressed and compressed versions. Additionally we show the speed down rate in latency and the percentage of memory, compared to uncompressed, needed to operate under compressed options.*\n\nWe would like to give you an extra bonus row from the above table. [Sphere](https://github.com/facebookresearch/sphere) is an open-source dataset recently released by Meta. It collects 768 dimensions and nearly a billion objects."], "query": "What does the document say about the performance comparison between uncompressed and compressed parameter sets?"}
{"relevant_passages": ["For example, you can create a backup called **first_backup** and push it to **GCS**, like this:\n\n```js\nPOST /v1/backups/gcs/\n{\n  \"id\": \"first_backup\"\n}\n```\n\nThen, you can check the backup status by calling:\n\n```js\nGET /v1/backups/gcs/first_backup\n```\n\n### Restore\nTo restore a backup, you can call:\n\n```js\nPOST /v1/backups/{store}/{backup_id}/restore\n```\n\nSo, using our previous example, you can restore the **first_backup**, like this:\n\n```js\nPOST /v1/backups/gcs/first_backup/restore\n```\n\nYou can also, check the status of an ongoing restoration by calling:\n\n```js\nGET /v1/backups/gcs/first_backup/restore\n```\n\n### Cross-cloud\nHere is one interesting thing that you might not have noticed. You can use this setup to run Weaviate with one cloud provider but then store and restore backups to/from another cloud provider. So, for example, you can run Weaviate on AWS and use GCS for your backup needs. How cool is that? ### Class backups\nYou can also create backups for specific classes or select which classes you want to restore."], "query": "How do you restore a backup named \"first_backup\" from GCS using the Weaviate API?"}
{"relevant_passages": ["You find things in the supermarket by understanding how they relate to each other. So if the store gets a new product\u2014say, guavas\u2014you know to look near the apples and bananas, not near garbage bags or other things that happen to also cost $1.98/lb. A key early milestone in the third wave happened in 2015 when Google changed its search algorithm from one based on page rankings to one based on a machine learning model that it dubbed RankBrain. Before then, Google's search engine was essentially a high-powered keyword search that ranked websites by the number of other sites that linked back to them. Essentially, Google trusted rankings to the collective users of the Internet."], "query": "When did Google introduce RankBrain to its search algorithm?"}
{"relevant_passages": ["Developers who want to build AI-powered applications can now skip the tedious process of complex training strategies. Now you can simply take models off-the-shelf and plug them into your apps. Applying a ranking model to hybrid search results is a promising approach to keep pushing the frontier of zero-shot AI. Imagine we want to retrieve information about the Weaviate Ref2Vec feature. If our application is using the Cohere embedding model, it has never seen this term or concept."], "query": "What is the Weaviate Ref2Vec feature and how does it relate to the Cohere embedding model in the context of zero-shot AI?"}
{"relevant_passages": ["The `nearText` filter also allows for [more specific filters](https://towardsdatascience.com/semantic-search-through-wikipedia-with-weaviate-graphql-sentence-bert-and-bert-q-a-3c8a5edeacf6#:~:text=more%20specific%20filters) like `moveAwayFrom` and `MoveTo` concepts to manipulate the search through vector space. ```graphql\n{\n  Get {\n    Paragraph(\n      nearText: {\n        concepts: [\"Italian food\"]\n      }\n      limit: 50\n    ) {\n      content\n      order\n      title\n      inArticle {\n        ... on Article {\n          title\n        }\n      }\n    }\n  }\n}\n```\n\n\ud83d\udca1 LIVE \u2014 [try out this query](https://console.weaviate.io/console/query#weaviate_uri=http://semantic-search-wikipedia-with-weaviate.api.vectors.network:8080&graphql_query=%23%23%0A%23%20Generic%20question%20about%20Italian%20food%0A%23%23%0A%7B%0A%20%20Get%20%7B%0A%20%20%20%20Paragraph(%0A%20%20%20%20%20%20nearText%3A%20%7B%0A%20%20%20%20%20%20%20%20concepts%3A%20%5B%22Italian%20food%22%5D%0A%20%20%20%20%20%20%7D%0A%20%20%20%20%20%20limit%3A%2050%0A%20%20%20%20)%20%7B%0A%20%20%20%20%20%20content%0A%20%20%20%20%20%20order%0A%20%20%20%20%20%20title%0A%20%20%20%20%20%20inArticle%20%7B%0A%20%20%20%20%20%20%20%20...%20on%20Article%20%7B%0A%20%20%20%20%20%20%20%20%20%20title%0A%20%20%20%20%20%20%20%20%7D%0A%20%20%20%20%20%20%7D%0A%20%20%20%20%7D%0A%20%20%7D%0A%7D)\n\n### Example 3 \u2014 mix natural language questions with scalar search\nWithin Weaviate you can also mix scalar search filters with vector search filters. In the specific case, we want to conduct a semantic search query through all the paragraphs of articles about the saxophone player Michael Brecker. ```graphql\n{\n  Get {\n    Paragraph(\n      ask: {\n        question: \"What was Michael Brecker's first saxophone?\"\n        properties: [\"content\"]\n      }\n      where: {\n        operator: Equal\n        path: [\"inArticle\", \"Article\", \"title\"]\n        valueText: \"Michael Brecker\"\n      }\n      limit: 1\n    ) {\n      _additional {\n        answer {\n          result\n        }\n      }\n      content\n      order\n      title\n      inArticle {\n        ..."], "query": "What are the specific filters available in Weaviate's `nearText` search to manipulate vector space, and how can you combine natural language questions with scalar search filters in a query?"}
{"relevant_passages": ["Notice that similar recall/latency results with less segments still mean better compression rate. ![res1](./img/image5.png)\n**Fig. 4**: *Time (min) to fit the Product Quantizer with 200,000 vectors and to encode 1,000,000 vectors, all compared to the recall achieved. The different curves are obtained varying the segment length (shown in the legend) from 1 to 8 dimensions per segment. The points in the curve are obtained varying the amount of centroids.*\n\n![res2](./img/image6.png)\n**Fig."], "query": "How does varying the segment length affect the time to fit a Product Quantizer and encode vectors in relation to the recall achieved?"}
{"relevant_passages": ["The code is then passed through the Python REPL. Python REPL is a code executor implemented in LangChain. Once the code is executed, the output of the code is printed. The language model then sees this output and judges if the code is correct. ## ChatVectorDB\nOne of the most exciting features of LangChain is its collection of preconfigured chains."], "query": "What is the role of the Python REPL in LangChain?"}
{"relevant_passages": ["The fact that the graph is still in memory makes it hard to see the difference between those different levels of compression. The more we compress the lower the recall we would expect. Let us discuss the lowest level of compression along with some expectations. For Sift1M we would require roughly 1277 MB to 1674 MB of memory to index our data using uncompressed HNSW. This version would give us recall ranging from 0.96811 to 0.99974 and latencies ranging from 293 to 1772 microseconds."], "query": "What is the range of memory required to index the Sift1M dataset using uncompressed HNSW, and what recall and latency can be expected?"}
{"relevant_passages": ["Only hosting the vectors in memory would take 3.1 TB. To work with data of this size, either you need to spend more to provision expensive resources, or you can sacrifice a bit on recall and latency and save drastically on resources. We show a simple test below to drive home the importance of compressing data and eventually moving graphs also to disk. The test was run over 10 million objects only. A final note on the numbers."], "query": "How much memory is required to host vectors in memory for 10 million objects, and what are the trade-offs of working with such large data sets?"}
{"relevant_passages": ["---\ntitle: Vector Embeddings Explained\nslug: vector-embeddings-explained\nauthors: [dan]\ndate: 2023-01-16\ntags: ['concepts']\nimage: ./img/hero.png\ndescription: \"Get an intuitive understanding of what exactly vector embeddings are, how they're generated, and how they're used in semantic search.\"\n---\nThe core function of Weaviate is to provide high-quality search results, going beyond simple keyword or synonym searches, and actually finding what the user _means_ by the query, or providing an actual answer to questions the user asks. <!-- truncate -->\n\nSemantic searches (as well as question answering) are essentially searches by similarity, such as by the meaning of text, or by what objects are contained in images. For example, consider a library of wine names and descriptions, one of which mentioning that the wine is \u201cgood with **fish**\u201d. A \u201cwine for **seafood**\u201d keyword search, or even a synonym search, won\u2019t find that wine. A meaning-based search should understand that \u201cfish\u201d is similar to \u201cseafood\u201d, and \u201cgood with X\u201d means the wine is \u201cfor X\u201d\u2014and should find the wine."], "query": "What are vector embeddings and how do they facilitate semantic search?"}
{"relevant_passages": ["\ud83d\ude00 <br/>\nKeep in touch and check [our blog](/blog) from time to time. import WhatNext from '/_includes/what-next.mdx'\n\n<WhatNext />"], "query": "Where can I find updates or new content related to the sender of this message?"}
{"relevant_passages": ["ML-Models\n1. Vector database\n\nIn this article, we have shown how you can bring the complete Wikipedia corpus (data) using open-source ML-models (Sentence-BERT) and a vector database (Weaviate) to production. import WhatNext from '/_includes/what-next.mdx'\n\n<WhatNext />"], "query": "How can you use Sentence-BERT and Weaviate to bring the Wikipedia corpus to production?"}
{"relevant_passages": [":::\n\n## Conclusions\nWe've managed to implement the indexing algorithm on DiskANN, and the resulting performance is good. From years of research & development, Weaviate has a highly optimized implementation of the HNSW algorithm. With the Vamana implementation, we achieved comparable in-memory results. There are still some challenges to overcome and questions to answer. For example:\n* How do we proceed to the natural disk solution of Weaviate?"], "query": "What indexing algorithm was implemented on DiskANN, and how does its performance compare to Weaviate's HNSW algorithm?"}
{"relevant_passages": ["* **Score Rankers** employ classifiers or regression models to score and detect content, acting as guardrails for generative models. These scores can help in filtering harmful or NSFW content and prevent hallucinations with cutting edge ideas such as Natural Language Inference filters. Each of these ranking models have particular use cases. However, the lines between these models are blurring with new trends such as translating tabular metadata features into text to facilitate transfer learning from transformers pre-trained on text. Of course, the recent successes of LLMs are causing a rethink of most AI workflows and the application of LLMs to rank and score rankers to filter generations are both exciting."], "query": "What are Score Rankers and how are they being influenced by the recent advancements in Large Language Models?"}
{"relevant_passages": ["It refers to an arrangement where locking occurs on multiple buckets or 'stripes'. Are you curious about, the challenge that we faced, which solutions we considered, and what was our final solution? Read on \ud83d\ude00. ## Background\nDatabases must be able to import data quickly and reliably while maintaining data integrity and reducing time overhead. Weaviate is no exception to this! Given that our users populate Weaviate with hundreds of millions of data objects (if not more), we appreciate that import performance is of the highest ..."], "query": "How did Weaviate improve its data import performance while maintaining data integrity?"}
{"relevant_passages": ["The query is {query}, the search results are {search_results}\u201d. The visualization below shows how an LLM can be used to evaluate the performance of RAG systems. ![RAG-evaluation](img/rag-eval.png)\n\nThere are three major opportunities for tuning Zero-Shot LLM Evaluation: 1. the design of the metrics such as precision, recall, or nDCG, 2. the exact language of these prompts, and 3."], "query": "What are the three major opportunities for tuning Zero-Shot LLM Evaluation?"}
{"relevant_passages": ["Share what you build with Weaviate in [Slack](https://weaviate.slack.com/), on our [Forum](https://forum.weaviate.io/), or on socials. ## Embracing Open Source and Sharing Knowledge\n\nAs AI accelerated throughout the year with ever-new innovations popping up, so did the community's curiosity to learn and share knowledge in that area. As an open-source solution, **community** is a foundational pillar of Weaviate. ### [Hacktoberfest](https://weaviate.io/blog/hacktoberfest-2023)\n\nCelebrating the spirit of **open source**, we participated in our first [Hacktoberfest](https://hacktoberfest.com/) this October, which was organized by our very own **[Leonie Monigatti](https://www.linkedin.com/in/804250ab/)**! This global event, aimed at engineers and machine learning enthusiasts, fosters collaboration and contributions to open-source technology. Participants who had four pull requests (PRs) accepted between October 1 and 31, 2023, earned a unique digital reward and some Weaviate Merch! Contributions varied in scope, ranging from minor, non-coding inputs to more substantial technical improvements."], "query": "Who organized Weaviate's first participation in Hacktoberfest 2023?"}
{"relevant_passages": ["## Discussions & wrap-up\n\nSo there it is. Throughout the above journey, we saw how exactly Weaviate creates vectors from the text data objects, which is:\n\n- Vectorize properties that use `string` or `text` data types\n- Sorts properties in alphabetical (a-z) order before concatenating values\n- Prepends the class name\n- And converts the whole string to lowercase\n\nAnd we also saw how this can be tweaked through the schema definition for each class. One implication of this is that your vectorization requirements are a very important part of considerations in the schema definition. It may determine how you break down related data objects before importing them into Weaviate, as well as which fields you choose to import. Let's consider again our quiz question corpus as a concrete example."], "query": "How does Weaviate create vectors from text data objects?"}
{"relevant_passages": ["This sensory exploration helps them link different perspectives of the same experience to create a holistic understanding of their environment. This fusion of multisensory data when learning new concepts is also partially responsible for why humans can learn with very few data points - making us great few-shot learners. Let's imagine you are trying to teach the concept of a dog to a child. The next time you see a dog at the park you point it out and say \u201cThis is a dog!\u201d. Let's say that this is a single observation/data point - in the supervised machine-learning sense."], "query": "How do humans, particularly children, utilize sensory exploration to learn new concepts with limited data points?"}
{"relevant_passages": ["## Tool Use\nThe last building block we will cover is tool use. [Tool use](https://python.langchain.com/docs/modules/agents/tools/) is a way to augment language models to use tools. For example, we can hook up an LLM to [vector databases](https://weaviate.io/blog/what-is-a-vector-database), calculators, or even code executors. Of course we will dive into the vector databases next, but let\u2019s start with an example of the code executor tool use. <img\n    src={require('./img/tool-use.gif').default}\n    alt=\"alt\"\n    style={{ width: \"100%\" }}\n/>\n\nThe task for the language model is to write python code for the bubble sort algorithm."], "query": "What is an example of augmenting language models with a tool to write Python code?"}
{"relevant_passages": ["There you will learn how each of the distances works in more detail, when to use each, and how they compare to other metrics. ## New Weaviate modules\n\n<!-- TODO: add an image for Weaviate modules -->\n![New Weaviate modules](./img/weaviate-modules.png)\n\nThe list of the new goodies included with Weaviate `v1.15` goes on. Courtesy of a fantastic community contribution from [Dasith Edirisinghe](https://github.com/DasithEdirisinghe), we have two new Weaviate modules for you: Summarization and Hugging Face modules. ### Summarization Module\nThe Summarization module allows you to summarize text data at query time. The module adds a `summary` filter under the `_additional` field, which lets you list the properties that should be summarized."], "query": "What new features does Weaviate version 1.15 offer, and who contributed to its development?"}
{"relevant_passages": ["It was that Ofir did such a phenomenal job of figuring out a way to measure the complexity of the knowledge that was extracted from the model. He gave us a benchmark, a ladder to climb, a way to measure whether we could retrieve certain kinds of information from models. And I think that's going to open the door to a ton more benchmarks. And you know what happens when there's a benchmark. We optimize the hell out of that benchmark and it moves science forward\u2026 [ truncated for visibility ] |\n| Hybrid Only            | Or, at least being able to ask follow up questions when it\u2019s unclear about and that\u2019s surprisingly not that difficult to do with these current systems, as long as you\u2019re halfway decent at prompting, you can build up these follow up systems and train them over the course of a couple 1,000 examples to perform really, really well, at least to cove r90, 95% of questions that you might get."], "query": "Who created a benchmark for measuring the complexity of knowledge extracted from models?"}
{"relevant_passages": ["And that\u2019s exactly what they\u2019ve been collecting. Usage of their browser extension over the last few years has enabled Moonsift to explore discovery and train models on over 60M products, 250M interactions, and 40K retailers across the internet. Now, Moonsift has the data they need (which is growing every day) to improve product discovery with AI. ## Building the discovery engine \nThe Moonsift team brought on Marcel Marais as lead machine learning engineer to build a system that could harness the data they\u2019ve gathered to take their product to the next level. At first, Marais looked at improving discovery through a keyword-based search system using BM25 and re-ranking, but he quickly assessed that would not be sufficient to power the type of recommendation engine they needed."], "query": "How many products, interactions, and retailers has Moonsift's browser extension collected data on, and who did they hire as the lead machine learning engineer to improve their discovery engine?"}
{"relevant_passages": ["We are still on memory, are we not? Don't worry. This was just the first step toward our goal. We want to ensure that we have a solid implementation in memory before we move to disk, and this milestone ends here. :::note\n!Spoilers alert, as you read this article, we are evaluating our implementation on disk.<br/>\nWe will prepare a similar article to outline; how we moved everything to disk and what the price was performance-wise."], "query": "What is the current stage of the implementation mentioned in the document, and what are the future plans regarding its transition to disk?"}
{"relevant_passages": ["<details>\n  <summary>Optional: Try it yourself (with minikube)</summary>\n\nYou can try running a local, multi-node Weaviate cluster with `minikube`, which can conveniently run a local Kubernetes cluster. We note that deploying Weaviate on a cloud provider\u2019s kubernetes service follows a similar process. <br/>\n\nFirst, install `minikube` and `helm` for your system by following these guides ([minikube](https://minikube.sigs.k8s.io/docs/start), [helm](https://helm.sh/docs/intro/install)). We also recommend installing `kubectl` ([by following this guide](https://kubernetes.io/docs/tasks/tools/#kubectl)). <br/>\n\nOnce minikube is installed, start a three-node minikube cluster by running the following from the shell:\n\n```shell\nminikube start --nodes 3\n```\n\nOnce the nodes have been created, you should be able to interact with them through the `kubectl` command-line tool."], "query": "How can I set up a local multi-node Weaviate cluster using minikube?"}
{"relevant_passages": ["---\ntitle: Multimodal Embedding Models\nslug: multimodal-models\nauthors: zain\ndate: 2023-06-27\nimage: ./img/hero.png\ntags: ['concepts']\ndescription: \"ML Models that can see, read, hear and more!\"\n\n---\n\n![Multimodal Models](./img/hero.png)\n\n<!-- truncate -->\n\n## The Multisensory Nature of Human Learning\n\nHumans have a remarkable ability to learn and build world models through the integration of multiple sensory inputs. Our combination of senses work synergistically to provide us with rich and diverse information about our environment. By combining and interpreting these sensory inputs, we are able to form a coherent understanding of the world, make predictions, and acquire new knowledge very efficiently. The process of learning through multi-sensory inputs begins from the early stages of human development. Infants explore the world through their senses, touching, tasting, listening, and observing objects and people around them."], "query": "What are ML models that mimic the human ability to integrate multiple sensory inputs called?"}
{"relevant_passages": ["Prompt tuning, 2. Few-Shot Examples, and 3. Fine-Tuning. Prompt tuning entails tweaking the particular language used such as: \u201cPlease answer the question based on the provided search results.\u201d versus \u201cPlease answer the question. IMPORTANT, please follow these instructions closely."], "query": "What is prompt tuning in AI language tasks and how does it differ from few-shot examples and fine-tuning?"}
{"relevant_passages": ["8**: *Time (min) to fit the Product Quantizer with 200,000 vectors and to encode 1,000,000 vectors, all compared to the recall achieved. The different curves are obtained varying the segment length (shown in the legend) from 1 to 6 dimensions per segment. The points in the curve are obtained varying the amount of centroids.*\n\n![res6](./img/image10.png)\n**Fig. 9**: *Average time (microseconds) to calculate distances from query vectors to all 1,000,000 vectors compared to the recall achieved. The different curves are obtained varying the segment length (shown in the legend) from 1 to 6 dimensions per segment."], "query": "What is the relationship between segment length and recall in the performance of a Product Quantizer when fitting and encoding vectors, as well as calculating distances?"}
{"relevant_passages": ["#### Solution\nWe addressed each of the points above individually and improved the overall MTTR substantially:\n\n- A deduplication process was added, so that large WALs with a lot of updates (i.e. redundant data) could be reduced to only the necessary information. - The recovery process now runs in parallel. If there are multiple places that require recovery, they can each recover independently, without one recovery having to wait for the other. - A mechanism was added that flushes any memtable that has been idle (no writes) for 60s or more. In addition to speeding up the recovery, this change also ensures that no recovery is needed at all in many cases."], "query": "What strategies were implemented to improve the Mean Time To Recovery (MTTR) in the described system?"}
{"relevant_passages": ["Unfortunately, the queries were slow, resulting in Out Of Memory kills in some cases. This was not good enough for what we expected of Weaviate. ### Investigation\n\nTo investigate the issue, we've set up a database with 1M objects and a profiler to watch memory consumption. We used that setup to run ten parallel filtered aggregations. Upon reviewing the memory consumption, we noted that some of the filtered aggregations were taking up to **200GB** of RAM (note, this was not the total allocated memory on the heap, as a big part of it was waiting to be collected by GC)."], "query": "How much RAM did some filtered aggregations consume during the investigation of slow queries in Weaviate?"}
{"relevant_passages": ["For example, if we broke down this blog post into **chapters** in Weaviate, with **title** and **content** properties. We could run a query to summarize the *\"New distance metrics\"* chapter like this:\n\n```graphql\n{\n  Get {\n    Chapter(\n      where: {\n        operator: Equal\n        path: \"title\"\n        valueText: \"New distance metrics\"\n      }\n    ) {\n      title\n      _additional{\n        summary(\n          properties: [\"content\"],\n        ) {\n          property\n          result\n        }\n      }\n    }\n  }\n}\n```\n\nWhich would return the following result:\n\n```graphql\n{\n  \"data\": {\n    \"Get\": {\n      \"Chapters\": [\n        {\n          \"_additional\": {\n            \"summary\": [\n              {\n                \"property\": \"content\",\n                \"result\": \"Weaviate 1.15 adds two new distance metrics - Hamming\n                 distance and Manhattan distance. In total, you can now choose\n                 between five various distance metrics to support your datasets. Check out the metrics documentation page, for the full overview\n                 of all the available metrics in Weaviate.\"\n              }\n            ]\n          },\n          \"title\": \"New distance metrics\"\n        }\n      ]\n    }\n  },\n  \"errors\": null\n}\n```\n\nHead to the [Summarization Module docs page](/developers/weaviate/modules/reader-generator-modules/sum-transformers) to learn more. ### Hugging Face Module\nThe Hugging Face module (`text2vec-huggingface`) opens up doors to over 600 [Hugging Face sentence similarity models](https://huggingface.co/models?pipeline_tag=sentence-similarity), ready to be used in Weaviate as a vectorization module."], "query": "What new distance metrics were added in Weaviate 1.15?"}
{"relevant_passages": ["---\ntitle: Weaviate 1.2 release - transformer models\nslug: weaviate-1-2-transformer-models\nauthors: [etienne]\ndate: 2021-03-30\ntags: ['release']\nimage: ./img/hero.png\n# canonical-url: https://medium.com/semi-technologies/weaviate-version-1-2-x-now-supports-transformer-models-4a12d858cce3\n# canonical-name: Medium\ndescription: \"Weaviate v1.2 introduced support for transformers (DistilBERT, BERT, RoBERTa, Sentence-BERT, etc) to vectorize and semantically search through your data.\"\n---\n![Weaviate 1.2 release - transformer models](./img/hero.png)\n\nIn the v1.0 release of Weaviate ([docs](/developers/weaviate/) \u2014 [GitHub](https://github.com/weaviate/weaviate)) we introduced the concept of [modules](/developers/weaviate/concepts/modules). Weaviate modules are used to extend the vector database with vectorizers or functionality that can be used to query your dataset. With the release of Weaviate v1.2, we have introduced the use of transformers ([DistilBERT](https://arxiv.org/abs/1910.01108), [BERT](https://github.com/google-research/bert), [RoBERTa](https://arxiv.org/abs/1907.11692), Sentence-[BERT](https://arxiv.org/abs/1908.10084), etc) to vectorize and semantically search through your data. <!-- truncate -->\n\n### Weaviate v1.2 introduction video\n\n<div className=\"youtube\">\n    <iframe src=\"//www.youtube.com/embed/S4lXPPZvGPQ\" frameBorder=\"0\" allowFullScreen></iframe>\n</div>\n\n## What are transformers? A [transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)) (e.g., [BERT](https://en.wikipedia.org/wiki/BERT_(language_model))) is a deep learning model that is used for NLP tasks."], "query": "What new feature related to transformer models was introduced in Weaviate version 1.2?"}
{"relevant_passages": ["Full dynamic scalability will be added in a future release. # highlight-start\nreplicas: 3\n# highlight-end\n... ```\n\nYou can now deploy Weaviate in this configuration by running:\n\n```shell\nkubectl create namespace weaviate\n\nhelm upgrade --install \\\n  \"weaviate\" \\\n  weaviate/weaviate \\\n  --namespace \"weaviate\" \\\n  --values ./values.yaml\n```\n\nThis will deploy the Weaviate clusters. You can check the status of the deployment by running:\n\n```shell\nkubectl get pods -n weaviate\n```\n\nThis should produce an output similar to the following:\n\n```shell\nNAME         READY   STATUS    RESTARTS   AGE\nweaviate-0   1/1     Running   0          3m00s\nweaviate-1   1/1     Running   0          2m50s\nweaviate-2   1/1     Running   0          2m40s\n```\n\nNow, you need to expose the Weaviate service to the outside world - i.e. to your local machine. You can do this by running:\n\n```shell\nminikube service weaviate --namespace weaviate\n```\n\nThis should show an output similar to the following that shows the URL to access the Weaviate cluster:\n\n```shell\n|-----------|----------|-------------|------------------------|\n| NAMESPACE |   NAME   | TARGET PORT |          URL           |\n|-----------|----------|-------------|------------------------|\n| weaviate  | weaviate |             | http://127.0.0.1:54847 |\n|-----------|----------|-------------|------------------------|\n```\n\nAnd it should also open a browser window showing the list of Weaviate endpoints."], "query": "How many replicas does the current Weaviate deployment configuration specify?"}
{"relevant_passages": ["Along with PQ they are also exploring [multi-tenancy](/blog/multi-tenancy-vector-search) that will allow them to scale and perform personalized vector search for millions of customers. [See a demo](https://www.youtube.com/watch?v=hOsBxvV9rvI) of Moonsift\u2019s shopping Copilot, and [sign up for early access today](https://www.moonsift.com/copilot)!\n\n\n## What's next\nInterested in using Weaviate to power your search and AI applications? [Give our fully managed cloud offering a spin](https://console.weaviate.cloud/) for free and check out our [Quickstart guide](/developers/weaviate/quickstart). You can reach out to us on [Slack](https://weaviate.io/slack) or [Twitter](https://twitter.com/weaviate_io), or [join the community forum](https://forum.weaviate.io/)."], "query": "What feature does Weaviate offer to scale personalized vector search for millions of customers?"}
{"relevant_passages": ["* Should it be just an implementation of DiskANN? * Or should we explore the capabilities of HNSW and adjust it to work on disk? * How can we guarantee the excellent database UX \u2013 so valuable to many Weaviate users \u2013 while reaping the benefits of a disk-based solution? Stay tuned as we explore these challenges and questions. We will share our insights as we go."], "query": "What challenges and questions are being explored regarding disk-based solutions for Weaviate users?"}
{"relevant_passages": ["**TL;DR**: We were inspired to write this blog post from our conversation with the creators of [Ragas](https://docs.ragas.io/en/latest/), Jithin James and Shauhul Es on the [77th Weaviate podcast](https://www.youtube.com/watch?v=C-UQwvO8Koc). These new advances in using LLMs to evaluate RAG systems, pioneered by Ragas and ARES, motivated us to reflect on previous metrics and take inventory of the RAG knobs to tune. Our investigation led us to think further about what RAG experiment tracking software may look like. We also further clarify how we distinguish RAG systems from Agent systems and how to evaluate each. Our blog post has 5 major sections:\n* [**LLM Evaluations**](#llm-evaluations): New trends in using LLMs to score RAG performance and scales of Zero-Shot, Few-Shot, and Fine-Tuned LLM Evaluators."], "query": "What inspired the authors to write the blog post about evaluating RAG systems and what podcast episode did they reference?"}
{"relevant_passages": ["This would result in 6 separate calaculations. <img\n  src={require('./img/knn-boules.png').default}\n  alt=\"kNN search in a game of Boules\"\n  style={{ maxWidth: \"50%\" }}\n/>\n\n*[Figure 1 - kNN search in a game of Boules.]*\n\n### A kNN search is computationally very expensive\nComparing a search vector with 10, 100, or 1000 data vectors in just two dimensions is an easy job. But of course, in the real world, we are more likely to deal with millions (like in the Wikipedia dataset) or even billions of data items. In addition, the number of dimensions that most ML models use in semantic search goes up to hundreds or thousands of dimensions!\n\nThe *brute* force of a **kNN search is computationally very expensive** - and depending on the size of your database, a single query could take anything from several seconds to even hours (yikes\ud83d\ude05). If you compare a vector with 300 dimensions with 10M vectors, the vector search would need to do 300 x 10M = 3B computations! The number of required calculations increases linearly with the number of data points (O(n)) (Figure 2)."], "query": "How many computations are required for a kNN search comparing a 300-dimensional vector with 10 million data vectors?"}
{"relevant_passages": ["[Weaviate](/developers/weaviate/), an open-source vector database written in Go, can serve thousands of queries per second. Running Weaviate on [Sift1M](https://www.tensorflow.org/datasets/catalog/sift1m) (a 128-dimensional representation of objects) lets you serve queries in single-digit milliseconds. But how is this possible? ![SIFT1M Benchmark example](./img/SIFT1M-benchmark.png)\n*See the [benchmark](/developers/weaviate/benchmarks/ann) page for more stats.*\n\nWeaviate does not look for the exact closest vectors in the store. Instead, it looks for approximate (close enough) elements."], "query": "How does Weaviate achieve high query performance on the Sift1M dataset?"}
{"relevant_passages": ["In conclusion, efforts on developing multimodal models attempt to mimic human learning by combining different inputs, such as images, text, and audio, to improve the performance and robustness of machine learning systems. By leveraging multi-sensory inputs, these models can learn to recognize complex multimodal patterns, understand context across modes, and generate more comprehensive and accurate outputs even in the absence of some modalities. The main goal is to give these models the ability to interact with data in a more natural way thus enabling them to be more powerful and general reasoning engines. ## Multimodal Models in Weaviate\n\nCurrently, the only out-of-the-box multimodal module that can be configured and used with Weaviate is `multi2vec-clip` which can be used to project images and text into a joint embedding space and then perform a `nearVector` or `nearImage` search over these two modalities. Outside of this, you can only use multimodal models if they are hosted on Huggingface or if you have your own proprietary multimodal models."], "query": "What is the only out-of-the-box multimodal module available in Weaviate for joint embedding of images and text?"}
{"relevant_passages": ["Furthermore aligning and normalizing the data across modalities is crucial to ensure compatibility. This is quite challenging due to differences in data formats, temporal alignment, and semantic alignment. If the data is stitched together from different datasets and is not aligned properly then it becomes very difficult for the machine-learning model to extract and learn interdependencies between the modalities. Current approaches address this by taking multiple rich datasets and fusing them across data modalities where possible. So for example you might be able to combine the image/video of a lion from a computer vision dataset with the roar of a lion from an audio dataset but perhaps not with motion since you might not have motion data for a lion."], "query": "Why is it important to align and normalize data across different modalities in machine learning?"}
{"relevant_passages": ["![animation](./img/animation.png)\n\nThe blog post included this great visual to help with the visualization of combining Bi-Encoders and Cross-Encoders. This fishing example explains the concept of coarse-grained retrieval (fishing net = vector search / bm25) and manual inspection of the fish (fishermen = ranking models). Depicted with manual inspection of fish, the main cost of ranking models is speed. In March, Bob van Luijt appeared on a Cohere panel to discuss [\u201cAI and The Future of Search\u201d](https://twitter.com/cohereai/status/1636396916157079554?s=46&t=Zzg6vgh4rwmYEkdV-3v5gg). Bob explained the effectiveness of combining zero-shot vector embedding models from providers such as Cohere, OpenAI, or HuggingFace with BM25 sparse search together in Hybrid Search."], "query": "What visual analogy does the blog post use to explain the combination of Bi-Encoders and Cross-Encoders in search technology?"}
{"relevant_passages": ["* At the **class** level, `vectorizeClassName` will determine whether the class name is used for vectorization. * At the **property** level:\n    * `skip` will determine whether the property should be skipped (i.e. ignored) in vectorization, and\n    * `vectorizePropertyName` will determine whether the property name will be used. * The property `dataType` determines whether Weaviate will ignore the property, as it will ignore everything but `string` and `text` values. > You can read more about each variable in the [schema configuration documentation](/developers/weaviate/manage-data/collections). Let's apply this to our data to set Weaviate's vectorization behavior, then we will confirm it manually using the Cohere API as we did above."], "query": "What settings determine the vectorization behavior of classes and properties in Weaviate, and which data types are eligible for vectorization?"}
{"relevant_passages": ["This is then passed through the language model to generate multiple responses. Another prompt is created to combine all of the initial outputs into one. This technique requires more than one call to the LLM. ### Refine\n\n<img\n    src={require('./img/refine.gif').default}\n    alt=\"alt\"\n    style={{ width: \"100%\" }}\n/>\n\nRefine is a unique technique because it has a local memory. An example of this is to ask the language model to summarize the documents one by one."], "query": "What is the \"Refine\" technique and how does it utilize a language model's local memory?"}
{"relevant_passages": ["Word2vec in particular uses a neural network [model](https://arxiv.org/pdf/1301.3781.pdf) to learn word associations from a large corpus of text (it was initially trained by Google with 100 billion words). It first creates a vocabulary from the corpus, then learns vector representations for the words, typically with 300 dimensions. Words found in similar contexts have vector representations that are close in vector space, but each word from the vocabulary has only one resulting word vector. Thus, the meaning of words can be quantified - \u201crun\u201d and \u201cran\u201d are recognized as being far more similar than \u201crun\u201d and \u201ccoffee\u201d, but words like \u201crun\u201d with multiple meanings have only one vector representation. As the name suggests, word2vec is a word-level model and cannot by itself produce a vector to represent longer text such as sentences, paragraphs or documents."], "query": "What are the characteristics of the Word2vec model for learning word associations from text?"}
{"relevant_passages": ["However, most of the LLM APIs don\u2019t actually give us these probabilities. Further, this is probably pretty slow. We will keep an eye on it, but it doesn\u2019t seem like the next step to take for now. ## Metadata Rankers\nWhereas I would describe Cross-Encoders as `content-based` re-ranking, I would say Metadata rankers are `context-based` re-rankers. Metadata rankers describe using symbolic features to rank relevance."], "query": "What is the difference between Cross-Encoders and Metadata rankers in the context of re-ranking?"}
{"relevant_passages": ["The data is persisted, so you can use it from future invocations, or you can [transfer it to another instance](/developers/weaviate/manage-data/read-all-objects/#restore-to-a-target-instance). You can learn more about running Weaviate locally from client code on the [Embedded Weaviate](/developers/weaviate/installation/embedded/) page. ## <i class=\"fa-solid fa-lightbulb\"></i> Use cases\n\nWhat can you do with Embedded Weaviate? Quite a few things!\n\nFirst off, you can get started very quickly with Weaviate on your local machine, without having to explicitly download, install or instantiate a server. ### Jupyter notebooks\n\nYou can also use Embedded Weaviate from Jupyter notebooks, including on Google Colaboratory."], "query": "Can Embedded Weaviate be used from Jupyter notebooks on Google Colaboratory?"}
{"relevant_passages": ["---\ntitle: Weaviate 1.23 Release\nslug: weaviate-1-23-release\nauthors: [jp, dave]\ndate: 2023-12-19\nimage: ./img/hero.png\ntags: ['release', 'engineering']\ndescription: \"Weaviate 1.23 released with AutoPQ, flat indexing + Binary Quantization, OSS LLM support through Anyscale, and more!\"\n\n---\n\nimport Core123 from './_core-1-23-include.mdx' ;\n\n<Core123 />\n\nimport WhatsNext from '/_includes/what-next.mdx'\n\n<WhatsNext />\n\nimport Ending from '/_includes/blog-end-oss-comment.md' ;\n\n<Ending />"], "query": "What features were introduced in Weaviate 1.23 released on 2023-12-19?"}
{"relevant_passages": ["Speaking of Cloud, arguably the easiest way to spin up a new use case with Weaviate is through the [Weaviate Cloud Services](/pricing). <br></br>\n\n### New Vector Indexes\n![vector indexes](./img/vector-indexes.png)\n\nLast year we gave you a sneak peek into our [Vector Indexing Research](/blog/ann-algorithms-vamana-vs-hnsw), and this year you will be able to try out new vector indexes for yourself. Since the beginning, Weaviate has supported vector indexing with [HNSW](/developers/weaviate/concepts/vector-index), which leads to [best-in-class query times](/developers/weaviate/benchmarks/ann). But not every use case requires single-digit millisecond latencies. Instead, some prefer cost-effectiveness."], "query": "What vector indexing method has Weaviate traditionally supported for fast query times?"}
{"relevant_passages": ["This demo is also using OpenAI for vectorization; you can choose another `text2vec` module [here](/developers/weaviate/modules/retriever-vectorizer-modules). ```python\nclient = weaviate.Client(\n    embedded_options=EmbeddedOptions(\n        additional_env_vars={\"OPENAI_APIKEY\": os.environ[\"OPENAI_APIKEY\"]}\n    )\n)\n```\n\n### Configure the Schema\n\nNow we need to configure our schema. We have the `document` class along with the `abstract` property. ```python\nclient.schema.delete_all()\n\nschema = {\n    \"class\": \"Document\",\n    \"vectorizer\": \"text2vec-openai\",\n    \"properties\": [\n        {\n            \"name\": \"source\",\n            \"dataType\": [\"text\"],\n        },\n        {\n            \"name\": \"abstract\",\n            \"dataType\": [\"text\"],\n            \"moduleConfig\": {\n                \"text2vec-openai\": {\"skip\": False, \"vectorizePropertyName\": False}\n            },\n        },\n    ],\n    \"moduleConfig\": {\n        \"generative-openai\": {},\n        \"text2vec-openai\": {\"model\": \"ada\", \"modelVersion\": \"002\", \"type\": \"text\"},\n    },\n}\n\nclient.schema.create_class(schema)\n```\n\n### Read/Import the documents\n\nNow that our schema is defined, we want to build the objects that we want to store in Weaviate. We wrote a helper class,  `AbstractExtractor` to aggregate the element class."], "query": "How do you configure a Weaviate schema to use OpenAI's `text2vec-openai` vectorizer with the `ada` model version `002`?"}
{"relevant_passages": ["The algorithm keeps a result set of points, starting with the entry point. On every iteration, it checks what points are in the result set that has not been visited yet and, from them, takes the best candidate (the one closest to the query point) and explores it. Exploring in this context means adding the candidate (out neighbors) from the graph to the result set and marking it as visited. Notice the size of the result set has to stay bounded, so every time it grows too much, we only keep those L points closer to the query. The bigger the maximum size of the result set, the more accurate the results and the slower the search."], "query": "How does the algorithm ensure that the result set of points remains bounded in size during the search process?"}
{"relevant_passages": ["have published \u201cLarge Language Models are easily distracted by irrelevant context\u201d, highlighting how problematic bad precision in search can be for retrieval-augmented generation. The recent developments in LLM agent tooling such as LangChain, LlamaIndex, and recent projects such as AutoGPT or Microsoft\u2019s Semantic Kernel are paving the way towards letting LLMs run for a while to complete complex tasks. By ranking each handoff from search to prompt, we can achieve better results in each intermediate task. Thus when we leave an LLM running overnight to research the future of ranking models, we can expect a better final result in the morning!\n\n\nimport WhatNext from '/_includes/what-next.mdx'\n\n<WhatNext />"], "query": "What are the recent developments in tooling for Large Language Models that aid in complex task completion?"}
{"relevant_passages": ["The good news is, there are companies \u2013 like Hugging Face, OpenAI, and Cohere \u2013 that offer running model inference as a service. > \"Running model inference in production is hard,\nlet them do it for you.\"\n\n## Support for Hugging Face Inference API in Weaviate\nStarting from Weaviate `v1.15`, Weaviate includes a Hugging Face module, which provides support for Hugging Face Inference straight from the vector database. The Hugging Face module, allows you to use the [Hugging Face Inference service](https://huggingface.co/inference-api#pricing) with sentence similarity models, to vectorize and query your data, straight from Weaviate. No need to run the Inference API yourself. > You can choose between `text2vec-huggingface` (Hugging Face) and `text2vec-openai` (OpenAI) modules to delegate your model inference tasks.<br/>\n> Both modules are enabled by default in the [Weaviate Cloud Services](/pricing)."], "query": "Which version of Weaviate started to include support for the Hugging Face Inference API?"}
{"relevant_passages": ["---\ntitle: Building an AI-Powered Shopping Copilot with Weaviate\nslug: moonsift-story\nauthors: [alea, zain]\ndate: 2023-11-15\ntags: []\nimage: ./img/hero.png\ndescription: \"UK-based startup Moonsift is harnessing the power of AI with Weaviate.\"\n---\n![hero](img/hero.png)\n\nUK-based startup Moonsift is harnessing the power of AI\u2014using machine learning models and Weaviate\u2019s vector database\u2014to help online shoppers discover the products they love. <!-- truncate -->\n\n[Moonsift](https://www.moonsift.com/) offers an ecommerce browser extension for users to curate shoppable boards with products from across the internet. Stylists and curators use Moonsift to create collections, registries, and wish lists that can be shared and shopped with a simple link. While thousands of customers add products from tens of thousands of retailers per month to Moonsift, co-founders David Wood and Alex Reed have a bigger vision for improving product discoverability for online shoppers. With combined experience in natural language processing (NLP), data science, and consulting for retail brands, Wood and Reed saw how retailers unknowingly restrict their own discoverability by tailoring keywords for search engines rather than users."], "query": "What is the UK-based startup that uses Weaviate's vector database to enhance online shopping discoverability?"}
{"relevant_passages": ["Each `text2vec-*` module uses an external API (like `text2vec-openai` or `text2vec-huggingface`) or a local instance like `text2vec-transformers` to produce a vector for each object. Let's try vectorizing data with the `text2vec-cohere` module. We will be using data from `tiny_jeopardy.csv` [available here](https://github.com/weaviate/weaviate-examples/tree/main/text2vec-behind-curtain) containing questions from the game show Jeopardy. We'll just use a few (20) questions here, but the [full dataset on Kaggle](https://www.kaggle.com/datasets/tunguz/200000-jeopardy-questions) includes 200k+ questions. Load the data into a Pandas dataframe, then populate Weaviate like this:\n\n```python\nclient.batch.configure(batch_size=100)  # Configure batch\nwith client.batch as batch:\n    for i, row in df.iterrows():\n        properties = {\n            \"question\": row.Question,\n            \"answer\": row.Answer\n        }\n        batch.add_data_object(properties, \"Question\")\n```\n\nThis should add a series of `Question` objects with text properties like this:\n\n```text\n{'question': 'In 1963, live on \"The Art Linkletter Show\", this company served its billionth burger',\n 'answer': \"McDonald's\"}\n```\n\nSince we use the `text2vec-cohere` module to vectorize our data, we can query Weaviate to find data objects most similar to any input text."], "query": "How can you use the `text2vec-cohere` module to find questions similar to \"In 1963, live on 'The Art Linkletter Show', this company served its billionth burger\"?"}
{"relevant_passages": ["We will call this in order to grab the abstract element along with the content. <details>\n  <summary>AbstractExtractor</summary>\n\n```python\nimport logging\n\nlogging.basicConfig(level=logging.INFO)\n\n\nclass AbstractExtractor:\n    def __init__(self):\n        self.current_section = None  # Keep track of the current section being processed\n        self.have_extracted_abstract = (\n            False  # Keep track of whether the abstract has been extracted\n        )\n        self.in_abstract_section = (\n            False  # Keep track of whether we're inside the Abstract section\n        )\n        self.texts = []  # Keep track of the extracted abstract text\n\n    def process(self, element):\n        if element.category == \"Title\":\n            self.set_section(element.text)\n\n            if self.current_section == \"Abstract\":\n                self.in_abstract_section = True\n                return True\n\n            if self.in_abstract_section:\n                return False\n\n        if self.in_abstract_section and element.category == \"NarrativeText\":\n            self.consume_abstract_text(element.text)\n            return True\n\n        return True\n\n    def set_section(self, text):\n        self.current_section = text\n        logging.info(f\"Current section: {self.current_section}\")\n\n    def consume_abstract_text(self, text):\n        logging.info(f\"Abstract part extracted: {text}\")\n        self.texts.append(text)\n\n    def consume_elements(self, elements):\n        for element in elements:\n            should_continue = self.process(element)\n\n            if not should_continue:\n                self.have_extracted_abstract = True\n                break\n\n        if not self.have_extracted_abstract:\n            logging.warning(\"No abstract found in the given list of objects.\")\n\n    def abstract(self):\n        return \"\\n\".join(self.texts)\n```\n</details>\n\n```python\ndata_folder = \"../data\"\n\ndata_objects = []\n\nfor path in Path(data_folder).iterdir():\n    if path.suffix != \".pdf\":\n        continue\n\n    print(f\"Processing {path.name}...\")\n\n    elements = partition_pdf(filename=path)\n\n    abstract_extractor = AbstractExtractor()\n    abstract_extractor.consume_elements(elements)\n\n    data_object = {\"source\": path.name, \"abstract\": abstract_extractor.abstract()}\n\n    data_objects.append(data_object)\n```\n\nThe next step is to import the objects into Weaviate. ```python\nclient.batch.configure(batch_size=100)  # Configure batch\nwith client.batch as batch:\n    for data_object in data_objects:\n        batch.add_data_object(data_object, \"Document\")\n```\n\n### Query Time\n\nNow that we have imported our two documents, we can run some queries! Starting with a simple BM25 search. We want to find a document that discusses house prices. ```python\nclient.query.get(\"Document\", \"source\").with_bm25(\n    query=\"some paper about housing prices\"\n).with_additional(\"score\").do()\n```\n\n<details>\n  <summary>Output</summary>\n\n```\n{'data': {'Get': {'Document': [{'_additional': {'score': '0.8450042'},\n     'source': 'paper02.pdf'},\n    {'_additional': {'score': '0.26854637'}, 'source': 'paper01.pdf'}]}}}\n```\n\n</details>\n\nWe can take this one step further by using the generative search module."], "query": "How can you extract abstracts from documents and perform a BM25 search for a specific topic in Weaviate?"}
{"relevant_passages": ["```\n\n</details>\n\n## Limitations\nThere are a few limitations when it comes to a document that has two columns. For example, if a document is structured with two columns, then the text doesn\u2019t extract perfectly. The workaround for this is to set `strategy=\"ocr_only\"` or `strategy=\"fast\"` into `partition_pdf`. There is a [GitHub issue](https://github.com/Unstructured-IO/unstructured/issues/356) on fixing multi-column documents, give it a \ud83d\udc4d up!\n\n<details>\n  <summary>strategy=\"ocr_only\"</summary>\n\n```python\nelements = partition_pdf(filename=\"../data/paper02.pdf\", strategy=\"ocr_only\")\nabstract_extractor = AbstractExtractor()\nabstract_extractor.consume_elements(elements)\n```\n\n</details>\n\n<details>\n  <summary>strategy=\u201dfast\u201d</summary>\n\n```python\nelements = partition_pdf(filename=\"../data/paper02.pdf\", strategy=\"fast\")\nabstract_extractor = AbstractExtractor()\nabstract_extractor.consume_elements(elements)\n```\n\n</details>\n\n## Weaviate Brick in Unstructured\nThere is a [GitHub issue](https://github.com/Unstructured-IO/unstructured/issues/566) to add a Weaviate staging brick! The goal of this integration is to add a Weaviate section to the documentation and show how to load unstructured outputs into Weaviate. Make sure to give this issue a \ud83d\udc4d up!\n\n## Last Thought\nThis demo introduced how you can ingest PDFs into Weaviate."], "query": "What strategies are recommended for extracting text from two-column PDF documents using `partition_pdf`?"}
{"relevant_passages": ["If you want to learn how to configure Weaviate to use PQ refer to the docs [here](/developers/weaviate/config-refs/schema/vector-index#how-to-configure-hnsw). ## KMeans encoding results\n\nFirst, the PQ feature added to version 1.18 of Weaviate is assessed in the sections below. To check performance and distortion, we compared our implementation to [NanoPQ](https://github.com/matsui528/nanopq) and we observed similar results. The main idea behind running these experiments is to explore how PQ compression would affect our current indexing algorithms. The experiments consist of fitting the Product Quantizer on some datasets and then calculating the recall by applying brute force search on the compressed vectors."], "query": "How does Weaviate's PQ feature performance compare to NanoPQ?"}
{"relevant_passages": ["![Hacktober video](img/hacktober.gif)\n\n### [Weaviate Academy](/developers/academy) & [Workshops](/learn/workshops)\nWeaviate Academy and Workshops have had a fantastic year of learning and growth! We've been focusing on ensuring everyone has the chance to understand and use vector databases and get a grasp on Generative AI and data handling. Every week, [Zain](https://www.linkedin.com/in/zainhas/), [JP](https://www.linkedin.com/in/jphwang/), [Daniel](https://www.linkedin.com/in/malgamves/), and [Duda](https://www.linkedin.com/in/dudanogueira/) have been running introductory workshops on vector databases and Weaviate, which have been a hit. Plus, we're super excited about [JP Hwang](https://www.linkedin.com/in/jphwang/)'s initiative, the Weaviate Academy. It's a program that takes you from the basics all the way to production-level skills. Later in the year, we teamed up with [DeepLearningAI](https://www.deeplearning.ai/) to create a short course with [Sebastian](https://www.linkedin.com/in/sebawita/) and [Zain](https://www.linkedin.com/in/zainhas/), \"[Vector Databases: from Embeddings to Applications with Weaviate](https://www.deeplearning.ai/short-courses/vector-databases-embeddings-applications/).\" It\u2019s been a year packed with learning."], "query": "Who has been running introductory workshops on vector databases and Weaviate every week as part of the Weaviate Academy and Workshops?"}
{"relevant_passages": ["This seems like a weird point to make but it will be very important later on!\n\n![three images](./img/three_images.png)\n\nLearning the true underlying distribution of any set of images is not computationally feasible  because it requires you to consider every pixel of every image. However, if our model could learn the underlying distribution of the training set of images it could calculate the likelihood that any new image came from that set. It could also generate novel images that it thinks are most likely to belong to the training set. One way to do this, using the underlying distribution, would be to start off with static noise (an image with random pixel values) and then slightly alter pixel values over and over again while making sure each time you alter the pixel values it increases the likelihood of the overall image coming from the dataset - this is indeed what diffusion models do!\n\nThe question then becomes how diffusion models can learn (or even approximate) the underlying distribution of the images in your training set? The main insight behind how this happens is: if you take any image from your training set and add a small amount of random static noise to it you will create a new image that is slightly less likely - since images with random noise are unlikely to be seen in the training set."], "query": "How do diffusion models generate images that are likely to belong to a training set?"}
