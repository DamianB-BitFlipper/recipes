{"relevant_passages": ["We instead may want to have another format, whether that be data stored in an S3 bucket or something else, that has the associated metadata with it, but provides a more economical way to experiment with this. On the other hand, we have model fine-tuning and continual learning with gradients, rather than data inserts or updates. The most common models used in RAG are embeddings, re-rankers, and of course, LLMs. Keeping machine learning models fresh with new data has been a longstanding focus of continual learning frameworks and MLops orchestration that manage the re-training and testing and deployment of new models. Starting with continual learning of LLMs, one of the biggest selling points of RAG systems is the ability to extend the \u201ccut-off\u201d date of the LLM\u2019s knowledge base, keeping it up to date with your data. Can the LLM do this directly?"], "query": "Can Large Language Models directly extend their knowledge base to stay current with new data in RAG systems?"}
{"relevant_passages": ["It then takes the summaries generated so far to influence the next output. It repeats this process until all documents have been processed. ### Map Rerank\n\n<img\n    src={require('./img/map-rerank.gif').default}\n    alt=\"alt\"\n    style={{ width: \"100%\" }}\n/>\n\nMap Rerank involves running an initial prompt that asks the model to give a relevance score. It is then passed through the language model and assigns a score based on the certainty of the answer. The documents are then ranked and the top two are stuffed to the language model to output a single response."], "query": "What is the Map Rerank process in document processing and how does it work?"}
{"relevant_passages": ["### Patch 1.15.1 note\nWe have published a patch release v1.15.1.<br/>\nTo learn more check the [Weaviate 1.15.1 patch release](/blog/weaviate-1-15-1-release) blog. ### Community effort\n![New Contributors](./img/new-contributors.jpg)\n\n\ud83d\ude00We are extremely happy about this release, as it includes two big community contributions from [Aakash Thatte](https://github.com/sky-2002) and [Dasith Edirisinghe](https://github.com/DasithEdirisinghe). Over the last few weeks, they collaborated with our engineers to make their contributions. \ud83d\ude80**Aakash** implemented the two **new distance metrics**, while **Dasith** contributed by implementing the two **new Weaviate modules**. \ud83d\udc55We will send some Weaviate t-shirts to Aakash and Dasith soon."], "query": "Who implemented the new distance metrics in Weaviate 1.15.1 patch release?"}
{"relevant_passages": ["A system decided to break it into the sub questions {sub_question_1} and {sub_question_2}. Does this decomposition of the question make sense?\u201d. We then have two separate RAG evaluations for each of the sub questions, and then an evaluation of whether the LLM was able to combine the answers from each question to answer the original question. As another example of evolving complexity from RAG to Agents, let\u2019s consider Routing Query Engines. The following visual illustrates an agent routing a query to either an SQL or Vector Database query."], "query": "How does a system decompose complex queries for evaluation and route them to the appropriate database?"}
{"relevant_passages": ["You can see how replication significantly improves availability. Weaviate provides further configurability and nuance for you in this area by way of a consistency guarantee setting. For example, a request made with a consistency level of QUORUM would require over half of the nodes which contain the data to be up, while a request with ONE consistency would only require one node to be up. :::note Notes\n- The replication algorithm makes sure that no node holds a tenant twice. Replication is always spread out across nodes."], "query": "How does Weaviate ensure high availability through its replication feature, and what are the different consistency levels it offers?"}
{"relevant_passages": ["A larger ef results in more distance comparisons done during the search, slowing it down significantly although producing a more accurate result. The next parameters to look at are the ones used in index building, efConstruction, the size of the queue when inserting data into the graph, and maxConnections, the number of edges per node, which also must be stored with each vector. Another new direction we are exploring is the impact of distribution shift on PQ centroids and the intersection with hybrid clustering and graph index algorithms such as [DiskANN](https://suhasjs.github.io/files/diskann_neurips19.pdf) or [IVFOADC+G+P](https://openaccess.thecvf.com/content_ECCV_2018/papers/Dmitry_Baranchuk_Revisiting_the_Inverted_ECCV_2018_paper.pdf). Using the Recall metric may be a good enough measure of this to trigger re-fitting the centroids, with the question then being: which subset of vectors to use in re-fitting. If we use the last 100K that may have caused the recall drop, we could risk overfitting to the new distribution, thus we likely want some hybrid sampling of the timeline of our data distribution when inserted into Weaviate."], "query": "What is the effect of increasing the `ef` parameter on the performance of a search algorithm?"}
{"relevant_passages": ["---\ntitle: Weaviate 1.15 release\nslug: weaviate-1-15-release\nauthors: [connor, erika, laura, sebastian]\ndate: 2022-09-07\ntags: ['release']\nimage: ./img/hero.png\ndescription: \"Weaviate 1.15 introduces Cloud-native Backups, Memory Optimizations, faster Filtered Aggregations and Ordered Imports, new Distance Metrics and new Weaviate modules.\"\n---\n![Weaviate 1.15 release](./img/hero.png)\n\n<!-- truncate -->\n\nWe are happy to announce the release of Weaviate 1.15, which is packed with great features, significant performance improvements, new distance metrics and modules, and many smaller improvements and fixes. ## The brief\n\nIf you like your content brief and to the point, here is the TL;DR of this release:\n1. [\u2601\ufe0fCloud-native backups](#cloud-native-backups) - allows you to configure your environment to create backups - of selected classes or the whole database - straight into AWS S3, GCS or local filesystem\n1. [Reduced memory usage](#reduced-memory-usage) - we found new ways to optimize memory usage, reducing RAM usage by 10-30%. 1."], "query": "What are the new features introduced in Weaviate 1.15?"}
{"relevant_passages": ["---\ntitle: Wikipedia and Weaviate\nslug: semantic-search-with-wikipedia-and-weaviate\nauthors: [bob]\ndate: 2021-11-25\ntags: ['how-to']\nimage: ./img/hero.jpg\n# canonical-url: https://towardsdatascience.com/semantic-search-through-wikipedia-with-weaviate-graphql-sentence-bert-and-bert-q-a-3c8a5edeacf6\n# canonical-name: Towards Data Science\ndescription: \"Semantic search on Wikipedia dataset with Weaviate \u2013 vector database.\"\n---\n![Wikipedia and Weaviate](./img/hero.jpg)\n\n<!-- truncate -->\n\nTo conduct semantic search queries on a large scale, one needs a vector database to search through the large number of vector representations that represent the data. To show you how this can be done, [we have open-sourced the complete English language Wikipedia corpus](https://github.com/weaviate/semantic-search-through-wikipedia-with-weaviate) backup in Weaviate. In this article, I will outline how we've created the dataset, show you how you can run the dataset yourself, and present search strategies on how to implement similar vector and semantic search solutions in your own projects and how to bring them to production. The Wikipedia dataset used is the \"truthy\" version of October 9th, 2021. After processing it contains 11.348.257 articles, 27.377.159 paragraphs, and 125.447.595 graph cross-references."], "query": "How can Weaviate be used for semantic search on the Wikipedia dataset?"}
{"relevant_passages": ["---\ntitle: Ingesting PDFs into Weaviate\nslug: ingesting-pdfs-into-weaviate\nauthors: [erika, shukri]\ndate: 2023-05-23\nimage: ./img/hero.png\ntags: ['integrations','how-to']\ndescription: \"Demo on how to ingest PDFs into Weaviate using Unstructured.\"\n\n---\n\n![PDFs to Weaviate](./img/hero.png)\n\n<!-- truncate -->\n\nSince the release of ChatGPT, and the subsequent realization of pairing Vector DBs with ChatGPT, one of the most compelling applications has been chatting with your PDFs (i.e. [ChatPDF](https://www.chatpdf.com/) or [ChatDOC](https://chatdoc.com/)). Why PDFs? PDFs are fairly universal for visual documents, encompassing research papers, resumes, powerpoints, letters, and many more. In our [latest Weaviate Podcast](https://www.youtube.com/watch?v=b84Q2cJ6po8) with Unstructured Founder Brian Raymond, Brian motivates this kind of data by saying \u201cImagine you have a non-disclosure agreement in a PDF and want to train a classifier\u201d. Although PDFs are great for human understanding, they have been very hard to process with computers."], "query": "What is the title of the document that explains how to ingest PDFs into Weaviate and discusses the application of ChatGPT with Vector DBs?"}
{"relevant_passages": ["They offer the advantage of further reasoning about the relevance of results without needing specialized training. Cross Encoders can be interfaced with Weaviate to re-rank search results, trading off performance for slower search speed. * **Metadata Rankers** are context-based re-rankers that use symbolic features to rank relevance. They take into account user and document features, such as age, gender, location, preferences, release year, genre, and box office, to predict the relevance of candidate documents. By incorporating metadata features, these rankers offer a more personalized and context-aware search experience."], "query": "What are Metadata Rankers and what features do they use to rank search results?"}
{"relevant_passages": ["---\ntitle: Combining LangChain and Weaviate\nslug: combining-langchain-and-weaviate\nauthors: [erika]\ndate: 2023-02-21\ntags: ['integrations']\nimage: ./img/hero.png\ndescription: \"LangChain is one of the most exciting new tools in AI. It helps overcome many limitations of LLMs, such as hallucination and limited input lengths.\"\n---\n![Combining LangChain and Weaviate](./img/hero.png)\n\nLarge Language Models (LLMs) have revolutionized the way we interact and communicate with computers. These machines can understand and generate human-like language on a massive scale. LLMs are a versatile tool that is seen in many applications like chatbots, content creation, and much more. Despite being a powerful tool, LLMs have the drawback of being too general."], "query": "What are the benefits of combining LangChain with Weaviate in the context of LLMs?"}
{"relevant_passages": ["Among a long list of capabilities, first- and second-wave databases have their strengths. For example, some are very good at finding every instance of a certain value in a database, and others are very good at storing time sequences. The third wave of database technologies focuses on data that is processed by a machine learning model first, where the AI models help in processing, storing and searching through the data as opposed to traditional ways. To better understand the concept, think of a supermarket with 50,000 items. Items on display are not organized alphabetically or by price, the way you'd expect a structured, digital system to do it; they're placed in context."], "query": "What is the unique feature of third-wave database technologies compared to first- and second-wave databases?"}
{"relevant_passages": ["The next step is the routing index used. For corpora of less than 10K vectors, RAG applications may be satisfied with a brute force index. However, with increased vectors brute force latency becomes far slower than Proximity Graph algorithms such as HNSW. As mentioned under RAG Metrics, HNSW performance is typically measured as a pareto-optimal point trading off queries per second with recall. This is done by varying the ef, or size of the search queue, used at inference time."], "query": "What indexing method is recommended for RAG applications with large corpora, and how is the performance of this method measured?"}
{"relevant_passages": ["Generative models are not limited to just generating images; they can also generate songs, written language or any other modality of data - however to make it easier for us to understand we will only consider generative models that for image data. The core idea behind all generative models is that they try to learn and understand what the training set \u201clooks\u201d like. In other words they try to learn the underlying distribution of the training set - which just means that they want to know how likely a datapoint is to be observed in the training set. For example, if you are training a generative model on images of beautiful landscapes then, for that generative model, images of trees and mountains are going to be much more common then images of someones kitchen. Furthering this line of reasoning, for that same generative model, an image of static noise would also be quite unlikely since we don\u2019t see that in the training set."], "query": "What is the core principle behind generative models in relation to their training data, and what types of data can they generate?"}
{"relevant_passages": ["\ud83e\udd14 With this, you can get more out of your existing setups and push your Weaviate instances to do more, or you could save on the resources. ## Better control over Garbage Collector\n\n![GOMEMLIMIT](./img/gomemlimit.jpg)\n\nWeaviate is built from the ground up in Go, which allows for building very performant and memory-safe applications. Go is a garbage-collected language. > *A quick refresher:*<br/>\n> In a garbage-collected language, such as Go, C#, or Java, the programmer doesn't have to deallocate objects manually after using them. Instead, a GC cycle runs periodically to collect memory no longer needed and ensure it can be assigned again."], "query": "What advantage does Weaviate have due to being built in Go related to garbage collection?"}
{"relevant_passages": ["Let's take a look in terms of speed as well as recall. The chart below illustrates a comparison of the C++ Vamana [reference code](https://github.com/microsoft/DiskANN) provided by Microsoft and our [HNSW implementation](https://github.com/weaviate/weaviate/tree/master/adapters/repos/db/vector/hnsw) when using Sift1M. Following Microsoft\u2019s experiments, we have used sift-query.fvecs (100,000 vectors sample) for building the index and sift-query.fvecs (1,000 vectors sample) for querying. We are retrieving 10 **(fig. 1)** and 100 **(fig."], "query": "How does the performance of the C++ Vamana reference code compare to the HNSW implementation when using the Sift1M dataset for indexing and querying?"}
{"relevant_passages": ["Further these approaches are well positioned to generalize to Recommendation. In Recommendation, instead of taking a [query, document] as input to a cross-encoder, we take as input a [user description, document] pair. For example, we can ask users to describe their preferences. Further, we could combine these in trios of [user description, query, item] for LLM, or more lightweight cross-encoder, ranking. There is a bonus 3rd idea where we use the log probabilities concatenating the query with the document."], "query": "What is the novel input method for cross-encoders in recommendation systems that involves user descriptions?"}
{"relevant_passages": [">\n> For GCS you can use a Google Application Credentials json file. Alternatively, you can configure backups with the **local filesystem**. All you need here is to provide the path to the backup folder. > Note, you can have multiple storage configurations - one for each S3, GCS and the local filesystem. ### Creating backups - API\nOnce you have the backup module up and running, you can create backups with a single `POST` command:\n\n```js\nPOST /v1/backups/{storage}/\n{\n  \"id\": \"backup_id\"\n}\n```\n\nThe `storage` values are `s3`, `gcs`, and `filesystem`."], "query": "How can I create backups using an API for different storage configurations such as S3, GCS, and the local filesystem?"}
{"relevant_passages": ["Since it conveys both content and context, such a representation obviously presents a more complete and nuanced data picture. The challenge comes from searching through myriad dimensions. Initially, this was done with a brute force approach, looking at every vector associated with every entry. Needless to say, that approach didn't scale. One breakthrough that helped third-wave vector databases to scale was an approach called \"approximate nearest neighbor\" (ANN) search."], "query": "What technique enabled third-wave vector databases to scale effectively?"}
{"relevant_passages": ["<!-- TODO: update with a link to the article once it is ready -->\n*We are working on an article that will guide you on how to create your own model and upload it to Hugging Face.*\n\n### Fully automated and optimized\nWeaviate manages the whole process for you. From the perspective of writing your code \u2013 once you have your schema configuration \u2013 you can almost forget that Hugging Face is involved at all. For example, when you import data into Weaviate, Weaviate will automatically extract the relevant text fields, send them Hugging Face to vectorize, and store the data with the new vectors in the database. ### Ready to use with a minimum of fuss\nEvery new Weaviate instance created with the [Weaviate Cloud Services](/pricing) has the Hugging Face module enabled out of the box. You don't need to update any configs or anything, it is there ready and waiting."], "query": "How does Weaviate automatically handle data vectorization with Hugging Face, and is the Hugging Face module enabled by default in new Weaviate Cloud Services instances?"}
{"relevant_passages": ["3**: *Summary of the previously presented results. We show results for the shortest and largest parameter sets for uncompressed and compressed versions. Additionally we show the speed down rate in latency and the percentage of memory, compared to uncompressed, needed to operate under compressed options.*\n\nWe would like to give you an extra bonus row from the above table. [Sphere](https://github.com/facebookresearch/sphere) is an open-source dataset recently released by Meta. It collects 768 dimensions and nearly a billion objects."], "query": "What does the document say about the performance comparison between uncompressed and compressed parameter sets?"}
{"relevant_passages": ["For example, you can create a backup called **first_backup** and push it to **GCS**, like this:\n\n```js\nPOST /v1/backups/gcs/\n{\n  \"id\": \"first_backup\"\n}\n```\n\nThen, you can check the backup status by calling:\n\n```js\nGET /v1/backups/gcs/first_backup\n```\n\n### Restore\nTo restore a backup, you can call:\n\n```js\nPOST /v1/backups/{store}/{backup_id}/restore\n```\n\nSo, using our previous example, you can restore the **first_backup**, like this:\n\n```js\nPOST /v1/backups/gcs/first_backup/restore\n```\n\nYou can also, check the status of an ongoing restoration by calling:\n\n```js\nGET /v1/backups/gcs/first_backup/restore\n```\n\n### Cross-cloud\nHere is one interesting thing that you might not have noticed. You can use this setup to run Weaviate with one cloud provider but then store and restore backups to/from another cloud provider. So, for example, you can run Weaviate on AWS and use GCS for your backup needs. How cool is that? ### Class backups\nYou can also create backups for specific classes or select which classes you want to restore."], "query": "How do you restore a backup named \"first_backup\" from GCS using the Weaviate API?"}
{"relevant_passages": ["You find things in the supermarket by understanding how they relate to each other. So if the store gets a new product\u2014say, guavas\u2014you know to look near the apples and bananas, not near garbage bags or other things that happen to also cost $1.98/lb. A key early milestone in the third wave happened in 2015 when Google changed its search algorithm from one based on page rankings to one based on a machine learning model that it dubbed RankBrain. Before then, Google's search engine was essentially a high-powered keyword search that ranked websites by the number of other sites that linked back to them. Essentially, Google trusted rankings to the collective users of the Internet."], "query": "When did Google introduce RankBrain to its search algorithm?"}
{"relevant_passages": ["Developers who want to build AI-powered applications can now skip the tedious process of complex training strategies. Now you can simply take models off-the-shelf and plug them into your apps. Applying a ranking model to hybrid search results is a promising approach to keep pushing the frontier of zero-shot AI. Imagine we want to retrieve information about the Weaviate Ref2Vec feature. If our application is using the Cohere embedding model, it has never seen this term or concept."], "query": "What is the Weaviate Ref2Vec feature and how does it relate to the Cohere embedding model in the context of zero-shot AI?"}
{"relevant_passages": ["The `nearText` filter also allows for [more specific filters](https://towardsdatascience.com/semantic-search-through-wikipedia-with-weaviate-graphql-sentence-bert-and-bert-q-a-3c8a5edeacf6#:~:text=more%20specific%20filters) like `moveAwayFrom` and `MoveTo` concepts to manipulate the search through vector space. ```graphql\n{\n  Get {\n    Paragraph(\n      nearText: {\n        concepts: [\"Italian food\"]\n      }\n      limit: 50\n    ) {\n      content\n      order\n      title\n      inArticle {\n        ... on Article {\n          title\n        }\n      }\n    }\n  }\n}\n```\n\n\ud83d\udca1 LIVE \u2014 [try out this query](https://console.weaviate.io/console/query#weaviate_uri=http://semantic-search-wikipedia-with-weaviate.api.vectors.network:8080&graphql_query=%23%23%0A%23%20Generic%20question%20about%20Italian%20food%0A%23%23%0A%7B%0A%20%20Get%20%7B%0A%20%20%20%20Paragraph(%0A%20%20%20%20%20%20nearText%3A%20%7B%0A%20%20%20%20%20%20%20%20concepts%3A%20%5B%22Italian%20food%22%5D%0A%20%20%20%20%20%20%7D%0A%20%20%20%20%20%20limit%3A%2050%0A%20%20%20%20)%20%7B%0A%20%20%20%20%20%20content%0A%20%20%20%20%20%20order%0A%20%20%20%20%20%20title%0A%20%20%20%20%20%20inArticle%20%7B%0A%20%20%20%20%20%20%20%20...%20on%20Article%20%7B%0A%20%20%20%20%20%20%20%20%20%20title%0A%20%20%20%20%20%20%20%20%7D%0A%20%20%20%20%20%20%7D%0A%20%20%20%20%7D%0A%20%20%7D%0A%7D)\n\n### Example 3 \u2014 mix natural language questions with scalar search\nWithin Weaviate you can also mix scalar search filters with vector search filters. In the specific case, we want to conduct a semantic search query through all the paragraphs of articles about the saxophone player Michael Brecker. ```graphql\n{\n  Get {\n    Paragraph(\n      ask: {\n        question: \"What was Michael Brecker's first saxophone?\"\n        properties: [\"content\"]\n      }\n      where: {\n        operator: Equal\n        path: [\"inArticle\", \"Article\", \"title\"]\n        valueText: \"Michael Brecker\"\n      }\n      limit: 1\n    ) {\n      _additional {\n        answer {\n          result\n        }\n      }\n      content\n      order\n      title\n      inArticle {\n        ..."], "query": "What are the specific filters available in Weaviate's `nearText` search to manipulate vector space, and how can you combine natural language questions with scalar search filters in a query?"}
{"relevant_passages": ["Notice that similar recall/latency results with less segments still mean better compression rate. ![res1](./img/image5.png)\n**Fig. 4**: *Time (min) to fit the Product Quantizer with 200,000 vectors and to encode 1,000,000 vectors, all compared to the recall achieved. The different curves are obtained varying the segment length (shown in the legend) from 1 to 8 dimensions per segment. The points in the curve are obtained varying the amount of centroids.*\n\n![res2](./img/image6.png)\n**Fig."], "query": "How does varying the segment length affect the time to fit a Product Quantizer and encode vectors in relation to the recall achieved?"}
{"relevant_passages": ["The code is then passed through the Python REPL. Python REPL is a code executor implemented in LangChain. Once the code is executed, the output of the code is printed. The language model then sees this output and judges if the code is correct. ## ChatVectorDB\nOne of the most exciting features of LangChain is its collection of preconfigured chains."], "query": "What is the role of the Python REPL in LangChain?"}
{"relevant_passages": ["Only hosting the vectors in memory would take 3.1 TB. To work with data of this size, either you need to spend more to provision expensive resources, or you can sacrifice a bit on recall and latency and save drastically on resources. We show a simple test below to drive home the importance of compressing data and eventually moving graphs also to disk. The test was run over 10 million objects only. A final note on the numbers."], "query": "How much memory is required to host vectors in memory for 10 million objects, and what are the trade-offs of working with such large data sets?"}
{"relevant_passages": ["---\ntitle: Vector Embeddings Explained\nslug: vector-embeddings-explained\nauthors: [dan]\ndate: 2023-01-16\ntags: ['concepts']\nimage: ./img/hero.png\ndescription: \"Get an intuitive understanding of what exactly vector embeddings are, how they're generated, and how they're used in semantic search.\"\n---\nThe core function of Weaviate is to provide high-quality search results, going beyond simple keyword or synonym searches, and actually finding what the user _means_ by the query, or providing an actual answer to questions the user asks. <!-- truncate -->\n\nSemantic searches (as well as question answering) are essentially searches by similarity, such as by the meaning of text, or by what objects are contained in images. For example, consider a library of wine names and descriptions, one of which mentioning that the wine is \u201cgood with **fish**\u201d. A \u201cwine for **seafood**\u201d keyword search, or even a synonym search, won\u2019t find that wine. A meaning-based search should understand that \u201cfish\u201d is similar to \u201cseafood\u201d, and \u201cgood with X\u201d means the wine is \u201cfor X\u201d\u2014and should find the wine."], "query": "What are vector embeddings and how do they facilitate semantic search?"}
{"relevant_passages": ["\ud83d\ude00 <br/>\nKeep in touch and check [our blog](/blog) from time to time. import WhatNext from '/_includes/what-next.mdx'\n\n<WhatNext />"], "query": "Where can I find updates or new content related to the sender of this message?"}
{"relevant_passages": ["ML-Models\n1. Vector database\n\nIn this article, we have shown how you can bring the complete Wikipedia corpus (data) using open-source ML-models (Sentence-BERT) and a vector database (Weaviate) to production. import WhatNext from '/_includes/what-next.mdx'\n\n<WhatNext />"], "query": "How can you use Sentence-BERT and Weaviate to bring the Wikipedia corpus to production?"}
{"relevant_passages": [":::\n\n## Conclusions\nWe've managed to implement the indexing algorithm on DiskANN, and the resulting performance is good. From years of research & development, Weaviate has a highly optimized implementation of the HNSW algorithm. With the Vamana implementation, we achieved comparable in-memory results. There are still some challenges to overcome and questions to answer. For example:\n* How do we proceed to the natural disk solution of Weaviate?"], "query": "What indexing algorithm was implemented on DiskANN, and how does its performance compare to Weaviate's HNSW algorithm?"}
{"relevant_passages": ["* **Score Rankers** employ classifiers or regression models to score and detect content, acting as guardrails for generative models. These scores can help in filtering harmful or NSFW content and prevent hallucinations with cutting edge ideas such as Natural Language Inference filters. Each of these ranking models have particular use cases. However, the lines between these models are blurring with new trends such as translating tabular metadata features into text to facilitate transfer learning from transformers pre-trained on text. Of course, the recent successes of LLMs are causing a rethink of most AI workflows and the application of LLMs to rank and score rankers to filter generations are both exciting."], "query": "What are Score Rankers and how are they being influenced by the recent advancements in Large Language Models?"}
{"relevant_passages": ["It refers to an arrangement where locking occurs on multiple buckets or 'stripes'. Are you curious about, the challenge that we faced, which solutions we considered, and what was our final solution? Read on \ud83d\ude00. ## Background\nDatabases must be able to import data quickly and reliably while maintaining data integrity and reducing time overhead. Weaviate is no exception to this! Given that our users populate Weaviate with hundreds of millions of data objects (if not more), we appreciate that import performance is of the highest ..."], "query": "How did Weaviate improve its data import performance while maintaining data integrity?"}
{"relevant_passages": ["The query is {query}, the search results are {search_results}\u201d. The visualization below shows how an LLM can be used to evaluate the performance of RAG systems. ![RAG-evaluation](img/rag-eval.png)\n\nThere are three major opportunities for tuning Zero-Shot LLM Evaluation: 1. the design of the metrics such as precision, recall, or nDCG, 2. the exact language of these prompts, and 3."], "query": "What are the three major opportunities for tuning Zero-Shot LLM Evaluation?"}
{"relevant_passages": ["Share what you build with Weaviate in [Slack](https://weaviate.slack.com/), on our [Forum](https://forum.weaviate.io/), or on socials. ## Embracing Open Source and Sharing Knowledge\n\nAs AI accelerated throughout the year with ever-new innovations popping up, so did the community's curiosity to learn and share knowledge in that area. As an open-source solution, **community** is a foundational pillar of Weaviate. ### [Hacktoberfest](https://weaviate.io/blog/hacktoberfest-2023)\n\nCelebrating the spirit of **open source**, we participated in our first [Hacktoberfest](https://hacktoberfest.com/) this October, which was organized by our very own **[Leonie Monigatti](https://www.linkedin.com/in/804250ab/)**! This global event, aimed at engineers and machine learning enthusiasts, fosters collaboration and contributions to open-source technology. Participants who had four pull requests (PRs) accepted between October 1 and 31, 2023, earned a unique digital reward and some Weaviate Merch! Contributions varied in scope, ranging from minor, non-coding inputs to more substantial technical improvements."], "query": "Who organized Weaviate's first participation in Hacktoberfest 2023?"}
{"relevant_passages": ["## Discussions & wrap-up\n\nSo there it is. Throughout the above journey, we saw how exactly Weaviate creates vectors from the text data objects, which is:\n\n- Vectorize properties that use `string` or `text` data types\n- Sorts properties in alphabetical (a-z) order before concatenating values\n- Prepends the class name\n- And converts the whole string to lowercase\n\nAnd we also saw how this can be tweaked through the schema definition for each class. One implication of this is that your vectorization requirements are a very important part of considerations in the schema definition. It may determine how you break down related data objects before importing them into Weaviate, as well as which fields you choose to import. Let's consider again our quiz question corpus as a concrete example."], "query": "How does Weaviate create vectors from text data objects?"}
{"relevant_passages": ["This sensory exploration helps them link different perspectives of the same experience to create a holistic understanding of their environment. This fusion of multisensory data when learning new concepts is also partially responsible for why humans can learn with very few data points - making us great few-shot learners. Let's imagine you are trying to teach the concept of a dog to a child. The next time you see a dog at the park you point it out and say \u201cThis is a dog!\u201d. Let's say that this is a single observation/data point - in the supervised machine-learning sense."], "query": "How do humans, particularly children, utilize sensory exploration to learn new concepts with limited data points?"}
{"relevant_passages": ["## Tool Use\nThe last building block we will cover is tool use. [Tool use](https://python.langchain.com/docs/modules/agents/tools/) is a way to augment language models to use tools. For example, we can hook up an LLM to [vector databases](https://weaviate.io/blog/what-is-a-vector-database), calculators, or even code executors. Of course we will dive into the vector databases next, but let\u2019s start with an example of the code executor tool use. <img\n    src={require('./img/tool-use.gif').default}\n    alt=\"alt\"\n    style={{ width: \"100%\" }}\n/>\n\nThe task for the language model is to write python code for the bubble sort algorithm."], "query": "What is an example of augmenting language models with a tool to write Python code?"}
{"relevant_passages": ["There you will learn how each of the distances works in more detail, when to use each, and how they compare to other metrics. ## New Weaviate modules\n\n<!-- TODO: add an image for Weaviate modules -->\n![New Weaviate modules](./img/weaviate-modules.png)\n\nThe list of the new goodies included with Weaviate `v1.15` goes on. Courtesy of a fantastic community contribution from [Dasith Edirisinghe](https://github.com/DasithEdirisinghe), we have two new Weaviate modules for you: Summarization and Hugging Face modules. ### Summarization Module\nThe Summarization module allows you to summarize text data at query time. The module adds a `summary` filter under the `_additional` field, which lets you list the properties that should be summarized."], "query": "What new features does Weaviate version 1.15 offer, and who contributed to its development?"}
{"relevant_passages": ["It was that Ofir did such a phenomenal job of figuring out a way to measure the complexity of the knowledge that was extracted from the model. He gave us a benchmark, a ladder to climb, a way to measure whether we could retrieve certain kinds of information from models. And I think that's going to open the door to a ton more benchmarks. And you know what happens when there's a benchmark. We optimize the hell out of that benchmark and it moves science forward\u2026 [ truncated for visibility ] |\n| Hybrid Only            | Or, at least being able to ask follow up questions when it\u2019s unclear about and that\u2019s surprisingly not that difficult to do with these current systems, as long as you\u2019re halfway decent at prompting, you can build up these follow up systems and train them over the course of a couple 1,000 examples to perform really, really well, at least to cove r90, 95% of questions that you might get."], "query": "Who created a benchmark for measuring the complexity of knowledge extracted from models?"}
{"relevant_passages": ["And that\u2019s exactly what they\u2019ve been collecting. Usage of their browser extension over the last few years has enabled Moonsift to explore discovery and train models on over 60M products, 250M interactions, and 40K retailers across the internet. Now, Moonsift has the data they need (which is growing every day) to improve product discovery with AI. ## Building the discovery engine \nThe Moonsift team brought on Marcel Marais as lead machine learning engineer to build a system that could harness the data they\u2019ve gathered to take their product to the next level. At first, Marais looked at improving discovery through a keyword-based search system using BM25 and re-ranking, but he quickly assessed that would not be sufficient to power the type of recommendation engine they needed."], "query": "How many products, interactions, and retailers has Moonsift's browser extension collected data on, and who did they hire as the lead machine learning engineer to improve their discovery engine?"}
{"relevant_passages": ["We are still on memory, are we not? Don't worry. This was just the first step toward our goal. We want to ensure that we have a solid implementation in memory before we move to disk, and this milestone ends here. :::note\n!Spoilers alert, as you read this article, we are evaluating our implementation on disk.<br/>\nWe will prepare a similar article to outline; how we moved everything to disk and what the price was performance-wise."], "query": "What is the current stage of the implementation mentioned in the document, and what are the future plans regarding its transition to disk?"}
{"relevant_passages": ["<details>\n  <summary>Optional: Try it yourself (with minikube)</summary>\n\nYou can try running a local, multi-node Weaviate cluster with `minikube`, which can conveniently run a local Kubernetes cluster. We note that deploying Weaviate on a cloud provider\u2019s kubernetes service follows a similar process. <br/>\n\nFirst, install `minikube` and `helm` for your system by following these guides ([minikube](https://minikube.sigs.k8s.io/docs/start), [helm](https://helm.sh/docs/intro/install)). We also recommend installing `kubectl` ([by following this guide](https://kubernetes.io/docs/tasks/tools/#kubectl)). <br/>\n\nOnce minikube is installed, start a three-node minikube cluster by running the following from the shell:\n\n```shell\nminikube start --nodes 3\n```\n\nOnce the nodes have been created, you should be able to interact with them through the `kubectl` command-line tool."], "query": "How can I set up a local multi-node Weaviate cluster using minikube?"}
{"relevant_passages": ["---\ntitle: Multimodal Embedding Models\nslug: multimodal-models\nauthors: zain\ndate: 2023-06-27\nimage: ./img/hero.png\ntags: ['concepts']\ndescription: \"ML Models that can see, read, hear and more!\"\n\n---\n\n![Multimodal Models](./img/hero.png)\n\n<!-- truncate -->\n\n## The Multisensory Nature of Human Learning\n\nHumans have a remarkable ability to learn and build world models through the integration of multiple sensory inputs. Our combination of senses work synergistically to provide us with rich and diverse information about our environment. By combining and interpreting these sensory inputs, we are able to form a coherent understanding of the world, make predictions, and acquire new knowledge very efficiently. The process of learning through multi-sensory inputs begins from the early stages of human development. Infants explore the world through their senses, touching, tasting, listening, and observing objects and people around them."], "query": "What are ML models that mimic the human ability to integrate multiple sensory inputs called?"}
{"relevant_passages": ["Prompt tuning, 2. Few-Shot Examples, and 3. Fine-Tuning. Prompt tuning entails tweaking the particular language used such as: \u201cPlease answer the question based on the provided search results.\u201d versus \u201cPlease answer the question. IMPORTANT, please follow these instructions closely."], "query": "What is prompt tuning in AI language tasks and how does it differ from few-shot examples and fine-tuning?"}
{"relevant_passages": ["8**: *Time (min) to fit the Product Quantizer with 200,000 vectors and to encode 1,000,000 vectors, all compared to the recall achieved. The different curves are obtained varying the segment length (shown in the legend) from 1 to 6 dimensions per segment. The points in the curve are obtained varying the amount of centroids.*\n\n![res6](./img/image10.png)\n**Fig. 9**: *Average time (microseconds) to calculate distances from query vectors to all 1,000,000 vectors compared to the recall achieved. The different curves are obtained varying the segment length (shown in the legend) from 1 to 6 dimensions per segment."], "query": "What is the relationship between segment length and recall in the performance of a Product Quantizer when fitting and encoding vectors, as well as calculating distances?"}
{"relevant_passages": ["#### Solution\nWe addressed each of the points above individually and improved the overall MTTR substantially:\n\n- A deduplication process was added, so that large WALs with a lot of updates (i.e. redundant data) could be reduced to only the necessary information. - The recovery process now runs in parallel. If there are multiple places that require recovery, they can each recover independently, without one recovery having to wait for the other. - A mechanism was added that flushes any memtable that has been idle (no writes) for 60s or more. In addition to speeding up the recovery, this change also ensures that no recovery is needed at all in many cases."], "query": "What strategies were implemented to improve the Mean Time To Recovery (MTTR) in the described system?"}
{"relevant_passages": ["Unfortunately, the queries were slow, resulting in Out Of Memory kills in some cases. This was not good enough for what we expected of Weaviate. ### Investigation\n\nTo investigate the issue, we've set up a database with 1M objects and a profiler to watch memory consumption. We used that setup to run ten parallel filtered aggregations. Upon reviewing the memory consumption, we noted that some of the filtered aggregations were taking up to **200GB** of RAM (note, this was not the total allocated memory on the heap, as a big part of it was waiting to be collected by GC)."], "query": "How much RAM did some filtered aggregations consume during the investigation of slow queries in Weaviate?"}
{"relevant_passages": ["For example, if we broke down this blog post into **chapters** in Weaviate, with **title** and **content** properties. We could run a query to summarize the *\"New distance metrics\"* chapter like this:\n\n```graphql\n{\n  Get {\n    Chapter(\n      where: {\n        operator: Equal\n        path: \"title\"\n        valueText: \"New distance metrics\"\n      }\n    ) {\n      title\n      _additional{\n        summary(\n          properties: [\"content\"],\n        ) {\n          property\n          result\n        }\n      }\n    }\n  }\n}\n```\n\nWhich would return the following result:\n\n```graphql\n{\n  \"data\": {\n    \"Get\": {\n      \"Chapters\": [\n        {\n          \"_additional\": {\n            \"summary\": [\n              {\n                \"property\": \"content\",\n                \"result\": \"Weaviate 1.15 adds two new distance metrics - Hamming\n                 distance and Manhattan distance. In total, you can now choose\n                 between five various distance metrics to support your datasets. Check out the metrics documentation page, for the full overview\n                 of all the available metrics in Weaviate.\"\n              }\n            ]\n          },\n          \"title\": \"New distance metrics\"\n        }\n      ]\n    }\n  },\n  \"errors\": null\n}\n```\n\nHead to the [Summarization Module docs page](/developers/weaviate/modules/reader-generator-modules/sum-transformers) to learn more. ### Hugging Face Module\nThe Hugging Face module (`text2vec-huggingface`) opens up doors to over 600 [Hugging Face sentence similarity models](https://huggingface.co/models?pipeline_tag=sentence-similarity), ready to be used in Weaviate as a vectorization module."], "query": "What new distance metrics were added in Weaviate 1.15?"}
{"relevant_passages": ["---\ntitle: Weaviate 1.2 release - transformer models\nslug: weaviate-1-2-transformer-models\nauthors: [etienne]\ndate: 2021-03-30\ntags: ['release']\nimage: ./img/hero.png\n# canonical-url: https://medium.com/semi-technologies/weaviate-version-1-2-x-now-supports-transformer-models-4a12d858cce3\n# canonical-name: Medium\ndescription: \"Weaviate v1.2 introduced support for transformers (DistilBERT, BERT, RoBERTa, Sentence-BERT, etc) to vectorize and semantically search through your data.\"\n---\n![Weaviate 1.2 release - transformer models](./img/hero.png)\n\nIn the v1.0 release of Weaviate ([docs](/developers/weaviate/) \u2014 [GitHub](https://github.com/weaviate/weaviate)) we introduced the concept of [modules](/developers/weaviate/concepts/modules). Weaviate modules are used to extend the vector database with vectorizers or functionality that can be used to query your dataset. With the release of Weaviate v1.2, we have introduced the use of transformers ([DistilBERT](https://arxiv.org/abs/1910.01108), [BERT](https://github.com/google-research/bert), [RoBERTa](https://arxiv.org/abs/1907.11692), Sentence-[BERT](https://arxiv.org/abs/1908.10084), etc) to vectorize and semantically search through your data. <!-- truncate -->\n\n### Weaviate v1.2 introduction video\n\n<div className=\"youtube\">\n    <iframe src=\"//www.youtube.com/embed/S4lXPPZvGPQ\" frameBorder=\"0\" allowFullScreen></iframe>\n</div>\n\n## What are transformers? A [transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)) (e.g., [BERT](https://en.wikipedia.org/wiki/BERT_(language_model))) is a deep learning model that is used for NLP tasks."], "query": "What new feature related to transformer models was introduced in Weaviate version 1.2?"}
{"relevant_passages": ["Full dynamic scalability will be added in a future release. # highlight-start\nreplicas: 3\n# highlight-end\n... ```\n\nYou can now deploy Weaviate in this configuration by running:\n\n```shell\nkubectl create namespace weaviate\n\nhelm upgrade --install \\\n  \"weaviate\" \\\n  weaviate/weaviate \\\n  --namespace \"weaviate\" \\\n  --values ./values.yaml\n```\n\nThis will deploy the Weaviate clusters. You can check the status of the deployment by running:\n\n```shell\nkubectl get pods -n weaviate\n```\n\nThis should produce an output similar to the following:\n\n```shell\nNAME         READY   STATUS    RESTARTS   AGE\nweaviate-0   1/1     Running   0          3m00s\nweaviate-1   1/1     Running   0          2m50s\nweaviate-2   1/1     Running   0          2m40s\n```\n\nNow, you need to expose the Weaviate service to the outside world - i.e. to your local machine. You can do this by running:\n\n```shell\nminikube service weaviate --namespace weaviate\n```\n\nThis should show an output similar to the following that shows the URL to access the Weaviate cluster:\n\n```shell\n|-----------|----------|-------------|------------------------|\n| NAMESPACE |   NAME   | TARGET PORT |          URL           |\n|-----------|----------|-------------|------------------------|\n| weaviate  | weaviate |             | http://127.0.0.1:54847 |\n|-----------|----------|-------------|------------------------|\n```\n\nAnd it should also open a browser window showing the list of Weaviate endpoints."], "query": "How many replicas does the current Weaviate deployment configuration specify?"}
{"relevant_passages": ["Along with PQ they are also exploring [multi-tenancy](/blog/multi-tenancy-vector-search) that will allow them to scale and perform personalized vector search for millions of customers. [See a demo](https://www.youtube.com/watch?v=hOsBxvV9rvI) of Moonsift\u2019s shopping Copilot, and [sign up for early access today](https://www.moonsift.com/copilot)!\n\n\n## What's next\nInterested in using Weaviate to power your search and AI applications? [Give our fully managed cloud offering a spin](https://console.weaviate.cloud/) for free and check out our [Quickstart guide](/developers/weaviate/quickstart). You can reach out to us on [Slack](https://weaviate.io/slack) or [Twitter](https://twitter.com/weaviate_io), or [join the community forum](https://forum.weaviate.io/)."], "query": "What feature does Weaviate offer to scale personalized vector search for millions of customers?"}
{"relevant_passages": ["* Should it be just an implementation of DiskANN? * Or should we explore the capabilities of HNSW and adjust it to work on disk? * How can we guarantee the excellent database UX \u2013 so valuable to many Weaviate users \u2013 while reaping the benefits of a disk-based solution? Stay tuned as we explore these challenges and questions. We will share our insights as we go."], "query": "What challenges and questions are being explored regarding disk-based solutions for Weaviate users?"}
{"relevant_passages": ["**TL;DR**: We were inspired to write this blog post from our conversation with the creators of [Ragas](https://docs.ragas.io/en/latest/), Jithin James and Shauhul Es on the [77th Weaviate podcast](https://www.youtube.com/watch?v=C-UQwvO8Koc). These new advances in using LLMs to evaluate RAG systems, pioneered by Ragas and ARES, motivated us to reflect on previous metrics and take inventory of the RAG knobs to tune. Our investigation led us to think further about what RAG experiment tracking software may look like. We also further clarify how we distinguish RAG systems from Agent systems and how to evaluate each. Our blog post has 5 major sections:\n* [**LLM Evaluations**](#llm-evaluations): New trends in using LLMs to score RAG performance and scales of Zero-Shot, Few-Shot, and Fine-Tuned LLM Evaluators."], "query": "What inspired the authors to write the blog post about evaluating RAG systems and what podcast episode did they reference?"}
{"relevant_passages": ["This would result in 6 separate calaculations. <img\n  src={require('./img/knn-boules.png').default}\n  alt=\"kNN search in a game of Boules\"\n  style={{ maxWidth: \"50%\" }}\n/>\n\n*[Figure 1 - kNN search in a game of Boules.]*\n\n### A kNN search is computationally very expensive\nComparing a search vector with 10, 100, or 1000 data vectors in just two dimensions is an easy job. But of course, in the real world, we are more likely to deal with millions (like in the Wikipedia dataset) or even billions of data items. In addition, the number of dimensions that most ML models use in semantic search goes up to hundreds or thousands of dimensions!\n\nThe *brute* force of a **kNN search is computationally very expensive** - and depending on the size of your database, a single query could take anything from several seconds to even hours (yikes\ud83d\ude05). If you compare a vector with 300 dimensions with 10M vectors, the vector search would need to do 300 x 10M = 3B computations! The number of required calculations increases linearly with the number of data points (O(n)) (Figure 2)."], "query": "How many computations are required for a kNN search comparing a 300-dimensional vector with 10 million data vectors?"}
{"relevant_passages": ["[Weaviate](/developers/weaviate/), an open-source vector database written in Go, can serve thousands of queries per second. Running Weaviate on [Sift1M](https://www.tensorflow.org/datasets/catalog/sift1m) (a 128-dimensional representation of objects) lets you serve queries in single-digit milliseconds. But how is this possible? ![SIFT1M Benchmark example](./img/SIFT1M-benchmark.png)\n*See the [benchmark](/developers/weaviate/benchmarks/ann) page for more stats.*\n\nWeaviate does not look for the exact closest vectors in the store. Instead, it looks for approximate (close enough) elements."], "query": "How does Weaviate achieve high query performance on the Sift1M dataset?"}
{"relevant_passages": ["In conclusion, efforts on developing multimodal models attempt to mimic human learning by combining different inputs, such as images, text, and audio, to improve the performance and robustness of machine learning systems. By leveraging multi-sensory inputs, these models can learn to recognize complex multimodal patterns, understand context across modes, and generate more comprehensive and accurate outputs even in the absence of some modalities. The main goal is to give these models the ability to interact with data in a more natural way thus enabling them to be more powerful and general reasoning engines. ## Multimodal Models in Weaviate\n\nCurrently, the only out-of-the-box multimodal module that can be configured and used with Weaviate is `multi2vec-clip` which can be used to project images and text into a joint embedding space and then perform a `nearVector` or `nearImage` search over these two modalities. Outside of this, you can only use multimodal models if they are hosted on Huggingface or if you have your own proprietary multimodal models."], "query": "What is the only out-of-the-box multimodal module available in Weaviate for joint embedding of images and text?"}
{"relevant_passages": ["Furthermore aligning and normalizing the data across modalities is crucial to ensure compatibility. This is quite challenging due to differences in data formats, temporal alignment, and semantic alignment. If the data is stitched together from different datasets and is not aligned properly then it becomes very difficult for the machine-learning model to extract and learn interdependencies between the modalities. Current approaches address this by taking multiple rich datasets and fusing them across data modalities where possible. So for example you might be able to combine the image/video of a lion from a computer vision dataset with the roar of a lion from an audio dataset but perhaps not with motion since you might not have motion data for a lion."], "query": "Why is it important to align and normalize data across different modalities in machine learning?"}
{"relevant_passages": ["![animation](./img/animation.png)\n\nThe blog post included this great visual to help with the visualization of combining Bi-Encoders and Cross-Encoders. This fishing example explains the concept of coarse-grained retrieval (fishing net = vector search / bm25) and manual inspection of the fish (fishermen = ranking models). Depicted with manual inspection of fish, the main cost of ranking models is speed. In March, Bob van Luijt appeared on a Cohere panel to discuss [\u201cAI and The Future of Search\u201d](https://twitter.com/cohereai/status/1636396916157079554?s=46&t=Zzg6vgh4rwmYEkdV-3v5gg). Bob explained the effectiveness of combining zero-shot vector embedding models from providers such as Cohere, OpenAI, or HuggingFace with BM25 sparse search together in Hybrid Search."], "query": "What visual analogy does the blog post use to explain the combination of Bi-Encoders and Cross-Encoders in search technology?"}
{"relevant_passages": ["* At the **class** level, `vectorizeClassName` will determine whether the class name is used for vectorization. * At the **property** level:\n    * `skip` will determine whether the property should be skipped (i.e. ignored) in vectorization, and\n    * `vectorizePropertyName` will determine whether the property name will be used. * The property `dataType` determines whether Weaviate will ignore the property, as it will ignore everything but `string` and `text` values. > You can read more about each variable in the [schema configuration documentation](/developers/weaviate/manage-data/collections). Let's apply this to our data to set Weaviate's vectorization behavior, then we will confirm it manually using the Cohere API as we did above."], "query": "What settings determine the vectorization behavior of classes and properties in Weaviate, and which data types are eligible for vectorization?"}
{"relevant_passages": ["This is then passed through the language model to generate multiple responses. Another prompt is created to combine all of the initial outputs into one. This technique requires more than one call to the LLM. ### Refine\n\n<img\n    src={require('./img/refine.gif').default}\n    alt=\"alt\"\n    style={{ width: \"100%\" }}\n/>\n\nRefine is a unique technique because it has a local memory. An example of this is to ask the language model to summarize the documents one by one."], "query": "What is the \"Refine\" technique and how does it utilize a language model's local memory?"}
{"relevant_passages": ["Word2vec in particular uses a neural network [model](https://arxiv.org/pdf/1301.3781.pdf) to learn word associations from a large corpus of text (it was initially trained by Google with 100 billion words). It first creates a vocabulary from the corpus, then learns vector representations for the words, typically with 300 dimensions. Words found in similar contexts have vector representations that are close in vector space, but each word from the vocabulary has only one resulting word vector. Thus, the meaning of words can be quantified - \u201crun\u201d and \u201cran\u201d are recognized as being far more similar than \u201crun\u201d and \u201ccoffee\u201d, but words like \u201crun\u201d with multiple meanings have only one vector representation. As the name suggests, word2vec is a word-level model and cannot by itself produce a vector to represent longer text such as sentences, paragraphs or documents."], "query": "What are the characteristics of the Word2vec model for learning word associations from text?"}
{"relevant_passages": ["However, most of the LLM APIs don\u2019t actually give us these probabilities. Further, this is probably pretty slow. We will keep an eye on it, but it doesn\u2019t seem like the next step to take for now. ## Metadata Rankers\nWhereas I would describe Cross-Encoders as `content-based` re-ranking, I would say Metadata rankers are `context-based` re-rankers. Metadata rankers describe using symbolic features to rank relevance."], "query": "What is the difference between Cross-Encoders and Metadata rankers in the context of re-ranking?"}
{"relevant_passages": ["The data is persisted, so you can use it from future invocations, or you can [transfer it to another instance](/developers/weaviate/manage-data/read-all-objects/#restore-to-a-target-instance). You can learn more about running Weaviate locally from client code on the [Embedded Weaviate](/developers/weaviate/installation/embedded/) page. ## <i class=\"fa-solid fa-lightbulb\"></i> Use cases\n\nWhat can you do with Embedded Weaviate? Quite a few things!\n\nFirst off, you can get started very quickly with Weaviate on your local machine, without having to explicitly download, install or instantiate a server. ### Jupyter notebooks\n\nYou can also use Embedded Weaviate from Jupyter notebooks, including on Google Colaboratory."], "query": "Can Embedded Weaviate be used from Jupyter notebooks on Google Colaboratory?"}
{"relevant_passages": ["---\ntitle: Weaviate 1.23 Release\nslug: weaviate-1-23-release\nauthors: [jp, dave]\ndate: 2023-12-19\nimage: ./img/hero.png\ntags: ['release', 'engineering']\ndescription: \"Weaviate 1.23 released with AutoPQ, flat indexing + Binary Quantization, OSS LLM support through Anyscale, and more!\"\n\n---\n\nimport Core123 from './_core-1-23-include.mdx' ;\n\n<Core123 />\n\nimport WhatsNext from '/_includes/what-next.mdx'\n\n<WhatsNext />\n\nimport Ending from '/_includes/blog-end-oss-comment.md' ;\n\n<Ending />"], "query": "What features were introduced in Weaviate 1.23 released on 2023-12-19?"}
{"relevant_passages": ["Speaking of Cloud, arguably the easiest way to spin up a new use case with Weaviate is through the [Weaviate Cloud Services](/pricing). <br></br>\n\n### New Vector Indexes\n![vector indexes](./img/vector-indexes.png)\n\nLast year we gave you a sneak peek into our [Vector Indexing Research](/blog/ann-algorithms-vamana-vs-hnsw), and this year you will be able to try out new vector indexes for yourself. Since the beginning, Weaviate has supported vector indexing with [HNSW](/developers/weaviate/concepts/vector-index), which leads to [best-in-class query times](/developers/weaviate/benchmarks/ann). But not every use case requires single-digit millisecond latencies. Instead, some prefer cost-effectiveness."], "query": "What vector indexing method has Weaviate traditionally supported for fast query times?"}
{"relevant_passages": ["This demo is also using OpenAI for vectorization; you can choose another `text2vec` module [here](/developers/weaviate/modules/retriever-vectorizer-modules). ```python\nclient = weaviate.Client(\n    embedded_options=EmbeddedOptions(\n        additional_env_vars={\"OPENAI_APIKEY\": os.environ[\"OPENAI_APIKEY\"]}\n    )\n)\n```\n\n### Configure the Schema\n\nNow we need to configure our schema. We have the `document` class along with the `abstract` property. ```python\nclient.schema.delete_all()\n\nschema = {\n    \"class\": \"Document\",\n    \"vectorizer\": \"text2vec-openai\",\n    \"properties\": [\n        {\n            \"name\": \"source\",\n            \"dataType\": [\"text\"],\n        },\n        {\n            \"name\": \"abstract\",\n            \"dataType\": [\"text\"],\n            \"moduleConfig\": {\n                \"text2vec-openai\": {\"skip\": False, \"vectorizePropertyName\": False}\n            },\n        },\n    ],\n    \"moduleConfig\": {\n        \"generative-openai\": {},\n        \"text2vec-openai\": {\"model\": \"ada\", \"modelVersion\": \"002\", \"type\": \"text\"},\n    },\n}\n\nclient.schema.create_class(schema)\n```\n\n### Read/Import the documents\n\nNow that our schema is defined, we want to build the objects that we want to store in Weaviate. We wrote a helper class,  `AbstractExtractor` to aggregate the element class."], "query": "How do you configure a Weaviate schema to use OpenAI's `text2vec-openai` vectorizer with the `ada` model version `002`?"}
{"relevant_passages": ["The algorithm keeps a result set of points, starting with the entry point. On every iteration, it checks what points are in the result set that has not been visited yet and, from them, takes the best candidate (the one closest to the query point) and explores it. Exploring in this context means adding the candidate (out neighbors) from the graph to the result set and marking it as visited. Notice the size of the result set has to stay bounded, so every time it grows too much, we only keep those L points closer to the query. The bigger the maximum size of the result set, the more accurate the results and the slower the search."], "query": "How does the algorithm ensure that the result set of points remains bounded in size during the search process?"}
{"relevant_passages": ["have published \u201cLarge Language Models are easily distracted by irrelevant context\u201d, highlighting how problematic bad precision in search can be for retrieval-augmented generation. The recent developments in LLM agent tooling such as LangChain, LlamaIndex, and recent projects such as AutoGPT or Microsoft\u2019s Semantic Kernel are paving the way towards letting LLMs run for a while to complete complex tasks. By ranking each handoff from search to prompt, we can achieve better results in each intermediate task. Thus when we leave an LLM running overnight to research the future of ranking models, we can expect a better final result in the morning!\n\n\nimport WhatNext from '/_includes/what-next.mdx'\n\n<WhatNext />"], "query": "What are the recent developments in tooling for Large Language Models that aid in complex task completion?"}
{"relevant_passages": ["The good news is, there are companies \u2013 like Hugging Face, OpenAI, and Cohere \u2013 that offer running model inference as a service. > \"Running model inference in production is hard,\nlet them do it for you.\"\n\n## Support for Hugging Face Inference API in Weaviate\nStarting from Weaviate `v1.15`, Weaviate includes a Hugging Face module, which provides support for Hugging Face Inference straight from the vector database. The Hugging Face module, allows you to use the [Hugging Face Inference service](https://huggingface.co/inference-api#pricing) with sentence similarity models, to vectorize and query your data, straight from Weaviate. No need to run the Inference API yourself. > You can choose between `text2vec-huggingface` (Hugging Face) and `text2vec-openai` (OpenAI) modules to delegate your model inference tasks.<br/>\n> Both modules are enabled by default in the [Weaviate Cloud Services](/pricing)."], "query": "Which version of Weaviate started to include support for the Hugging Face Inference API?"}
{"relevant_passages": ["---\ntitle: Building an AI-Powered Shopping Copilot with Weaviate\nslug: moonsift-story\nauthors: [alea, zain]\ndate: 2023-11-15\ntags: []\nimage: ./img/hero.png\ndescription: \"UK-based startup Moonsift is harnessing the power of AI with Weaviate.\"\n---\n![hero](img/hero.png)\n\nUK-based startup Moonsift is harnessing the power of AI\u2014using machine learning models and Weaviate\u2019s vector database\u2014to help online shoppers discover the products they love. <!-- truncate -->\n\n[Moonsift](https://www.moonsift.com/) offers an ecommerce browser extension for users to curate shoppable boards with products from across the internet. Stylists and curators use Moonsift to create collections, registries, and wish lists that can be shared and shopped with a simple link. While thousands of customers add products from tens of thousands of retailers per month to Moonsift, co-founders David Wood and Alex Reed have a bigger vision for improving product discoverability for online shoppers. With combined experience in natural language processing (NLP), data science, and consulting for retail brands, Wood and Reed saw how retailers unknowingly restrict their own discoverability by tailoring keywords for search engines rather than users."], "query": "What is the UK-based startup that uses Weaviate's vector database to enhance online shopping discoverability?"}
{"relevant_passages": ["We will call this in order to grab the abstract element along with the content. <details>\n  <summary>AbstractExtractor</summary>\n\n```python\nimport logging\n\nlogging.basicConfig(level=logging.INFO)\n\n\nclass AbstractExtractor:\n    def __init__(self):\n        self.current_section = None  # Keep track of the current section being processed\n        self.have_extracted_abstract = (\n            False  # Keep track of whether the abstract has been extracted\n        )\n        self.in_abstract_section = (\n            False  # Keep track of whether we're inside the Abstract section\n        )\n        self.texts = []  # Keep track of the extracted abstract text\n\n    def process(self, element):\n        if element.category == \"Title\":\n            self.set_section(element.text)\n\n            if self.current_section == \"Abstract\":\n                self.in_abstract_section = True\n                return True\n\n            if self.in_abstract_section:\n                return False\n\n        if self.in_abstract_section and element.category == \"NarrativeText\":\n            self.consume_abstract_text(element.text)\n            return True\n\n        return True\n\n    def set_section(self, text):\n        self.current_section = text\n        logging.info(f\"Current section: {self.current_section}\")\n\n    def consume_abstract_text(self, text):\n        logging.info(f\"Abstract part extracted: {text}\")\n        self.texts.append(text)\n\n    def consume_elements(self, elements):\n        for element in elements:\n            should_continue = self.process(element)\n\n            if not should_continue:\n                self.have_extracted_abstract = True\n                break\n\n        if not self.have_extracted_abstract:\n            logging.warning(\"No abstract found in the given list of objects.\")\n\n    def abstract(self):\n        return \"\\n\".join(self.texts)\n```\n</details>\n\n```python\ndata_folder = \"../data\"\n\ndata_objects = []\n\nfor path in Path(data_folder).iterdir():\n    if path.suffix != \".pdf\":\n        continue\n\n    print(f\"Processing {path.name}...\")\n\n    elements = partition_pdf(filename=path)\n\n    abstract_extractor = AbstractExtractor()\n    abstract_extractor.consume_elements(elements)\n\n    data_object = {\"source\": path.name, \"abstract\": abstract_extractor.abstract()}\n\n    data_objects.append(data_object)\n```\n\nThe next step is to import the objects into Weaviate. ```python\nclient.batch.configure(batch_size=100)  # Configure batch\nwith client.batch as batch:\n    for data_object in data_objects:\n        batch.add_data_object(data_object, \"Document\")\n```\n\n### Query Time\n\nNow that we have imported our two documents, we can run some queries! Starting with a simple BM25 search. We want to find a document that discusses house prices. ```python\nclient.query.get(\"Document\", \"source\").with_bm25(\n    query=\"some paper about housing prices\"\n).with_additional(\"score\").do()\n```\n\n<details>\n  <summary>Output</summary>\n\n```\n{'data': {'Get': {'Document': [{'_additional': {'score': '0.8450042'},\n     'source': 'paper02.pdf'},\n    {'_additional': {'score': '0.26854637'}, 'source': 'paper01.pdf'}]}}}\n```\n\n</details>\n\nWe can take this one step further by using the generative search module."], "query": "How can you extract abstracts from documents and perform a BM25 search for a specific topic in Weaviate?"}
{"relevant_passages": ["```\n\n</details>\n\n## Limitations\nThere are a few limitations when it comes to a document that has two columns. For example, if a document is structured with two columns, then the text doesn\u2019t extract perfectly. The workaround for this is to set `strategy=\"ocr_only\"` or `strategy=\"fast\"` into `partition_pdf`. There is a [GitHub issue](https://github.com/Unstructured-IO/unstructured/issues/356) on fixing multi-column documents, give it a \ud83d\udc4d up!\n\n<details>\n  <summary>strategy=\"ocr_only\"</summary>\n\n```python\nelements = partition_pdf(filename=\"../data/paper02.pdf\", strategy=\"ocr_only\")\nabstract_extractor = AbstractExtractor()\nabstract_extractor.consume_elements(elements)\n```\n\n</details>\n\n<details>\n  <summary>strategy=\u201dfast\u201d</summary>\n\n```python\nelements = partition_pdf(filename=\"../data/paper02.pdf\", strategy=\"fast\")\nabstract_extractor = AbstractExtractor()\nabstract_extractor.consume_elements(elements)\n```\n\n</details>\n\n## Weaviate Brick in Unstructured\nThere is a [GitHub issue](https://github.com/Unstructured-IO/unstructured/issues/566) to add a Weaviate staging brick! The goal of this integration is to add a Weaviate section to the documentation and show how to load unstructured outputs into Weaviate. Make sure to give this issue a \ud83d\udc4d up!\n\n## Last Thought\nThis demo introduced how you can ingest PDFs into Weaviate."], "query": "What strategies are recommended for extracting text from two-column PDF documents using `partition_pdf`?"}
{"relevant_passages": ["If you want to learn how to configure Weaviate to use PQ refer to the docs [here](/developers/weaviate/config-refs/schema/vector-index#how-to-configure-hnsw). ## KMeans encoding results\n\nFirst, the PQ feature added to version 1.18 of Weaviate is assessed in the sections below. To check performance and distortion, we compared our implementation to [NanoPQ](https://github.com/matsui528/nanopq) and we observed similar results. The main idea behind running these experiments is to explore how PQ compression would affect our current indexing algorithms. The experiments consist of fitting the Product Quantizer on some datasets and then calculating the recall by applying brute force search on the compressed vectors."], "query": "How does Weaviate's PQ feature performance compare to NanoPQ?"}
{"relevant_passages": ["![Hacktober video](img/hacktober.gif)\n\n### [Weaviate Academy](/developers/academy) & [Workshops](/learn/workshops)\nWeaviate Academy and Workshops have had a fantastic year of learning and growth! We've been focusing on ensuring everyone has the chance to understand and use vector databases and get a grasp on Generative AI and data handling. Every week, [Zain](https://www.linkedin.com/in/zainhas/), [JP](https://www.linkedin.com/in/jphwang/), [Daniel](https://www.linkedin.com/in/malgamves/), and [Duda](https://www.linkedin.com/in/dudanogueira/) have been running introductory workshops on vector databases and Weaviate, which have been a hit. Plus, we're super excited about [JP Hwang](https://www.linkedin.com/in/jphwang/)'s initiative, the Weaviate Academy. It's a program that takes you from the basics all the way to production-level skills. Later in the year, we teamed up with [DeepLearningAI](https://www.deeplearning.ai/) to create a short course with [Sebastian](https://www.linkedin.com/in/sebawita/) and [Zain](https://www.linkedin.com/in/zainhas/), \"[Vector Databases: from Embeddings to Applications with Weaviate](https://www.deeplearning.ai/short-courses/vector-databases-embeddings-applications/).\" It\u2019s been a year packed with learning."], "query": "Who has been running introductory workshops on vector databases and Weaviate every week as part of the Weaviate Academy and Workshops?"}
{"relevant_passages": ["This seems like a weird point to make but it will be very important later on!\n\n![three images](./img/three_images.png)\n\nLearning the true underlying distribution of any set of images is not computationally feasible  because it requires you to consider every pixel of every image. However, if our model could learn the underlying distribution of the training set of images it could calculate the likelihood that any new image came from that set. It could also generate novel images that it thinks are most likely to belong to the training set. One way to do this, using the underlying distribution, would be to start off with static noise (an image with random pixel values) and then slightly alter pixel values over and over again while making sure each time you alter the pixel values it increases the likelihood of the overall image coming from the dataset - this is indeed what diffusion models do!\n\nThe question then becomes how diffusion models can learn (or even approximate) the underlying distribution of the images in your training set? The main insight behind how this happens is: if you take any image from your training set and add a small amount of random static noise to it you will create a new image that is slightly less likely - since images with random noise are unlikely to be seen in the training set."], "query": "How do diffusion models generate images that are likely to belong to a training set?"}
