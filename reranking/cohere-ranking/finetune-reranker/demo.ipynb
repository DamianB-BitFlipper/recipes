{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning a Reranker using Cohere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This demo will show you how to:\n",
    "1. Generate synthetic data using DSPy\n",
    "2. Export all your data from your Weaviate instance\n",
    "3. Steps to fine-tune a reranker using Cohere\n",
    "4. Query in Weaviate using your fine-tuned reranker model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: To fine-tune a model in Cohere, you need to have a minimum of 256 unique queries with at least 1 relevant passage per query. If you already have a dataset with query + relevant passages, you can skip to the end of the notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For a full walkthrough of the demo, check out the [complimenting blog post](https://weaviate.io/blog/fine-tuning-coheres-reranker)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Weaviate Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "import json \n",
    "\n",
    "client = weaviate.Client(\n",
    "    url = \"WEAVIATE_URL\",  # Replace with your cluster url\n",
    "    auth_client_secret=weaviate.AuthApiKey(api_key=\"AUTH_KEY\"),  # Replace w/ your Weaviate instance API key\n",
    "    additional_headers = {\n",
    "        \"X-Cohere-Api-Key\": \"API-KEY\" # Replace with your inference API key\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def chunk_list(lst, chunk_size):\n",
    "    \"\"\"Break a list into chunks of the specified size.\"\"\"\n",
    "    return [lst[i:i + chunk_size] for i in range(0, len(lst), chunk_size)]\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    \"\"\"Split text into sentences using regular expressions.\"\"\"\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
    "    return [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "\n",
    "def read_and_chunk_index_files(main_folder_path):\n",
    "    \"\"\"Read index.md files from subfolders, split into sentences, and chunk every 5 sentences.\"\"\"\n",
    "    blog_chunks = []\n",
    "    for folder_name in os.listdir(main_folder_path):\n",
    "        subfolder_path = os.path.join(main_folder_path, folder_name)\n",
    "        if os.path.isdir(subfolder_path):\n",
    "            index_file_path = os.path.join(subfolder_path, 'index.mdx')\n",
    "            if os.path.isfile(index_file_path):\n",
    "                with open(index_file_path, 'r', encoding='utf-8') as file:\n",
    "                    content = file.read()\n",
    "                    sentences = split_into_sentences(content)\n",
    "                    sentence_chunks = chunk_list(sentences, 5)\n",
    "                    sentence_chunks = [' '.join(chunk) for chunk in sentence_chunks]\n",
    "                    blog_chunks.extend(sentence_chunks)\n",
    "    return blog_chunks\n",
    "\n",
    "# Example usage\n",
    "main_folder_path = './blog'\n",
    "blogs = read_and_chunk_index_files(main_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to reset your schema and delete objects in a collection, run:\n",
    "`client.schema.delete_all()` or `client.schema.delete_class(\"Blogs\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = {\n",
    "   \"classes\": [\n",
    "       {\n",
    "           \"class\": \"Blogs\",\n",
    "           \"description\": \"Weaviate blogs\",\n",
    "           \"vectorizer\": \"text2vec-cohere\",\n",
    "           \"properties\": [\n",
    "               {\n",
    "                   \"name\": \"content\",\n",
    "                   \"dataType\": [\"text\"],\n",
    "                   \"description\": \"Content from the blogs.\",\n",
    "               },\n",
    "               {\n",
    "                   \"name\": \"synthetic_query\",\n",
    "                   \"dataType\": [\"text\"],\n",
    "                   \"description\": \"Synthetic query generated from a LM.\"\n",
    "               }\n",
    "           ]\n",
    "       }      \n",
    "   ]\n",
    "}\n",
    "    \n",
    "client.schema.create(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install dspy-ai > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To generate the synthetic queries, we will use DSPy's signature and chain-of-thought module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "import cohere\n",
    "cohere_key = \"API-KEY\"\n",
    "\n",
    "class WriteQuery(dspy.Signature):\n",
    "    \"\"\"Write a query that this document would have the answer to.\"\"\"\n",
    "\n",
    "    document = dspy.InputField(desc=\"A document containing information.\") \n",
    "    query = dspy.OutputField(desc=\"A short question uniquely answered by the document.\")\n",
    "\n",
    "command_nightly = dspy.Cohere(model=\"command-nightly\",max_tokens=1000, api_key=cohere_key)\n",
    "\n",
    "for blog_chunk in blogs:\n",
    "    with dspy.context(lm=command_nightly):\n",
    "        llm_query = dspy.ChainOfThought(WriteQuery)(document=blog_chunk)\n",
    "    print(llm_query)\n",
    "    data_properties = {\n",
    "        \"content\": blog_chunk,\n",
    "        \"synthetic_query\": llm_query.query\n",
    "    }\n",
    "    print(f\"{data_properties}\\n\")\n",
    "    client.data_object.create(data_properties, \"Blogs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here is one example of the chain-of-thought module in DSPy. It is taking my initial signature (prompt) and putting the first blog chunk into the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Write a query that this document would have the answer to.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Document: A document containing information.\n",
      "Reasoning: Let's think step by step in order to ${produce the query}. We ...\n",
      "Query: A short question uniquely answered by the document.\n",
      "\n",
      "---\n",
      "\n",
      "Document: --- title: Combining LangChain and Weaviate slug: combining-langchain-and-weaviate authors: [erika] date: 2023-02-21 tags: ['integrations'] image: ./img/hero.png description: \"LangChain is one of the most exciting new tools in AI. It helps overcome many limitations of LLMs, such as hallucination and limited input lengths.\" --- ![Combining LangChain and Weaviate](./img/hero.png) Large Language Models (LLMs) have revolutionized the way we interact and communicate with computers. These machines can understand and generate human-like language on a massive scale. LLMs are a versatile tool that is seen in many applications like chatbots, content creation, and much more. Despite being a powerful tool, LLMs have the drawback of being too general.\n",
      "Reasoning: Let's think step by step in order to\u001b[32m produce the query. We need to identify the unique aspects of the document that would allow us to formulate a question that this document can answer. The document seems to focus on the combination of LangChain and Weaviate, mentioning the benefits of LangChain in overcoming limitations of LLMs such as hallucination and limited input lengths. It also provides a date, author, and tags related to integrations. Given this information, we can create a query that asks about the purpose of combining LangChain with Weaviate, as this is a specific topic that the document addresses.\n",
      "\n",
      "Query: What are the benefits of combining LangChain with Weaviate in the context of LLMs?\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with dspy.context(lm=command_nightly):\n",
    "    dspy.ChainOfThought(WriteQuery)(document=blogs[0]).query\n",
    "command_nightly.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the data from your Weaviate instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fine-tune the model, we need to export our data and upload it to Cohere's reranker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your data is out of Weaviate!\n",
      "Extracted 405 in 0.6107177734375 seconds.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This example will show you how to get all of your data\n",
    "out of Weaviate and into a JSON file using the Cursor API!\n",
    "'''\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "# Step 1 - Get the UUID of the first object inserted into Weaviate\n",
    "\n",
    "get_first_object_weaviate_query = \"\"\"\n",
    "{\n",
    "  Get {\n",
    "    Blogs {\n",
    "      _additional {\n",
    "        id\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "results = client.query.raw(get_first_object_weaviate_query)\n",
    "uuid_cursor = results[\"data\"][\"Get\"][\"Blogs\"][0][\"_additional\"][\"id\"]\n",
    "\n",
    "# Step 2 - Get the Total Objects in Weaviate\n",
    "\n",
    "total_objs_query = \"\"\"\n",
    "{\n",
    "    Aggregate {\n",
    "        Blogs {\n",
    "            meta {\n",
    "                count\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "results = client.query.raw(total_objs_query)\n",
    "total_objects = results[\"data\"][\"Aggregate\"][\"Blogs\"][0][\"meta\"][\"count\"]\n",
    "\n",
    "# Step 3 - Iterate through Weaviate with the Cursor\n",
    "increment = 50\n",
    "data = []\n",
    "for i in range(0, total_objects, increment):\n",
    "    results = (\n",
    "        client.query.get(\"Blogs\", [\"content\", \"synthetic_query\"])\n",
    "        .with_additional([\"id\"])\n",
    "        .with_limit(50)\n",
    "        .with_after(uuid_cursor)\n",
    "        .do()\n",
    "    )[\"data\"][\"Get\"][\"Blogs\"]\n",
    "    # extract data from result into JSON\n",
    "    for result in results:\n",
    "        if len(result[\"synthetic_query\"]) < 5:\n",
    "            continue\n",
    "        new_obj = {}\n",
    "        for key in result.keys():\n",
    "            if key == \"_additional\":\n",
    "                continue\n",
    "            if key == \"synthetic_query\":\n",
    "                new_obj[\"query\"] = result[key]\n",
    "            if key == \"content\":\n",
    "                new_obj[\"relevant_passages\"] = [result[key]]\n",
    "        data.append(new_obj)\n",
    "    # update uuid cursor to continue the loop\n",
    "    # we have just exited a loop where result holds the last obj\n",
    "    uuid_cursor = result[\"_additional\"][\"id\"]\n",
    "\n",
    "# save JSON\n",
    "file_path = \"data.jsonl\"\n",
    "with open(file_path, 'w') as jsonl_file:\n",
    "    for item in data:\n",
    "        jsonl_file.write(json.dumps(item) + '\\n')\n",
    "\n",
    "print(\"Your data is out of Weaviate!\")\n",
    "print(f\"Extracted {total_objects} in {time.time() - start} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open(\"data.jsonl\", \"r\") as file:\n",
    "    for line in file:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "split_index = int(len(data)*0.8)\n",
    "train_data = data[:split_index]\n",
    "validation_data = data[split_index:]\n",
    "\n",
    "with open(\"./train.jsonl\", \"w\") as train_file:\n",
    "    for line in train_data:\n",
    "        train_file.write(json.dumps(line) + \"\\n\")\n",
    "\n",
    "with open(\"./validation.jsonl\", \"w\") as validation_file:\n",
    "    for line in validation_data:\n",
    "        validation_file.write(json.dumps(line) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tune on Cohere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You will need to upload your training and validation dataset to [Cohere](https://dashboard.cohere.com/fine-tuning/create?endpoint=rerank). Once you start the training, it will take a few hours (estimate) to train and output your `model_id`. You will also have access to a dashboard that shows the model performance (screenshot below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Cohere Dashboard](cohere-dashboard.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-Index Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use our fine-tuned reranker, we will need to upload our data again to a new collection and add the `model_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = {\n",
    "   \"classes\": [\n",
    "       {\n",
    "           \"class\": \"BlogsFineTuned\",\n",
    "           \"description\": \"Weaviate blogs\",\n",
    "           \"vectorizer\": \"text2vec-cohere\",\n",
    "           \"moduleConfig\": {\n",
    "                \"reranker-cohere\": {\n",
    "                    \"model\": \"model_id\" # grab the model_id from Cohere\n",
    "                }\n",
    "           },\n",
    "           \"properties\": [\n",
    "               {\n",
    "                   \"name\": \"content\",\n",
    "                   \"dataType\": [\"text\"],\n",
    "                   \"description\": \"Content from the blogs.\",\n",
    "               }\n",
    "           ]\n",
    "       }      \n",
    "   ]\n",
    "}\n",
    "    \n",
    "client.schema.create(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload data (same as above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for blog in blogs:\n",
    "    data_properties = {\n",
    "        \"content\": blog\n",
    "    }\n",
    "    client.data_object.create(\n",
    "        data_object = data_properties,\n",
    "        class_name = \"BlogsFineTuned\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query without Reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"data\": {\n",
      "    \"Get\": {\n",
      "      \"BlogsFineTuned\": [\n",
      "        {\n",
      "          \"content\": \"---\\ntitle: What is Ref2Vec and why you need it for your recommendation system\\nslug: ref2vec-centroid\\nauthors: [connor]\\ndate: 2022-11-23\\ntags: ['integrations', 'concepts']\\nimage: ./img/hero.png\\ndescription: \\\"Weaviate introduces Ref2Vec, a new module that utilises Cross-References for Recommendation!\\\"\\n---\\n![Ref2vec-centroid](./img/hero.png)\\n\\n<!-- truncate -->\\n\\nWeaviate 1.16 introduced the [Ref2Vec](/developers/weaviate/modules/retriever-vectorizer-modules/ref2vec-centroid) module. In this article, we give you an overview of what Ref2Vec is and some examples in which it can add value such as recommendations or representing long objects. ## What is Ref2Vec? The name Ref2Vec is short for reference-to-vector, and it offers the ability to vectorize a data object with its cross-references to other objects. The Ref2Vec module currently holds the name ref2vec-**centroid** because it uses the average, or centroid vector, of the cross-referenced vectors to represent the **referencing** object.\"\n",
      "        },\n",
      "        {\n",
      "          \"content\": \"In other words, the User vector is being updated in real-time here to take into account their preferences and actions, which helps to produce more relevant results at speed. Another benefit of Ref2Vec is that this calculation is not compute-heavy, leading to low overhead. With Ref2Vec, you can use Weaviate to provide Recommendation with \\\"user-as-query\\\". This is a very common and powerful way to build Home Feed style features in apps. This can be done by sending queries like this to Weaviate:\\n\\n```graphql\\n{\\n  Get {\\n    Product (\\n      nearObject: {\\n        id: \\\"8abc5-4d5...\\\" # id for the User object with vector defined by ref2vec-centroid\\n      }\\n    ) {\\n      product_name\\n      price\\n    }\\n  }\\n}\\n```\\n\\nThis short query encapsulates the power of Ref2Vec.\"\n",
      "        },\n",
      "        {\n",
      "          \"content\": \"![Cross-reference](./img/Weaviate-Ref2Vec_1.png)\\n\\nRef2Vec gives Weaviate another way to vectorize a class, such as the User class, based on their relationships to other classes. This allows Weaviate to quickly create up-to-date representations of users based on their relationships such as recent interactions. If a user clicks on 3 shoe images on an e-commerce store, it is a safe bet that they want to see more shoes. Ref2Vec captures this intuition by calculating vectors that aggregate each User's interaction with another class. The below animation visualizes a real example of this in e-Commerce images.\"\n",
      "        },\n",
      "        {\n",
      "          \"content\": \"The following image depicts how Ref2Vec aggregates the representations of 3 Product items to represent a User who has purchased a pair of boots, shorts, and Weaviate t-shirt!\\n\\n![Ref2Vec Image](./img/ref2vec.png)\\n\\nSuch a representation of the User, by an aggregation of their cross-references, allows Weaviate to conveniently and immediately learn from each User's preferences and actions to provide improved and up-to-date characterizations. Ref2Vec can in other words capture each User's interests and tendencies across multiple axes, such as product categories or even fashion styles! And by doing so, the resulting recommendations can more closely match the User's product and style preferences. We envision Ref2Vec to have great potential in multiple application areas. Let's take a look at a few of them in more detail, starting with recommendation systems. ## Recommendation in Weaviate\\nMany of you might primarily know Weaviate as a vector database and search engine, but Weaviate can also power high-quality, lightning-fast recommendations.\"\n",
      "        },\n",
      "        {\n",
      "          \"content\": \"## More Coming Soon\\nWe are very excited about the potential of Ref2Vec, and how it leverages existing symbolic data to augment vector searches in a new way. One of my favorite articles about Weaviate is Bob van Luijt's [\\\"The AI-First Database Ecosystem\\\"](/blog/semantic-search-with-wikipedia-and-weaviate). In this article, Bob describes emerging **waves** of database technology; from SQL, to NoSQL, and now, AI-first databases that focus \\\"on data that is processed by a machine learning model first, where the AI models help in processing, storing and searching through the data as opposed to traditional ways\\\". Although Weaviate puts Machine Learning data representations first, **this doesn't mean we discard symbolic data and many features of previous systems**. Rather, we are actively searching for how symbolic data can improve neural functionality and vice versa.\"\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = (\n",
    "    client.query\n",
    "    .get(\"BlogsFineTuned\", [\"content\"])\n",
    "    .with_near_text({\n",
    "        \"concepts\": [\"Ref2Vec in Weaviate\"]\n",
    "    })\n",
    "    .with_limit(5)\n",
    "    .do()\n",
    ")\n",
    "\n",
    "print(json.dumps(response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query with Reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"data\": {\n",
      "    \"Get\": {\n",
      "      \"BlogsFineTuned\": [\n",
      "        {\n",
      "          \"_additional\": {\n",
      "            \"rerank\": [\n",
      "              {\n",
      "                \"score\": 0.9261703\n",
      "              }\n",
      "            ]\n",
      "          },\n",
      "          \"content\": \"![Cross-reference](./img/Weaviate-Ref2Vec_1.png)\\n\\nRef2Vec gives Weaviate another way to vectorize a class, such as the User class, based on their relationships to other classes. This allows Weaviate to quickly create up-to-date representations of users based on their relationships such as recent interactions. If a user clicks on 3 shoe images on an e-commerce store, it is a safe bet that they want to see more shoes. Ref2Vec captures this intuition by calculating vectors that aggregate each User's interaction with another class. The below animation visualizes a real example of this in e-Commerce images.\"\n",
      "        },\n",
      "        {\n",
      "          \"_additional\": {\n",
      "            \"rerank\": [\n",
      "              {\n",
      "                \"score\": 0.34444344\n",
      "              }\n",
      "            ]\n",
      "          },\n",
      "          \"content\": \"The following image depicts how Ref2Vec aggregates the representations of 3 Product items to represent a User who has purchased a pair of boots, shorts, and Weaviate t-shirt!\\n\\n![Ref2Vec Image](./img/ref2vec.png)\\n\\nSuch a representation of the User, by an aggregation of their cross-references, allows Weaviate to conveniently and immediately learn from each User's preferences and actions to provide improved and up-to-date characterizations. Ref2Vec can in other words capture each User's interests and tendencies across multiple axes, such as product categories or even fashion styles! And by doing so, the resulting recommendations can more closely match the User's product and style preferences. We envision Ref2Vec to have great potential in multiple application areas. Let's take a look at a few of them in more detail, starting with recommendation systems. ## Recommendation in Weaviate\\nMany of you might primarily know Weaviate as a vector database and search engine, but Weaviate can also power high-quality, lightning-fast recommendations.\"\n",
      "        },\n",
      "        {\n",
      "          \"_additional\": {\n",
      "            \"rerank\": [\n",
      "              {\n",
      "                \"score\": 0.007121429\n",
      "              }\n",
      "            ]\n",
      "          },\n",
      "          \"content\": \"In other words, the User vector is being updated in real-time here to take into account their preferences and actions, which helps to produce more relevant results at speed. Another benefit of Ref2Vec is that this calculation is not compute-heavy, leading to low overhead. With Ref2Vec, you can use Weaviate to provide Recommendation with \\\"user-as-query\\\". This is a very common and powerful way to build Home Feed style features in apps. This can be done by sending queries like this to Weaviate:\\n\\n```graphql\\n{\\n  Get {\\n    Product (\\n      nearObject: {\\n        id: \\\"8abc5-4d5...\\\" # id for the User object with vector defined by ref2vec-centroid\\n      }\\n    ) {\\n      product_name\\n      price\\n    }\\n  }\\n}\\n```\\n\\nThis short query encapsulates the power of Ref2Vec.\"\n",
      "        },\n",
      "        {\n",
      "          \"_additional\": {\n",
      "            \"rerank\": [\n",
      "              {\n",
      "                \"score\": 5.5508026e-06\n",
      "              }\n",
      "            ]\n",
      "          },\n",
      "          \"content\": \"---\\ntitle: What is Ref2Vec and why you need it for your recommendation system\\nslug: ref2vec-centroid\\nauthors: [connor]\\ndate: 2022-11-23\\ntags: ['integrations', 'concepts']\\nimage: ./img/hero.png\\ndescription: \\\"Weaviate introduces Ref2Vec, a new module that utilises Cross-References for Recommendation!\\\"\\n---\\n![Ref2vec-centroid](./img/hero.png)\\n\\n<!-- truncate -->\\n\\nWeaviate 1.16 introduced the [Ref2Vec](/developers/weaviate/modules/retriever-vectorizer-modules/ref2vec-centroid) module. In this article, we give you an overview of what Ref2Vec is and some examples in which it can add value such as recommendations or representing long objects. ## What is Ref2Vec? The name Ref2Vec is short for reference-to-vector, and it offers the ability to vectorize a data object with its cross-references to other objects. The Ref2Vec module currently holds the name ref2vec-**centroid** because it uses the average, or centroid vector, of the cross-referenced vectors to represent the **referencing** object.\"\n",
      "        },\n",
      "        {\n",
      "          \"_additional\": {\n",
      "            \"rerank\": [\n",
      "              {\n",
      "                \"score\": 4.7478566e-06\n",
      "              }\n",
      "            ]\n",
      "          },\n",
      "          \"content\": \"## More Coming Soon\\nWe are very excited about the potential of Ref2Vec, and how it leverages existing symbolic data to augment vector searches in a new way. One of my favorite articles about Weaviate is Bob van Luijt's [\\\"The AI-First Database Ecosystem\\\"](/blog/semantic-search-with-wikipedia-and-weaviate). In this article, Bob describes emerging **waves** of database technology; from SQL, to NoSQL, and now, AI-first databases that focus \\\"on data that is processed by a machine learning model first, where the AI models help in processing, storing and searching through the data as opposed to traditional ways\\\". Although Weaviate puts Machine Learning data representations first, **this doesn't mean we discard symbolic data and many features of previous systems**. Rather, we are actively searching for how symbolic data can improve neural functionality and vice versa.\"\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = (\n",
    "    client.query\n",
    "    .get(\"BlogsFineTuned\", [\"content\"])\n",
    "    .with_near_text({\n",
    "        \"concepts\": [\"Ref2Vec in Weaviate\"]\n",
    "    })\n",
    "    .with_additional(\"rerank(property: \\\"content\\\" query: \\\"Represent users based on their product interactions\\\") { score }\")\n",
    "    .with_limit(5)\n",
    "    .do()\n",
    ")\n",
    "\n",
    "print(json.dumps(response, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
